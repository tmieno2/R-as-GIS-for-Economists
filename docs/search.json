[{"path":"index.html","id":"welcome","chapter":"Welcome","heading":"Welcome","text":"book developed part effort put together course materials data science course targeted upper-level undergraduate graduate students University Nebraska Lincoln. book aims particularly spatial data processing econometric projects, spatial variables become part econometric analysis. years, seen many students researchers spend much time just processing spatial data (often involving clicking ArcGIS (QGIS) user interface death), waste time perspective academic productivity. hope book help researchers become proficient spatial data processing enhance overall productivity fields economics spatial data essential.meI Associate Professor Department Agricultural Economics University Nebraska Lincoln, also teach Econometrics Master’s students. research interests lie precision agriculture, water economics, agricultural policy. personal website .Contributors bookHere list contributors book parts contributed :Bowen Chen, Data Scientist, Bunge, Missouri\nSection 9.2\nSection 9.2Shinya Uryu, Assistant Professor, Center Design-Oriented AI Education Research, Tokushima University, Japan (github account: https://github.com/uribo)\nSection 8\nSection 8Gal Koss, Graduate Student, Colorado State University\nSection 4.6 9.5\nSection 4.6 9.5Jude Bayham, Assistant Professor, Colorado State University\nSection 4.6 9.5\nSection 4.6 9.5Comments Suggestions?constructive comments suggestions can improve book welcome. Please send email tmieno2@unl.edu create issue github page book.work licensed Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License.","code":""},{"path":"preface.html","id":"preface","chapter":"Preface","heading":"Preface","text":"","code":""},{"path":"preface.html","id":"why-r-as-gis-for-economists","chapter":"Preface","heading":"Why R as GIS for Economists?","text":"R extensive capabilities GIS software. opinion, \\(99\\%\\) spatial data processing needs economist satisfied R. , several popular options GIS tasks R:PythonArcGISQGISHere, compare briefly discuss R good option.","code":""},{"path":"preface.html","id":"r-vs-python","chapter":"Preface","heading":"R vs Python","text":"R Python actually heavily dependent open source software GDAL GEOS core GIS operations (GDAL reading spatial data, GEOS geometrical operations like intersecting two spatial layers).1 , run GIS tasks R Python basically tell R Python want talk software, let job, return results . means R Python much different capability GIS tasks dependent common open source software many GIS tasks. GDAL GEOS get better, R Python get better (short lag thanks make updates available R). good spatial visualization tools well. Moreover, R Python can communicate QGIS ArcGIS (long installed course) use functionalities within R Python via bridging packages: RQGIS PyQGIS QGIS, R-ArcGIS ArcPy.2 , familiar Python R, go ahead go Python. now , discussions assume going R option, otherwise, reading rest book anyway.","code":""},{"path":"preface.html","id":"r-vs-arcgis-or-qgis","chapter":"Preface","heading":"R vs ArcGIS or QGIS","text":"ArcGIS commercial software quite expensive (likely able get significant discount student work University). hand, QGIS open source free. seen significant developments decade, say just competitive ArcGIS. QGIS also uses open source geospatial software GDAL, GEOS, others (SAGA, GRASS GIS). graphical interface aids implement various GIS tasks unlike R requires programming.Now, since R can use ArcGIS QGIS bridging packages, precise question asking whether program GIS tasks using R (possibly using bridging packages) manually implement GIS tasks using graphical interface ArcGIS QGIS. answer programming GIS tasks using R. First, manual GIS operations hard repeat. often case course project need redo GIS task except underlying datasets changed. programmed process R, just run code ’s . get desired results. program , need go many clicks graphical interface , potentially trying remember actually last time.3 Second important, manual operations scalable. become much common need process many large spatial datasets. Imagine operations \\(1,000\\) files using graphical interface, even \\(50\\) files. know good tasks without complaining? computer. Just let likes . better things .Finally, learn ArcGIS QGIS addition () R? doubtful. economists, GIS tasks need super convoluted time. Suppose \\(\\Omega_R\\) \\(\\Omega_{AQ}\\) represent set GIS tasks R \\(ArcGIS/QGIS\\) can implement, respectively. , let \\(\\Omega_E\\) represent set skills economists need implement. , \\(\\Omega_E \\\\Omega_R\\) \\(99\\%\\) (maybe \\(95\\%\\) safe) time \\(\\Omega_E \\\\subset \\Omega_{AQ}\\setminus\\Omega_R\\) \\(99\\%\\) time. Personally, never rely either ArcGIS QGIS research projects learned use R GIS.One things ArcGIS QGIS can R (\\(\\Omega_{AQ}\\setminus\\Omega_R\\)) create spatial objects hand using graphical user interface, like drawing polygons lines. Another thing R lags behind ArcGIS QGIS 3D data visualization. , must say neither essential economists moment. Finally, sometime easier faster make map using ArcGIS QGIS especially complicated map.4Using R GIS, however, comes learning curve never used R basic knowledge R general programming knowledge required. hand, GUI-based use ArcGIS QGIS low start-cost. used R purposes like data wrangling regression analysis, already (almost) climbed hill ready learn use R GIS.","code":""},{"path":"preface.html","id":"summary","chapter":"Preface","heading":"Summary","text":"never used GIS software, comfortable R?Learn use R GIS first. find really complete GIS tasks like using R, turn options.never used GIS software R?tough. expect significant amount GIS work, learning R basics use R GIS good investment time.used ArcGIS QGIS like crash often?don’t try R?5 may realize actually need .used ArcGIS QGIS comfortable , need program repetitive GIS tasks?Learn R maybe take advantage R-ArcGIS RQGIS, book cover.know sure need run simple GIS task never GIS tasks ever ?Stop reading ask one friends job. Pay /\\(\\$20\\) per hour, way opportunity cost setting either ArcGIS QGIS learning simple task.","code":""},{"path":"preface.html","id":"how-is-this-book-different-from-other-online-books-and-resources","chapter":"Preface","heading":"How is this book different from other online books and resources?","text":"seeing explosion online (free) resources teach use R spatial data processing.6 incomplete list resources:Geocomputation RSpatial Data ScienceSpatial Data Science RIntroduction GIS using RCode Introduction Spatial Analysis Mapping RIntroduction GIS RIntro GIS Spatial AnalysisIntroduction Spatial Data Programming RReproducible GIS analysis RR Earth-System ScienceRspatialNEON Data SkillsSimple Features RGeospatial Health Data: Modeling Visualization R-INLA Shiny\nThanks resources, become much easier self-teach R GIS work six seven years ago first started using R GIS. Even though read resources carefully, pretty sure every topic found book can also found somewhere resources (except demonstrations). , may wonder earth can benefit reading book. boils search costs. Researchers different disciplines require different sets spatial data skills. available resources typically general covering many topics, economists unlikely use. particularly hard much experience GIS identify whether particular skills essential . , spend much time learning something really useful. value book lies deliberate incomprehensiveness. packages materials satisfy need economists, cutting many topics likely limited use economists.looking comprehensive treatments spatial data handling processing one book, personally like Geocomputation R lot. Increasingly, developer R packages created website dedicated R packages, can often find vignettes (tutorials), like Simple Features R.","code":""},{"path":"preface.html","id":"topics-covered-in-this-book","chapter":"Preface","heading":"Topics covered in this book","text":"book starts basics spatial data handling (e.g., importing exporting spatial datasets) moves practical spatial data operations (e.g., spatial data join) useful research projects. parts books still development. Right now, Chapters 1 8, parts Chapter 9, Appendix available.Chapter 1: Demonstrations R GIS\ngroundwater pumping groundwater level\nprecision agriculture\nland use weather\ncorn planted acreage railroads\ngroundwater pumping weather\ngroundwater pumping groundwater levelprecision agricultureland use weathercorn planted acreage railroadsgroundwater pumping weatherChapter 2: basics vector data handling using sf package\nspatial data structure sf\nimport export vector data\n(re)projection spatial datasets\nsingle-layer geometrical operations (e.g., create buffers, find centroids)\nmiscellaneous basic operations\nspatial data structure sfimport export vector data(re)projection spatial datasetssingle-layer geometrical operations (e.g., create buffers, find centroids)miscellaneous basic operationsChapter 3: Spatial interactions vector datasets\nunderstand topological relations multiple sf objects\nspatially subset layer based another layer\nextracting values one layer another layer\nunderstand topological relations multiple sf objectsspatially subset layer based another layerextracting values one layer another layerChapter 4: basics raster data handling using raster terra packages\nunderstand object classes terra raster packages\nimport export raster data\nstack raster data\nquick plotting\nunderstand object classes terra raster packagesimport export raster datastack raster dataquick plottingChapter 5: Spatial interactions vector raster datasets\ncropping raster layer geographic extent vector layer\nextracting values raster layer vector layer\ncropping raster layer geographic extent vector layerextracting values raster layer vector layerChapter 6: Speed things \nmake raster data extraction faster parallelization\nmake raster data extraction faster parallelizationChapter 7: Spatiotemporal raster data handling stars packageChapter 8: Creating Maps using ggplot2 package\nuse ggplot2 packages create maps\nuse ggplot2 packages create mapsChapter 9: Download process publicly available spatial datasets (partially available)\nUSDA NASS QuickStat (tidyUSDA) - available\nPRISM (prism) - available\nDaymet (daymetr) - available\ngridMET - available\nCropland Data Layer (CropScapeR) - available\nUSGS (dataRetrieval) - construction\nSentinel 2 (sen2r) - construction\nCensus (tidycensus) - construction\nUSDA NASS QuickStat (tidyUSDA) - availablePRISM (prism) - availableDaymet (daymetr) - availablegridMET - availableCropland Data Layer (CropScapeR) - availableUSGS (dataRetrieval) - constructionSentinel 2 (sen2r) - constructionCensus (tidycensus) - constructionAppendix : Loop parallel computation (available)Appendix B: Cheatsheet - constructionAs can see , book spend time basics GIS concepts. start reading book, know followings least (’s much):Geographic Coordinate System (GCS), Coordinate Reference System (CRS), projection (good resource)Distinctions vector raster data (simple summary difference)book spatial data processing provide detailed explanations non-spatial R operations, assuming basic knowledge R. particular, dplyr data.table packages extensively used data wrangling. data wrangling using tidyverse (collection packages including dplyr), see R Data Science. data.table, good resource.Finally, book cover spatial statistics spatial econometrics . book spatial data processing. Spatial analysis something processed spatial data.","code":""},{"path":"preface.html","id":"conventions-of-the-book-and-some-notes","chapter":"Preface","heading":"Conventions of the book and some notes","text":"notes conventions book notes R beginners used reading rmarkdown-generated html documents.","code":""},{"path":"preface.html","id":"texts-in-gray-boxes","chapter":"Preface","heading":"Texts in gray boxes","text":"one following:objects defined R demonstrationsR functionsR packagesWhen function, always put parentheses end like : st_read(). Sometimes, combine package function one like : sf::st_read(). means function called st_read() sf package.","code":""},{"path":"preface.html","id":"colored-boxes","chapter":"Preface","heading":"Colored Boxes","text":"Codes blue boxes, outcomes red boxes.Codes:Outcomes:","code":"\nrunif(5)## [1] 0.2390179 0.1672033 0.6750498 0.3046832 0.4915697"},{"path":"preface.html","id":"parentheses-around-codes","chapter":"Preface","heading":"Parentheses around codes","text":"Sometimes see codes enclosed parenthesis like :parentheses prints ’s inside newly created object () without explicitly evaluating object. , basically signaling looking inside object just created.one prints nothing.","code":"\n(\n  a <- runif(5)\n)## [1] 0.03551853 0.97016650 0.81131664 0.20098150 0.98102364\na <- runif(5)"},{"path":"preface.html","id":"footnotes","chapter":"Preface","heading":"Footnotes","text":"Footnotes appear bottom page. can easily get footnote clicking footnote number. can also go back main narrative footnote number clicking curved arrow end footnote. , don’t worry scroll way reading footnotes.","code":""},{"path":"preface.html","id":"session-information","chapter":"Preface","heading":"Session Information","text":"session information compiling book:","code":"\nsessionInfo()## R version 4.1.3 (2022-03-10)\n## Platform: x86_64-apple-darwin17.0 (64-bit)\n## Running under: macOS Mojave 10.14.6\n## \n## Matrix products: default\n## BLAS:   /Library/Frameworks/R.framework/Versions/4.1/Resources/lib/libRblas.0.dylib\n## LAPACK: /Library/Frameworks/R.framework/Versions/4.1/Resources/lib/libRlapack.dylib\n## \n## locale:\n## [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8\n## \n## attached base packages:\n## [1] stats     graphics  grDevices utils     datasets  methods   base     \n## \n## loaded via a namespace (and not attached):\n##  [1] bookdown_0.25   digest_0.6.29   R6_2.5.1        jsonlite_1.8.0 \n##  [5] magrittr_2.0.3  evaluate_0.15   stringi_1.7.6   cachem_1.0.6   \n##  [9] rlang_1.0.2     cli_3.2.0       fs_1.5.2        rstudioapi_0.13\n## [13] jquerylib_0.1.4 xml2_1.3.3      bslib_0.3.1     rmarkdown_2.13 \n## [17] tools_4.1.3     stringr_1.4.0   xfun_0.30       yaml_2.3.5     \n## [21] fastmap_1.1.0   compiler_4.1.3  memoise_2.0.1   htmltools_0.5.2\n## [25] downlit_0.4.0   knitr_1.37      sass_0.4.0"},{"path":"demo.html","id":"demo","chapter":"1 R as GIS: Demonstrations","heading":"1 R as GIS: Demonstrations","text":"","code":""},{"path":"demo.html","id":"before-you-start","chapter":"1 R as GIS: Demonstrations","heading":"Before you start","text":"primary objective chapter showcase power R GIS demonstrations using mock-econometric research projects7. project consists project overview (objective, datasets used, econometric model, GIS tasks involved) demonstration. really place learn nuts bolts R spatial operations. Indeed, intentionally explain details R codes work. reiterate main purpose demonstrations get better idea R can used process spatial data help research projects involving spatial datasets. Finally, note mock-projects use extremely simple econometric models completely lacks careful thoughts need real research projects. , don’t waste time judging econometric models, just focus GIS tasks. familiar html documents generated rmarkdown, might benefit reading conventions book Preface. Finally, interested replicating demonstrations, directions replication provided . However, suggest focusing narratives first time around, learn nuts bolts spatial operations Chapters 2 5, come back replicate .","code":""},{"path":"demo.html","id":"target-audience","chapter":"1 R as GIS: Demonstrations","heading":"Target Audience","text":"target audience chapter familiar R GIS. Knowledge R certainly helps. , tried write way R beginners can still understand power R GIS8. get bogged complex-looking R codes. Just focus narratives figures get sense R can .","code":""},{"path":"demo.html","id":"direction-for-replication","chapter":"1 R as GIS: Demonstrations","heading":"Direction for replication","text":"DatasetsRunning codes chapter involves reading datasets disk. datasets imported available . chapter, path files set relative working directory (hidden). run codes without mess paths files, follow steps:9set folder (folder) working directory using setwd()create folder called “Data” inside folder designated working directorydownload pertinent datasets hereplace files downloaded folder “Data” folder","code":""},{"path":"demo.html","id":"Demo1","chapter":"1 R as GIS: Demonstrations","heading":"1.1 Demonstration 1: The impact of groundwater pumping on depth to water table","text":"","code":""},{"path":"demo.html","id":"project-overview","chapter":"1 R as GIS: Demonstrations","heading":"1.1.1 Project Overview","text":"Objective:Understand impact groundwater pumping groundwater level.DatasetsGroundwater pumping irrigation wells Chase, Dundy, Perkins Counties southwest corner NebraskaGroundwater levels observed USGS monitoring wells located three counties retrieved National Water Information System (NWIS) maintained USGS using dataRetrieval package.Econometric ModelIn order achieve project objective, estimate following model:\\[\n y_{,t} - y_{,t-1} = \\alpha + \\beta gw_{,t-1} + v\n\\]\\(y_{,t}\\) depth groundwater table10 March11 year \\(t\\) USGS monitoring well \\(\\), \\(gw_{,t-1}\\) total amount groundwater pumping happened within 2-mile radius monitoring well \\(\\).GIS tasksread ESRI shape file sf (spatial) object\nuse sf::st_read()\nuse sf::st_read()download depth water table data using dataRetrieval package developed USGS\nuse dataRetrieval::readNWISdata() dataRetrieval::readNWISsite()\nuse dataRetrieval::readNWISdata() dataRetrieval::readNWISsite()create buffer around USGS monitoring wells\nuse sf::st_buffer()\nuse sf::st_buffer()convert regular data.frame (non-spatial) geographic coordinates sf (spatial) objects\nuse sf::st_as_sf() sf::st_set_crs()\nuse sf::st_as_sf() sf::st_set_crs()reproject sf object another CRS\nuse sf::st_transform()\nuse sf::st_transform()identify irrigation wells located inside buffers calculate total pumping\nuse sf::st_join()\nuse sf::st_join()create maps\nuse tmap package\nuse tmap packagePreparation replicationRun following code install load (already installed) pacman package, install load (already installed) listed package inside pacman::p_load() function.","code":"\nif (!require(\"pacman\")) install.packages(\"pacman\")\npacman::p_load(\n  sf, # vector data operations\n  dplyr, # data wrangling\n  dataRetrieval, # download USGS NWIS data\n  lubridate, # Date object handling\n  modelsummary, # regression table generation\n  lfe # fast regression with many fixed effects\n)"},{"path":"demo.html","id":"project-demonstration","chapter":"1 R as GIS: Demonstrations","heading":"1.1.2 Project Demonstration","text":"geographic focus project southwest corner Nebraska consisting Chase, Dundy, Perkins County (see Figure 1.1 locations within Nebraska). Let’s read shape file three counties represented polygons. use later spatially filter groundwater level data downloaded NWIS.\nFigure 1.1: location Chase, Dundy, Perkins County Nebraska\nalready collected groundwater pumping data, let’s import .well_id unique irrigation well identifier, vol_af amount groundwater pumped acre-feet. dataset just regular data.frame coordinates. need convert dataset object class sf can later identify irrigation wells located within 2-mile radius USGS monitoring wells (see Figure 1.2 spatial distribution irrigation wells).\nFigure 1.2: Spatial distribution irrigation wells\nrest steps take create regression-ready dataset analysis.download groundwater level data observed USGS monitoring wells National Water Information System (NWIS) using dataRetrieval packageidentify irrigation wells located within 2-mile radius USGS wells calculate total groundwater pumping occurred around USGS wells yearmerge groundwater pumping data groundwater level dataLet’s download groundwater level data NWIS first. following code downloads groundwater level data Nebraska Jan 1, 1990, Jan 1, 2016.site_no unique monitoring well identifier, date date groundwater level monitoring, dwt depth water table.calculate average groundwater level March USGS monitoring well (right irrigation season starts):12Since NE_gwl missing geographic coordinates monitoring wells, download using readNWISsite() function select monitoring wells inside three counties.now identify irrigation wells located within 2-mile radius monitoring wells13. first create polygons 2-mile radius circles around monitoring wells (see Figure 1.3).\nFigure 1.3: 2-mile buffers around USGS monitoring wells\nnow identify irrigation wells inside buffers get associated groundwater pumping values. st_join() function sf package trick.Let’s take look USGS monitoring well (site_no = \\(400012101323401\\)).can see, well seven irrigation wells within 2-mile radius 2010.Now, get total nearby pumping monitoring well year.now merge nearby pumping data groundwater level data, transform data obtain dataset ready regression analysis.Finally, estimate model using feols() fixest package (see introduction).regression result.","code":"\nthree_counties <-\n  st_read(dsn = \"Data\", layer = \"urnrd\") %>%\n  #--- project to WGS84/UTM 14N ---#\n  st_transform(32614)Reading layer `urnrd' from data source \n  `/Users/tmieno2/Dropbox/TeachingUNL/R_as_GIS/Data' using driver `ESRI Shapefile'\nSimple feature collection with 3 features and 1 field\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: -102.0518 ymin: 40.00257 xmax: -101.248 ymax: 41.00395\nGeodetic CRS:  NAD83\n#--- groundwater pumping data ---#\n(\n  urnrd_gw <- readRDS(\"Data/urnrd_gw_pumping.rds\")\n)       well_id year  vol_af      lon     lat\n    1:    1706 2007 182.566 245322.3 4542717\n    2:    2116 2007  46.328 245620.9 4541125\n    3:    2583 2007  38.380 245660.9 4542523\n    4:    2597 2007  70.133 244816.2 4541143\n    5:    3143 2007 135.870 243614.0 4541579\n   ---                                      \n18668:    2006 2012 148.713 284782.5 4432317\n18669:    2538 2012 115.567 284462.6 4432331\n18670:    2834 2012  15.766 283338.0 4431341\n18671:    2834 2012 381.622 283740.4 4431329\n18672:    4983 2012      NA 284636.0 4432725\nurnrd_gw_sf <-\n  urnrd_gw %>%\n  #--- convert to sf ---#\n  st_as_sf(coords = c(\"lon\", \"lat\")) %>%\n  #--- set CRS WGS UTM 14 (you need to know the CRS of the coordinates to do this) ---#\n  st_set_crs(32614)\n\n#--- now sf ---#\nurnrd_gw_sfSimple feature collection with 18672 features and 3 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 239959 ymin: 4431329 xmax: 310414.4 ymax: 4543146\nProjected CRS: WGS 84 / UTM zone 14N\nFirst 10 features:\n   well_id year  vol_af                 geometry\n1     1706 2007 182.566 POINT (245322.3 4542717)\n2     2116 2007  46.328 POINT (245620.9 4541125)\n3     2583 2007  38.380 POINT (245660.9 4542523)\n4     2597 2007  70.133 POINT (244816.2 4541143)\n5     3143 2007 135.870   POINT (243614 4541579)\n6     5017 2007 196.799 POINT (243539.9 4543146)\n7     1706 2008 171.250 POINT (245322.3 4542717)\n8     2116 2008 171.650 POINT (245620.9 4541125)\n9     2583 2008  46.100 POINT (245660.9 4542523)\n10    2597 2008 124.830 POINT (244816.2 4541143)\n#--- download groundwater level data ---#\nNE_gwl <-\n  readNWISdata(\n    stateCd = \"Nebraska\",\n    startDate = \"1990-01-01\",\n    endDate = \"2016-01-01\",\n    service = \"gwlevels\"\n  ) %>%\n  dplyr::select(site_no, lev_dt, lev_va) %>%\n  rename(date = lev_dt, dwt = lev_va)\n\n#--- take a look ---#\nhead(NE_gwl, 10)           site_no       date   dwt\n1  400008097545301 2000-11-08 17.40\n2  400008097545301 2008-10-09 13.99\n3  400008097545301 2009-04-09 11.32\n4  400008097545301 2009-10-06 15.54\n5  400008097545301 2010-04-12 11.15\n6  400008100050501 1990-03-15 24.80\n7  400008100050501 1990-10-04 27.20\n8  400008100050501 1991-03-08 24.20\n9  400008100050501 1991-10-07 26.90\n10 400008100050501 1992-03-02 24.70\n#--- Average depth to water table in March ---#\nNE_gwl_march <-\n  NE_gwl %>%\n  mutate(\n    date = as.Date(date),\n    month = month(date),\n    year = year(date),\n  ) %>%\n  #--- select observation in March ---#\n  filter(year >= 2007, month == 3) %>%\n  #--- gwl average in March ---#\n  group_by(site_no, year) %>%\n  summarize(dwt = mean(dwt))\n\n#--- take a look ---#\nhead(NE_gwl_march, 10)# A tibble: 10 × 3\n# Groups:   site_no [2]\n   site_no          year   dwt\n   <chr>           <dbl> <dbl>\n 1 400032101022901  2008 118. \n 2 400032101022901  2009 117. \n 3 400032101022901  2010 118. \n 4 400032101022901  2011 118. \n 5 400032101022901  2012 118. \n 6 400032101022901  2013 118. \n 7 400032101022901  2014 116. \n 8 400032101022901  2015 117. \n 9 400038099244601  2007  24.3\n10 400038099244601  2008  21.7\n#--- get the list of site ids ---#\nNE_site_ls <- NE_gwl$site_no %>% unique()\n\n#--- get the locations of the site ids ---#\nsites_info <-\n  readNWISsite(siteNumbers = NE_site_ls) %>%\n  dplyr::select(site_no, dec_lat_va, dec_long_va) %>%\n  #--- turn the data into an sf object ---#\n  st_as_sf(coords = c(\"dec_long_va\", \"dec_lat_va\")) %>%\n  #--- NAD 83 ---#\n  st_set_crs(4269) %>%\n  #--- project to WGS UTM 14 ---#\n  st_transform(32614) %>%\n  #--- keep only those located inside the three counties ---#\n  .[three_counties, ]\nbuffers <- st_buffer(sites_info, dist = 2 * 1609.34) # in meter\n#--- find irrigation wells inside the buffer and calculate total pumping  ---#\npumping_nearby <- st_join(buffers, urnrd_gw_sf)\nfilter(pumping_nearby, site_no == 400012101323401, year == 2010)Simple feature collection with 7 features and 4 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 279690.7 ymin: 4428006 xmax: 286128 ymax: 4434444\nProjected CRS: WGS 84 / UTM zone 14N\n             site_no well_id year  vol_af                       geometry\n9.3  400012101323401    6331 2010      NA POLYGON ((286128 4431225, 2...\n9.24 400012101323401    1883 2010 180.189 POLYGON ((286128 4431225, 2...\n9.25 400012101323401    2006 2010  79.201 POLYGON ((286128 4431225, 2...\n9.26 400012101323401    2538 2010  68.205 POLYGON ((286128 4431225, 2...\n9.27 400012101323401    2834 2010      NA POLYGON ((286128 4431225, 2...\n9.28 400012101323401    2834 2010 122.981 POLYGON ((286128 4431225, 2...\n9.29 400012101323401    4983 2010      NA POLYGON ((286128 4431225, 2...\n(\n  total_pumping_nearby <-\n    pumping_nearby %>%\n    #--- calculate total pumping by monitoring well ---#\n    group_by(site_no, year) %>%\n    summarize(nearby_pumping = sum(vol_af, na.rm = TRUE)) %>%\n    #--- NA means 0 pumping ---#\n    mutate(\n      nearby_pumping = ifelse(is.na(nearby_pumping), 0, nearby_pumping)\n    )\n)Simple feature collection with 2396 features and 3 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 237904.5 ymin: 4428006 xmax: 313476.5 ymax: 4545687\nProjected CRS: WGS 84 / UTM zone 14N\n# A tibble: 2,396 × 4\n# Groups:   site_no [401]\n   site_no          year nearby_pumping                                 geometry\n * <chr>           <int>          <dbl>                            <POLYGON [m]>\n 1 400012101323401  2007           571. ((286128 4431225, 286123.6 4431057, 286…\n 2 400012101323401  2008           772. ((286128 4431225, 286123.6 4431057, 286…\n 3 400012101323401  2009           500. ((286128 4431225, 286123.6 4431057, 286…\n 4 400012101323401  2010           451. ((286128 4431225, 286123.6 4431057, 286…\n 5 400012101323401  2011           545. ((286128 4431225, 286123.6 4431057, 286…\n 6 400012101323401  2012          1028. ((286128 4431225, 286123.6 4431057, 286…\n 7 400130101374401  2007           485. ((278847.4 4433844, 278843 4433675, 278…\n 8 400130101374401  2008           515. ((278847.4 4433844, 278843 4433675, 278…\n 9 400130101374401  2009           351. ((278847.4 4433844, 278843 4433675, 278…\n10 400130101374401  2010           374. ((278847.4 4433844, 278843 4433675, 278…\n# … with 2,386 more rows\n#--- regression-ready data ---#\nreg_data <-\n  NE_gwl_march %>%\n  #--- pick monitoring wells that are inside the three counties ---#\n  filter(site_no %in% unique(sites_info$site_no)) %>%\n  #--- merge with the nearby pumping data ---#\n  left_join(., total_pumping_nearby, by = c(\"site_no\", \"year\")) %>%\n  #--- lead depth to water table ---#\n  arrange(site_no, year) %>%\n  group_by(site_no) %>%\n  mutate(\n    #--- lead depth ---#\n    dwt_lead1 = dplyr::lead(dwt, n = 1, default = NA, order_by = year),\n    #--- first order difference in dwt  ---#\n    dwt_dif = dwt_lead1 - dwt\n  )\n\n#--- take a look ---#\ndplyr::select(reg_data, site_no, year, dwt_dif, nearby_pumping)# A tibble: 2,022 × 4\n# Groups:   site_no [230]\n   site_no          year dwt_dif nearby_pumping\n   <chr>           <dbl>   <dbl>          <dbl>\n 1 400130101374401  2011  NA               358.\n 2 400134101483501  2007   2.87           2038.\n 3 400134101483501  2008   0.780          2320.\n 4 400134101483501  2009  -2.45           2096.\n 5 400134101483501  2010   3.97           2432.\n 6 400134101483501  2011   1.84           2634.\n 7 400134101483501  2012  -1.35            985.\n 8 400134101483501  2013  44.8              NA \n 9 400134101483501  2014 -26.7              NA \n10 400134101483501  2015  NA                NA \n# … with 2,012 more rows\n#--- OLS with site_no and year FEs (error clustered by site_no) ---#\nreg_dwt <-\n  feols(\n    dwt_dif ~ nearby_pumping | site_no + year,\n    cluster = \"site_no\",\n    data = reg_data\n  )\nmodelsummary(\n  reg_dwt,\n  stars = TRUE,\n  gof_omit = \"IC|Log|Adj|Within|Pseudo\"\n)"},{"path":"demo.html","id":"demonstration-2-precision-agriculture","chapter":"1 R as GIS: Demonstrations","heading":"1.2 Demonstration 2: Precision Agriculture","text":"","code":""},{"path":"demo.html","id":"project-overview-1","chapter":"1 R as GIS: Demonstrations","heading":"1.2.1 Project Overview","text":"Objectives:Understand impact nitrogen corn yieldUnderstand electric conductivity (EC) affects marginal impact nitrogen cornDatasets:experimental design -farm randomized nitrogen trail 80-acre fieldData generated experiment\n-applied nitrogen rate\nYield measures\n-applied nitrogen rateYield measuresElectric conductivityEconometric Model:econometric model, like estimate:\\[\nyield_i = \\beta_0 + \\beta_1 N_i + \\beta_2 N_i^2 + \\beta_3 N_i \\cdot EC_i + \\beta_4 N_i^2 \\cdot EC_i + v_i\n\\]\\(yield_i\\), \\(N_i\\), \\(EC_i\\), \\(v_i\\) corn yield, nitrogen rate, EC, error term subplot \\(\\). Subplots obtained dividing experimental plots six equal-area compartments.GIS tasksread spatial data various formats: R data set (rds), shape file, GeoPackage file\nuse sf::st_read()\nuse sf::st_read()create maps using ggplot2 package\nuse ggplot2::geom_sf()\nuse ggplot2::geom_sf()create subplots within experimental plots\nuser-defined function makes use st_geometry()\nuser-defined function makes use st_geometry()identify corn yield, -applied nitrogen, electric conductivity (EC) data points within experimental plots find averages\nuse sf::st_join() sf::aggregate()\nuse sf::st_join() sf::aggregate()Preparation replicationRun following code install load (already installed) pacman package, install load (already installed) listed package inside pacman::p_load() function.Run following code define theme map:","code":"\nif (!require(\"pacman\")) install.packages(\"pacman\")\npacman::p_load(\n  sf, # vector data operations\n  dplyr, # data wrangling\n  ggplot2, # for map creation\n  modelsummary, # regression table generation\n  patchwork # arrange multiple plots\n)\ntheme_for_map <-\n  theme(\n    axis.ticks = element_blank(),\n    axis.text = element_blank(),\n    axis.line = element_blank(),\n    panel.border = element_blank(),\n    panel.grid = element_line(color = \"transparent\"),\n    panel.background = element_blank(),\n    plot.background = element_rect(fill = \"transparent\", color = \"transparent\")\n  )"},{"path":"demo.html","id":"project-demonstration-1","chapter":"1 R as GIS: Demonstrations","heading":"1.2.2 Project Demonstration","text":"already run whole-field randomized nitrogen experiment 80-acre field. Let’s import trial design dataFigure 1.4 map trial design generated using ggplot2 package.\nFigure 1.4: Experimental Design Randomize Nitrogen Trial\ncollected yield, -applied NH3, EC data. Let’s read datasets:14Figure 1.5 shows spatial distribution three variables. map variable made first, combined one figure using patchwork package15.\nFigure 1.5: Spatial distribution yield, NH3, EC\nInstead using plot observation unit, like create subplots inside plots make unit analysis avoid masking within-plot spatial heterogeneity EC. , divide plot six subplots.following function generate subplots supplying trial design number subplots like create within plot:Let’s run function create six subplots within experimental plots.Figure 1.6 map subplots generated.\nFigure 1.6: Map subplots\nnow identify mean value corn yield, nitrogen rate, EC subplots using sf::aggregate() sf::st_join().visualization subplot-level data (Figure 1.7):\nFigure 1.7: Spatial distribution subplot-level yield, NH3, EC\nLet’s estimate model see results:","code":"\n#--- read the trial design data ---#\ntrial_design_16 <- readRDS(\"Data/trial_design.rds\")\n#--- map of trial design ---#\nggplot(data = trial_design_16) +\n  geom_sf(aes(fill = factor(NRATE))) +\n  scale_fill_brewer(name = \"N\", palette = \"OrRd\", direction = 1) +\n  theme_for_map\n#--- read yield data (sf data saved as rds) ---#\nyield <- readRDS(\"Data/yield.rds\")\n\n#--- read NH3 data (GeoPackage data) ---#\nNH3_data <- st_read(\"Data/NH3.gpkg\")\n\n#--- read ec data (shape file) ---#\nec <- st_read(dsn = \"Data\", \"ec\")\n#--- yield map ---#\ng_yield <-\n  ggplot() +\n  geom_sf(data = trial_design_16) +\n  geom_sf(data = yield, aes(color = yield), size = 0.5) +\n  scale_color_distiller(name = \"Yield\", palette = \"OrRd\", direction = 1) +\n  theme_for_map\n\n#--- NH3 map ---#\ng_NH3 <-\n  ggplot() +\n  geom_sf(data = trial_design_16) +\n  geom_sf(data = NH3_data, aes(color = aa_NH3), size = 0.5) +\n  scale_color_distiller(name = \"NH3\", palette = \"OrRd\", direction = 1) +\n  theme_for_map\n\n#--- NH3 map ---#\ng_ec <-\n  ggplot() +\n  geom_sf(data = trial_design_16) +\n  geom_sf(data = ec, aes(color = ec), size = 0.5) +\n  scale_color_distiller(name = \"EC\", palette = \"OrRd\", direction = 1) +\n  theme_for_map\n\n#--- stack the figures vertically and display (enabled by the patchwork package) ---#\ng_yield / g_NH3 / g_ec\ngen_subplots <- function(plot, num_sub) {\n\n  #--- extract geometry information ---#\n  geom_mat <- st_geometry(plot)[[1]][[1]]\n\n  #--- upper left ---#\n  top_start <- (geom_mat[2, ])\n\n  #--- upper right ---#\n  top_end <- (geom_mat[3, ])\n\n  #--- lower right ---#\n  bot_start <- (geom_mat[1, ])\n\n  #--- lower left ---#\n  bot_end <- (geom_mat[4, ])\n\n  top_step_vec <- (top_end - top_start) / num_sub\n  bot_step_vec <- (bot_end - bot_start) / num_sub\n\n  # create a list for the sub-grid\n\n  subplots_ls <- list()\n\n  for (j in 1:num_sub) {\n    rec_pt1 <- top_start + (j - 1) * top_step_vec\n    rec_pt2 <- top_start + j * top_step_vec\n    rec_pt3 <- bot_start + j * bot_step_vec\n    rec_pt4 <- bot_start + (j - 1) * bot_step_vec\n\n    rec_j <- rbind(rec_pt1, rec_pt2, rec_pt3, rec_pt4, rec_pt1)\n\n    temp_quater_sf <- list(st_polygon(list(rec_j))) %>%\n      st_sfc(.) %>%\n      st_sf(., crs = 26914)\n\n    subplots_ls[[j]] <- temp_quater_sf\n  }\n\n  return(do.call(\"rbind\", subplots_ls))\n}\n#--- generate subplots ---#\nsubplots <-\n  lapply(\n    1:nrow(trial_design_16),\n    function(x) gen_subplots(trial_design_16[x, ], 6)\n  ) %>%\n  do.call(\"rbind\", .)\n#--- here is what subplots look like ---#\nggplot(subplots) +\n  geom_sf() +\n  theme_for_map\n(\n  reg_data <- subplots %>%\n    #--- yield ---#\n    st_join(., aggregate(yield, ., mean), join = st_equals) %>%\n    #--- nitrogen ---#\n    st_join(., aggregate(NH3_data, ., mean), join = st_equals) %>%\n    #--- EC ---#\n    st_join(., aggregate(ec, ., mean), join = st_equals)\n)Simple feature collection with 816 features and 3 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 560121.3 ymin: 4533410 xmax: 560758.9 ymax: 4533734\nProjected CRS: NAD83 / UTM zone 14N\nFirst 10 features:\n      yield   aa_NH3       ec                       geometry\n1  220.1789 194.5155 28.33750 POLYGON ((560121.3 4533428,...\n2  218.9671 194.4291 29.37667 POLYGON ((560134.5 4533428,...\n3  220.3286 195.2903 30.73600 POLYGON ((560147.7 4533428,...\n4  215.3121 196.7649 32.24000 POLYGON ((560160.9 4533429,...\n5  216.9709 195.2199 36.27000 POLYGON ((560174.1 4533429,...\n6  227.8761 184.6362 31.21000 POLYGON ((560187.3 4533429,...\n7  226.0991 179.2143 31.99250 POLYGON ((560200.5 4533430,...\n8  225.3973 179.0916 31.56500 POLYGON ((560213.7 4533430,...\n9  221.1820 178.9585 33.01000 POLYGON ((560227 4533430, 5...\n10 219.4659 179.0057 41.89750 POLYGON ((560240.2 4533430,...\n(ggplot() +\n  geom_sf(data = reg_data, aes(fill = yield), color = NA) +\n  scale_fill_distiller(name = \"Yield\", palette = \"OrRd\", direction = 1) +\n  theme_for_map) /\n  (ggplot() +\n    geom_sf(data = reg_data, aes(fill = aa_NH3), color = NA) +\n    scale_fill_distiller(name = \"NH3\", palette = \"OrRd\", direction = 1) +\n    theme_for_map) /\n  (ggplot() +\n    geom_sf(data = reg_data, aes(fill = ec), color = NA) +\n    scale_fill_distiller(name = \"EC\", palette = \"OrRd\", direction = 1) +\n    theme_for_map)\nols_res <- lm(yield ~ aa_NH3 + I(aa_NH3^2) + I(aa_NH3 * ec) + I(aa_NH3^2 * ec), data = reg_data)\n\nmodelsummary(\n  ols_res,\n  stars = TRUE,\n  gof_omit = \"IC|Log|Adj|Within|Pseudo\"\n)"},{"path":"demo.html","id":"demo3","chapter":"1 R as GIS: Demonstrations","heading":"1.3 Demonstration 3: Land Use and Weather","text":"","code":""},{"path":"demo.html","id":"project-overview-2","chapter":"1 R as GIS: Demonstrations","heading":"1.3.1 Project Overview","text":"ObjectiveUnderstand impact past precipitation crop choice Iowa (IA).DatasetsIA county boundaryRegular grids IA, created using sf::st_make_grid()PRISM daily precipitation data downloaded using prism packageLand use data Cropland Data Layer (CDL) IA 2015, downloaded using cdlTools packageEconometric ModelThe econometric model like estimate :\\[\n CS_i = \\alpha + \\beta_1 PrN_{} + \\beta_2 PrC_{} + v_i\n\\]\n\\(CS_i\\) area share corn divided soy 2015 grid \\(\\) (generate regularly-sized grids Demo section), \\(PrN_i\\) total precipitation observed April May September 2014, \\(PrC_i\\) total precipitation observed June August 2014, \\(v_i\\) error term. run econometric model, need find crop share weather variables observed grids. first tackle crop share variable, precipitation variable.GIS tasksdownload Cropland Data Layer (CDL) data USDA NASS\nuse cdlTools::getCDL()\nuse cdlTools::getCDL()download PRISM weather data\nuse prism::get_prism_dailys()\nuse prism::get_prism_dailys()crop PRISM data geographic extent IA\nuse raster::crop()\nuse raster::crop()create regular grids within IA, become observation units econometric analysis\nuse sf::st_make_grid()\nuse sf::st_make_grid()remove grids share small area IA\nuse sf::st_intersection() sf::st_area\nuse sf::st_intersection() sf::st_areaassign crop share weather data generated IA grids (parallelized)\nuse exactextractr::exact_extract() future.apply::future_lapply()\nuse exactextractr::exact_extract() future.apply::future_lapply()create maps\nuse tmap package\nuse tmap packagePreparation replicationRun following code install load (already installed) pacman package, install load (already installed) listed package inside pacman::p_load() function.","code":"\nif (!require(\"pacman\")) install.packages(\"pacman\")\npacman::p_load(\n  sf, # vector data operations\n  raster, # raster data operations\n  exactextractr, # fast raster data extraction for polygons\n  maps, # to get county boundary data\n  data.table, # data wrangling\n  dplyr, # data wrangling\n  lubridate, # Date object handling\n  tmap, # for map creation\n  modelsummary, # regression table generation\n  future.apply, # parallel computation\n  cdlTools, # download CDL data\n  rgdal, # required for cdlTools\n  prism, # download PRISM data\n  stringr # string manipulation\n)"},{"path":"demo.html","id":"project-demonstration-2","chapter":"1 R as GIS: Demonstrations","heading":"1.3.2 Project Demonstration","text":"geographic focus project Iowas. Let’s get Iowa state border (see Figure 1.8 map).\nFigure 1.8: Iowa state boundary\nunit analysis artificial grids create Iowa. grids regularly-sized rectangles except around edge Iowa state border16. , let’s create grids remove overlap much Iowa.generated grids look like (Figure 1.9):\nFigure 1.9: Map regular grids generated IA\nLet’s work crop share data. can download CDL data using getCDL() function cdlTools package.cells (30 meter 30 meter) imported raster layer take value ranging 0 255. Corn soybean represented 1 5, respectively (Figure 1.10).Figure 1.10 shows map one IA grids CDL cells overlaps .\nFigure 1.10: Spatial overlap IA grid CDL layer\nlike extract cell values within blue border.use exactextractr::exact_extract() identify cells CDL raster layer fall within IA grids extract land use type values. find share corn soybean grids.find corn soy ratio IA grids.still missing daily precipitation data moment. decided use daily weather data PRISM. Daily PRISM data raster data cell size 4 km 4 km. Figure 1.11 presents precipitation data downloaded April 1, 2010. covers entire contiguous U.S.\nFigure 1.11: Map PRISM raster data layer\nLet’s now download PRISM data (run code get data. included data folder replication ). can done using get_prism_dailys() function prism package.17When use get_prism_dailys() download data18, creates one folder day. , 180 folders inside folder designated download destination options() function.now try extract precipitation value day IA grids geographically overlaying IA grids onto PRISM data layer identify PRISM cells IA grid encompass. Figure 1.12 shows first IA grid overlaps PRISM cells19.\nFigure 1.12: Spatial overlap IA grid PRISM cells\ncan see, PRISM grids fully inside analysis grid, others partially inside . , assigning precipitation values grids, use coverage-weighted mean precipitations20.Unlike CDL layer, 183 raster layers process. Fortunately, can process many raster files time quickly first “stacking” many raster files first applying exact_extract() function. Using future_lapply(), let \\(6\\) cores take care task processing 31 files, except one handling 28 files.21\nfirst get paths PRISM files.now prepare parallelized extractions implement using future_apply() (can look Chapter familiarize parallel computation using future.apply package).function run parallel 6 cores.Now, let’s run function parallel calculate precipitation period.now grid-level crop share precipitation data.Let’s merge run regression.22Here regression results table., read results econometric model terrible.","code":"\n#--- IA state boundary ---#\nIA_boundary <- st_as_sf(maps::map(\"state\", \"iowa\", plot = FALSE, fill = TRUE))\n#--- create regular grids (40 cells by 40 columns) over IA ---#\nIA_grids <-\n  IA_boundary %>%\n  #--- create grids ---#\n  st_make_grid(, n = c(40, 40)) %>%\n  #--- convert to sf ---#\n  st_as_sf() %>%\n  #--- find the intersections of IA grids and IA polygon ---#\n  st_intersection(., IA_boundary) %>%\n  #--- make some of the invalid polygons valid ---#\n  st_make_valid() %>%\n  #--- calculate the area of each grid ---#\n  mutate(\n    area = as.numeric(st_area(.)),\n    area_ratio = area / max(area)\n  ) %>%\n  #--- keep only if the intersected area is large enough ---#\n  filter(area_ratio > 0.8) %>%\n  #--- assign grid id for future merge ---#\n  mutate(grid_id = 1:nrow(.))\n#--- plot the grids over the IA state border ---#\ntm_shape(IA_boundary) +\n  tm_polygons(col = \"green\") +\n  tm_shape(IA_grids) +\n  tm_polygons(alpha = 0) +\n  tm_layout(frame = FALSE)\n#--- download the CDL data for IA in 2015 ---#\n(\n  IA_cdl_2015 <- getCDL(\"Iowa\", 2015)$IA2015\n)\n#--- reproject grids to the CRS of the CDL data ---#\nIA_grids_rp_cdl <- st_transform(IA_grids, projection(IA_cdl_2015))\n\n#--- extract crop type values and find frequencies ---#\ncdl_extracted <-\n  exact_extract(IA_cdl_2015, IA_grids_rp_cdl) %>%\n  lapply(., function(x) data.table(x)[, .N, by = value]) %>%\n  #--- combine the list of data.tables into one data.table ---#\n  rbindlist(idcol = TRUE) %>%\n  #--- find the share of each land use type ---#\n  .[, share := N / sum(N), by = .id] %>%\n  .[, N := NULL] %>%\n  #--- keep only the share of corn and soy ---#\n  .[value %in% c(1, 5), ]\n#--- find corn/soy ratio ---#\ncorn_soy <-\n  cdl_extracted %>%\n  #--- long to wide ---#\n  dcast(.id ~ value, value.var = \"share\") %>%\n  #--- change variable names ---#\n  setnames(c(\".id\", \"1\", \"5\"), c(\"grid_id\", \"corn_share\", \"soy_share\")) %>%\n  #--- corn share divided by soy share ---#\n  .[, c_s_ratio := corn_share / soy_share]\noptions(prism.path = \"Data/PRISM\")\n\nget_prism_dailys(\n  type = \"ppt\",\n  minDate = \"2014-04-01\",\n  maxDate = \"2014-09-30\",\n  keepZip = FALSE\n)\n#--- read a PRISM dataset ---#\nprism_whole <- raster(\"Data/PRISM/PRISM_ppt_stable_4kmD2_20140401_bil/PRISM_ppt_stable_4kmD2_20140401_bil.bil\")\n\n#--- align the CRS ---#\nIA_grids_rp_prism <- st_transform(IA_grids, projection(prism_whole))\n\n#--- crop the PRISM data for the 1st IA grid ---#\nPRISM_1 <- raster::crop(prism_whole, st_buffer(IA_grids_rp_prism[1, ], dist = 2000))\n\n#--- map them ---#\ntm_shape(PRISM_1) +\n  tm_raster() +\n  tm_shape(IA_grids_rp_prism[1, ]) +\n  tm_borders(col = \"blue\") +\n  tm_layout(frame = NA)\n#--- get all the dates ---#\ndates_ls <- seq(as.Date(\"2014-04-01\"), as.Date(\"2014-09-30\"), \"days\")\n\n#--- remove hyphen ---#\ndates_ls_no_hyphen <- str_remove_all(dates_ls, \"-\")\n\n#--- get all the prism file names ---#\nfolder_name <- paste0(\"PRISM_ppt_stable_4kmD2_\", dates_ls_no_hyphen, \"_bil\")\nfile_name <- paste0(\"PRISM_ppt_stable_4kmD2_\", dates_ls_no_hyphen, \"_bil.bil\")\nfile_paths <- paste0(\"Data/PRISM/\", folder_name, \"/\", file_name)\n\n#--- take a look ---#\nhead(file_paths)[1] \"Data/PRISM/PRISM_ppt_stable_4kmD2_20140401_bil/PRISM_ppt_stable_4kmD2_20140401_bil.bil\"\n[2] \"Data/PRISM/PRISM_ppt_stable_4kmD2_20140402_bil/PRISM_ppt_stable_4kmD2_20140402_bil.bil\"\n[3] \"Data/PRISM/PRISM_ppt_stable_4kmD2_20140403_bil/PRISM_ppt_stable_4kmD2_20140403_bil.bil\"\n[4] \"Data/PRISM/PRISM_ppt_stable_4kmD2_20140404_bil/PRISM_ppt_stable_4kmD2_20140404_bil.bil\"\n[5] \"Data/PRISM/PRISM_ppt_stable_4kmD2_20140405_bil/PRISM_ppt_stable_4kmD2_20140405_bil.bil\"\n[6] \"Data/PRISM/PRISM_ppt_stable_4kmD2_20140406_bil/PRISM_ppt_stable_4kmD2_20140406_bil.bil\"\n#--- define the number of cores to use ---#\nnum_core <- 6\n\n#--- prepare some parameters for parallelization ---#\nfile_len <- length(file_paths)\nfiles_per_core <- ceiling(file_len / num_core)\n\n#--- prepare for parallel processing ---#\nplan(multiprocess, workers = num_core)\n\n#--- reproject IA grids to the CRS of PRISM data ---#\nIA_grids_reprojected <- st_transform(IA_grids, projection(prism_whole))\n#--- define the function to extract PRISM values by block of files ---#\nextract_by_block <- function(i, files_per_core) {\n\n  #--- files processed by core  ---#\n  start_file_index <- (i - 1) * files_per_core + 1\n\n  #--- indexes for files to process ---#\n  file_index <- seq(\n    from = start_file_index,\n    to = min((start_file_index + files_per_core), file_len),\n    by = 1\n  )\n\n  #--- extract values ---#\n  data_temp <- file_paths[file_index] %>% # get file names\n    #--- stack files ---#\n    stack() %>%\n    #--- extract ---#\n    exact_extract(., IA_grids_reprojected) %>%\n    #--- combine into one data set ---#\n    rbindlist(idcol = \"ID\") %>%\n    #--- wide to long ---#\n    melt(id.var = c(\"ID\", \"coverage_fraction\")) %>%\n    #--- calculate \"area\"-weighted mean ---#\n    .[, .(value = sum(value * coverage_fraction) / sum(coverage_fraction)), by = .(ID, variable)]\n\n  return(data_temp)\n}\n#--- run the function ---#\nprecip_by_period <-\n  future_lapply(\n    1:num_core,\n    function(x) extract_by_block(x, files_per_core)\n  ) %>%\n  rbindlist() %>%\n  #--- recover the date ---#\n  .[, variable := as.Date(str_extract(variable, \"[0-9]{8}\"), \"%Y%m%d\")] %>%\n  #--- change the variable name to date ---#\n  setnames(\"variable\", \"date\") %>%\n  #--- define critical period ---#\n  .[, critical := \"non_critical\"] %>%\n  .[month(date) %in% 6:8, critical := \"critical\"] %>%\n  #--- total precipitation by critical dummy  ---#\n  .[, .(precip = sum(value)), by = .(ID, critical)] %>%\n  #--- wide to long ---#\n  dcast(ID ~ critical, value.var = \"precip\")\n#--- crop share ---#\nreg_data <- corn_soy[precip_by_period, on = c(grid_id = \"ID\")]\n\n#--- OLS ---#\nreg_results <- lm(c_s_ratio ~ critical + non_critical, data = reg_data)\n#--- regression table ---#\nmodelsummary(\n  reg_results,\n  stars = TRUE,\n  gof_omit = \"IC|Log|Adj|Within|Pseudo\"\n)"},{"path":"demo.html","id":"demo4","chapter":"1 R as GIS: Demonstrations","heading":"1.4 Demonstration 4: The Impact of Railroad Presence on Corn Planted Acreage","text":"","code":""},{"path":"demo.html","id":"project-overview-3","chapter":"1 R as GIS: Demonstrations","heading":"1.4.1 Project Overview","text":"ObjectiveUnderstand impact railroad corn planted acreage IllinoisDatasetsUSDA corn planted acreage Illinois downloaded USDA NationalAgricultural Statistics Service (NASS) QuickStats service using tidyUSDA packageUS railroads (line data) downloaded hereEconometric ModelWe estimate following model:\\[\n  y_i = \\beta_0 + \\beta_1 RL_i + v_i\n\\]\\(y_i\\) corn planted acreage county \\(\\) Illinois, \\(RL_i\\) total length railroad, \\(v_i\\) error term.GIS tasksDownload USDA corn planted acreage county spatial dataset (sf object)\nuse tidyUSDA::getQuickStat()\nuse tidyUSDA::getQuickStat()Import US railroad shape file spatial dataset (sf object)\nuse sf:st_read()\nuse sf:st_read()Spatially subset (crop) railroad data geographic boundary Illinois\nuse sf_1[sf_2, ]\nuse sf_1[sf_2, ]Find railroads county (cross-county railroad chopped pieces fit within single county)\nuse sf::st_intersection()\nuse sf::st_intersection()Calculate travel distance railroad piece\nuse sf::st_length()\nuse sf::st_length()create maps using ggplot2 package\nuse ggplot2::geom_sf()\nuse ggplot2::geom_sf()Preparation replicationRun following code install load (already installed) pacman package, install load (already installed) listed package inside pacman::p_load() function.Run following code define theme map:","code":"\nif (!require(\"pacman\")) install.packages(\"pacman\")\npacman::p_load(\n  tidyUSDA, # access USDA NASS data\n  sf, # vector data operations\n  dplyr, # data wrangling\n  ggplot2, # for map creation\n  modelsummary, # regression table generation\n  keyring # API management\n)\ntheme_for_map <-\n  theme(\n    axis.ticks = element_blank(),\n    axis.text = element_blank(),\n    axis.line = element_blank(),\n    panel.border = element_blank(),\n    panel.grid.major = element_line(color = \"transparent\"),\n    panel.grid.minor = element_line(color = \"transparent\"),\n    panel.background = element_blank(),\n    plot.background = element_rect(fill = \"transparent\", color = \"transparent\")\n  )"},{"path":"demo.html","id":"project-demonstration-3","chapter":"1 R as GIS: Demonstrations","heading":"1.4.2 Project Demonstration","text":"first download corn planted acreage data 2018 USDA NASS QuickStat service using tidyUSDA package23.nice thing function data downloaded sf object county geometry geometry = TRUE. , can immediately plot (Figure 1.13) use later spatial interactions without merge downloaded data independent county boundary data.\nFigure 1.13: Map Con Planted Acreage Illinois 2018\nLet’s import U.S. railroad data reproject CRS IL_corn_planted:looks like:\nFigure 1.14: Map Railroads\nnow crop Illinois state border (Figure 1.15):\nFigure 1.15: Map railroads Illinois\nLet’s now find railroads county, cross-county railroads chopped pieces piece fits completely within single county, using st_intersection().railroads Richland County:now calculate travel distance (Great-circle distance) railroad piece using st_length() sum county find total railroad length county.merge railroad length data corn planted acreage data estimate model.","code":"\n(\n  IL_corn_planted <-\n    getQuickstat(\n      #--- use your own API key here fore replication ---#\n      key = key_get(\"usda_nass_qs_api\"),\n      program = \"SURVEY\",\n      data_item = \"CORN - ACRES PLANTED\",\n      geographic_level = \"COUNTY\",\n      state = \"ILLINOIS\",\n      year = \"2018\",\n      geometry = TRUE\n    ) %>%\n    #--- keep only some of the variables ---#\n    dplyr::select(year, NAME, county_code, short_desc, Value)\n)Simple feature collection with 90 features and 5 fields (with 6 geometries empty)\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -91.51308 ymin: 36.9703 xmax: -87.4952 ymax: 42.50848\nGeodetic CRS:  NAD83\nFirst 10 features:\n   year        NAME county_code           short_desc  Value\n1  2018      Bureau         011 CORN - ACRES PLANTED 264000\n2  2018     Carroll         015 CORN - ACRES PLANTED 134000\n3  2018       Henry         073 CORN - ACRES PLANTED 226500\n4  2018  Jo Daviess         085 CORN - ACRES PLANTED  98500\n5  2018         Lee         103 CORN - ACRES PLANTED 236500\n6  2018      Mercer         131 CORN - ACRES PLANTED 141000\n7  2018        Ogle         141 CORN - ACRES PLANTED 217000\n8  2018      Putnam         155 CORN - ACRES PLANTED  32300\n9  2018 Rock Island         161 CORN - ACRES PLANTED  68400\n10 2018  Stephenson         177 CORN - ACRES PLANTED 166500\n                         geometry\n1  MULTIPOLYGON (((-89.85691 4...\n2  MULTIPOLYGON (((-90.16133 4...\n3  MULTIPOLYGON (((-90.43247 4...\n4  MULTIPOLYGON (((-90.50668 4...\n5  MULTIPOLYGON (((-89.63118 4...\n6  MULTIPOLYGON (((-90.99255 4...\n7  MULTIPOLYGON (((-89.68598 4...\n8  MULTIPOLYGON (((-89.33303 4...\n9  MULTIPOLYGON (((-90.33573 4...\n10 MULTIPOLYGON (((-89.92577 4...\nggplot(IL_corn_planted) +\n  geom_sf(aes(fill = Value / 1000)) +\n  scale_fill_distiller(name = \"Planted Acreage (1000 acres)\", palette = \"YlOrRd\", trans = \"reverse\") +\n  theme(legend.position = \"bottom\") +\n  theme_for_map\nrail_roads <-\n  st_read(dsn = \"Data/\", layer = \"tl_2015_us_rails\") %>%\n  #--- reproject to the CRS of IL_corn_planted ---#\n  st_transform(st_crs(IL_corn_planted))Reading layer `tl_2015_us_rails' from data source \n  `/Users/tmieno2/Dropbox/TeachingUNL/R_as_GIS/Data' using driver `ESRI Shapefile'\nSimple feature collection with 180958 features and 3 fields\nGeometry type: MULTILINESTRING\nDimension:     XY\nBounding box:  xmin: -165.4011 ymin: 17.95174 xmax: -65.74931 ymax: 65.00006\nGeodetic CRS:  NAD83\nggplot(rail_roads) +\n  geom_sf() +\n  theme_for_map\nrail_roads_IL <- rail_roads[IL_corn_planted, ]\nggplot() +\n  geom_sf(data = rail_roads_IL) +\n  theme_for_map\nrails_IL_segmented <- st_intersection(rail_roads_IL, IL_corn_planted)\nggplot() +\n  geom_sf(data = dplyr::filter(IL_corn_planted, NAME == \"Richland\")) +\n  geom_sf(\n    data = dplyr::filter(rails_IL_segmented, NAME == \"Richland\"),\n    aes(color = LINEARID)\n  ) +\n  theme(legend.position = \"bottom\") +\n  theme_for_map\n(\n  rail_length_county <-\n    mutate(\n      rails_IL_segmented,\n      length_in_m = as.numeric(st_length(rails_IL_segmented))\n    ) %>%\n    #--- geometry no longer needed ---#\n    st_drop_geometry() %>%\n    #--- group by county ID ---#\n    group_by(county_code) %>%\n    #--- sum rail length by county ---#\n    summarize(length_in_m = sum(length_in_m))\n)# A tibble: 82 × 2\n   county_code length_in_m\n   <chr>             <dbl>\n 1 001              77189.\n 2 003              77335.\n 3 007              36686.\n 4 011             255173.\n 5 015             161397.\n 6 017              30611.\n 7 019             389061.\n 8 021             155706.\n 9 023              78532.\n10 025              92002.\n# … with 72 more rows\nreg_data <- left_join(IL_corn_planted, rail_length_county, by = \"county_code\")\nlm(Value ~ length_in_m, data = reg_data) %>%\n  modelsummary(\n    stars = TRUE,\n    gof_omit = \"IC|Log|Adj|Within|Pseudo\"\n  )"},{"path":"demo.html","id":"demonstration-5-groundwater-use-for-agricultural-irrigation","chapter":"1 R as GIS: Demonstrations","heading":"1.5 Demonstration 5: Groundwater use for agricultural irrigation","text":"","code":""},{"path":"demo.html","id":"project-overview-4","chapter":"1 R as GIS: Demonstrations","heading":"1.5.1 Project Overview","text":"ObjectiveUnderstand impact monthly precipitation groundwater use agricultural irrigationDatasetsAnnual groundwater pumping irrigation wells Kansas 2010 2011 (originally obtained Water Information Management & Analysis System (WIMAS) database)Daymet24 daily precipitation maximum temperature downloaded using daymetr packageEconometric ModelThe econometric model like estimate :\\[\n   y_{,t}  = \\alpha +  P_{,t} \\beta + T_{,t} \\gamma + \\phi_i + \\eta_t + v_{,t}\n\\]\\(y\\) total groundwater extracted year \\(t\\), \\(P_{,t}\\) \\(T_{,t}\\) collection monthly total precipitation mean maximum temperature April September year \\(t\\), respectively, \\(\\phi_i\\) well fixed effect, \\(\\eta_t\\) year fixed effect, \\(v_{,t}\\) error term.GIS tasksdownload Daymet precipitation maximum temperature data well within R parallel\nuse daymetr::download_daymet() future.apply::future_lapply()\nuse daymetr::download_daymet() future.apply::future_lapply()Preparation replicationRun following code install load (already installed) pacman package, install load (already installed) listed package inside pacman::p_load() function.","code":"\nif (!require(\"pacman\")) install.packages(\"pacman\")\npacman::p_load(\n  daymetr, # get Daymet data\n  sf, # vector data operations\n  dplyr, # data wrangling\n  data.table, # data wrangling\n  ggplot2, # for map creation\n  RhpcBLASctl, # to get the number of available cores\n  future.apply, # parallelization\n  lfe, # fast regression with many fixed effects\n  modelsummary # regression table generation\n)"},{"path":"demo.html","id":"project-demonstration-4","chapter":"1 R as GIS: Demonstrations","heading":"1.5.2 Project Demonstration","text":"already collected annual groundwater pumping data irrigation wells 2010 2011 Kansas Water Information Management & Analysis System (WIMAS) database. Let’s read groundwater use data.28553 wells total, well records groundwater pumping (af_used) years 2010 2011. spatial distribution wells.now need get monthly precipitation maximum temperature data. decided use Daymet weather data. use download_daymet() function daymetr package25 allows us download weather variables specified geographic location time period26. write wrapper function downloads Daymet data processes find monthly total precipitation mean maximum temperature27. loop 56225 wells, parallelized using future_apply() function28 future.apply package. process takes hour Mac parallelization 7 cores. data available data repository course (named “all_daymet.rds”).one run (first well) get_daymet() returnsWe get number cores can use RhpcBLASctl::get_num_procs() parallelize loop wells using future_lapply().29Before merging Daymet data, need reshape data wide format get monthly precipitation maximum temperature columns.Now, let’s merge weather data groundwater pumping dataset.Let’s run regression display results.’s . bother try read regression results. , just illustration R can used prepare regression-ready dataset spatial variables.","code":"\n#--- read in the data ---#\n(\n  gw_KS_sf <- readRDS(\"Data/gw_KS_sf.rds\")\n)Simple feature collection with 56225 features and 3 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: -102.0495 ymin: 36.99561 xmax: -94.70746 ymax: 40.00191\nGeodetic CRS:  NAD83\nFirst 10 features:\n   well_id year   af_used                   geometry\n1        1 2010  67.00000 POINT (-100.4423 37.52046)\n2        1 2011 171.00000 POINT (-100.4423 37.52046)\n3        3 2010  30.93438 POINT (-100.7118 39.91526)\n4        3 2011  12.00000 POINT (-100.7118 39.91526)\n5        7 2010   0.00000 POINT (-101.8995 38.78077)\n6        7 2011   0.00000 POINT (-101.8995 38.78077)\n7       11 2010 154.00000 POINT (-101.7114 39.55035)\n8       11 2011 160.00000 POINT (-101.7114 39.55035)\n9       12 2010  28.17239 POINT (-95.97031 39.16121)\n10      12 2011  89.53479 POINT (-95.97031 39.16121)\nKS_counties <- readRDS(\"Data/KS_county_borders.rds\")\n\ntm_shape(KS_counties) +\n  tm_polygons() +\n  tm_shape(gw_KS_sf) +\n  tm_symbols(size = 0.05, col = \"black\")\n#--- get the geographic coordinates of the wells ---#\nwell_locations <-\n  gw_KS_sf %>%\n  unique(by = \"well_id\") %>%\n  dplyr::select(well_id) %>%\n  cbind(., st_coordinates(.))\n\n#--- define a function that downloads Daymet data by well and process it ---#\nget_daymet <- function(i) {\n  temp_site <- well_locations[i, ]$well_id\n  temp_long <- well_locations[i, ]$X\n  temp_lat <- well_locations[i, ]$Y\n\n  data_temp <-\n    download_daymet(\n      site = temp_site,\n      lat = temp_lat,\n      lon = temp_long,\n      start = 2010,\n      end = 2011,\n      #--- if TRUE, tidy data is returned ---#\n      simplify = TRUE,\n      #--- if TRUE, the downloaded data can be assigned to an R object ---#\n      internal = TRUE\n    ) %>%\n    data.table() %>%\n    #--- keep only precip and tmax ---#\n    .[measurement %in% c(\"prcp..mm.day.\", \"tmax..deg.c.\"), ] %>%\n    #--- recover calender date from Julian day ---#\n    .[, date := as.Date(paste(year, yday, sep = \"-\"), \"%Y-%j\")] %>%\n    #--- get month ---#\n    .[, month := month(date)] %>%\n    #--- keep only April through September ---#\n    .[month %in% 4:9, ] %>%\n    .[, .(site, year, month, date, measurement, value)] %>%\n    #--- long to wide ---#\n    dcast(site + year + month + date ~ measurement, value.var = \"value\") %>%\n    #--- change variable names ---#\n    setnames(c(\"prcp..mm.day.\", \"tmax..deg.c.\"), c(\"prcp\", \"tmax\")) %>%\n    #--- find the total precip and mean tmax by month-year ---#\n    .[, .(prcp = sum(prcp), tmax = mean(tmax)), by = .(month, year)] %>%\n    .[, well_id := temp_site]\n\n  return(data_temp)\n  gc()\n}\n#--- one run ---#\n(\n  returned_data <- get_daymet(1)[]\n)    month year  prcp     tmax well_id\n 1:     4 2010 40.72 20.71700       1\n 2:     5 2010 93.60 24.41677       1\n 3:     6 2010 70.45 32.59933       1\n 4:     7 2010 84.58 33.59903       1\n 5:     8 2010 66.41 34.17323       1\n 6:     9 2010 15.58 31.25800       1\n 7:     4 2011 24.04 21.86367       1\n 8:     5 2011 25.59 26.51097       1\n 9:     6 2011 23.15 35.37533       1\n10:     7 2011 35.10 38.60548       1\n11:     8 2011 36.66 36.94871       1\n12:     9 2011  9.59 28.31800       1\n#--- prepare for parallelization ---#\nnum_cores <- get_num_procs() - 1 # number of cores\nplan(multiprocess, workers = num_cores) # set up cores\n\n#--- run get_daymet with parallelization ---#\n(\n  all_daymet <-\n    future_lapply(\n      1:nrow(well_locations),\n      get_daymet\n    ) %>%\n    rbindlist()\n)        month year prcp     tmax well_id\n     1:     4 2010   42 20.96667       1\n     2:     5 2010   94 24.19355       1\n     3:     6 2010   70 32.51667       1\n     4:     7 2010   89 33.50000       1\n     5:     8 2010   63 34.17742       1\n    ---                                 \n336980:     5 2011   18 26.11290   78051\n336981:     6 2011   25 34.61667   78051\n336982:     7 2011    6 38.37097   78051\n336983:     8 2011   39 36.66129   78051\n336984:     9 2011   23 28.45000   78051\n#--- long to wide ---#\ndaymet_to_merge <-\n  all_daymet %>%\n  dcast(\n    well_id + year ~ month,\n    value.var = c(\"prcp\", \"tmax\")\n  )\n\n#--- take a look ---#\ndaymet_to_merge       well_id year prcp_4 prcp_5 prcp_6 prcp_7 prcp_8 prcp_9   tmax_4   tmax_5\n    1:       1 2010     42     94     70     89     63     15 20.96667 24.19355\n    2:       1 2011     25     26     23     35     37      9 21.91667 26.30645\n    3:       3 2010     85     62    109    112     83     41 19.93333 21.64516\n    4:       3 2011     80    104     44    124    118     14 18.40000 22.62903\n    5:       7 2010     44     83     23     99    105     13 18.81667 22.14516\n   ---                                                                         \n56160:   78049 2011     27      6     38     37     34     36 22.81667 26.70968\n56161:   78050 2010     35     48     68    111     56      9 21.38333 24.85484\n56162:   78050 2011     26      7     44     38     34     35 22.76667 26.70968\n56163:   78051 2010     30     62     48     29     76      3 21.05000 24.14516\n56164:   78051 2011     33     18     25      6     39     23 21.90000 26.11290\n         tmax_6   tmax_7   tmax_8   tmax_9\n    1: 32.51667 33.50000 34.17742 31.43333\n    2: 35.16667 38.62903 36.90323 28.66667\n    3: 30.73333 32.80645 33.56452 28.93333\n    4: 30.08333 35.08065 32.90323 25.81667\n    5: 31.30000 33.12903 32.67742 30.16667\n   ---                                    \n56160: 35.01667 38.32258 36.54839 28.80000\n56161: 33.16667 33.88710 34.40323 32.11667\n56162: 34.91667 38.32258 36.54839 28.83333\n56163: 32.90000 33.83871 34.38710 31.56667\n56164: 34.61667 38.37097 36.66129 28.45000\n(\n  reg_data <-\n    data.table(gw_KS_sf) %>%\n    #--- keep only the relevant variables ---#\n    .[, .(well_id, year, af_used)] %>%\n    #--- join ---#\n    daymet_to_merge[., on = c(\"well_id\", \"year\")]\n)       well_id year prcp_4 prcp_5 prcp_6 prcp_7 prcp_8 prcp_9   tmax_4   tmax_5\n    1:       1 2010     42     94     70     89     63     15 20.96667 24.19355\n    2:       1 2011     25     26     23     35     37      9 21.91667 26.30645\n    3:       3 2010     85     62    109    112     83     41 19.93333 21.64516\n    4:       3 2011     80    104     44    124    118     14 18.40000 22.62903\n    5:       7 2010     44     83     23     99    105     13 18.81667 22.14516\n   ---                                                                         \n56221:   79348 2011     NA     NA     NA     NA     NA     NA       NA       NA\n56222:   79349 2011     NA     NA     NA     NA     NA     NA       NA       NA\n56223:   79367 2011     NA     NA     NA     NA     NA     NA       NA       NA\n56224:   79372 2011     NA     NA     NA     NA     NA     NA       NA       NA\n56225:   80930 2011     NA     NA     NA     NA     NA     NA       NA       NA\n         tmax_6   tmax_7   tmax_8   tmax_9   af_used\n    1: 32.51667 33.50000 34.17742 31.43333  67.00000\n    2: 35.16667 38.62903 36.90323 28.66667 171.00000\n    3: 30.73333 32.80645 33.56452 28.93333  30.93438\n    4: 30.08333 35.08065 32.90323 25.81667  12.00000\n    5: 31.30000 33.12903 32.67742 30.16667   0.00000\n   ---                                              \n56221:       NA       NA       NA       NA  76.00000\n56222:       NA       NA       NA       NA 182.00000\n56223:       NA       NA       NA       NA   0.00000\n56224:       NA       NA       NA       NA 134.00000\n56225:       NA       NA       NA       NA  23.69150\n#--- run FE ---#\nreg_results <-\n  fixest::feols(\n    af_used ~ # dependent variable\n      prcp_4 + prcp_5 + prcp_6 + prcp_7 + prcp_8 + prcp_9\n        + tmax_4 + tmax_5 + tmax_6 + tmax_7 + tmax_8 + tmax_9 |\n        well_id + year, # FEs\n    cluster = \"well_id\",\n    data = reg_data\n  )\n\n#--- display regression results ---#\nmodelsummary::modelsummary(\n  reg_results,\n  stars = TRUE,\n  gof_omit = \"IC|Log|Adj|Within|Pseudo\"\n)"},{"path":"vector-basics.html","id":"vector-basics","chapter":"2 Vector Data Handling with sf","heading":"2 Vector Data Handling with sf","text":"","code":""},{"path":"vector-basics.html","id":"before-you-start-1","chapter":"2 Vector Data Handling with sf","heading":"Before you start","text":"chapter learn use sf package handle operate spatial datasets. sf package uses class simple feature (sf)30 spatial objects R. first learn sf objects store represent spatial datasets. move following practical topics:read write shapefile spatial data formats (might want use shapefile system , use alternative formats)project reproject spatial objectsconvert sf objects sp objects, vice versaconfirm dplyr works well sf objectsimplement non-interactive (involve two sf objects) geometric operations sf objects\ncreate buffers\nfind area polygons\nfind centroid polygons\ncalculate length lines\ncreate buffersfind area polygonsfind centroid polygonscalculate length lines","code":""},{"path":"vector-basics.html","id":"sf-or-sp","chapter":"2 Vector Data Handling with sf","heading":"sf or sp?","text":"sf package designed replace sp package, one popular powerful spatial packages R decade. four years since sf package first registered CRAN. couple years back, many spatial packages support package yet. blog post author responded questions whether one learn sp sf saying,“’s tough question. time, say, learn use . sf pretty new, lot packages depend spatial classes still rely sp. need know sp want integration many packages, including raster (March 2018).However, future see increasing shift toward sf package greater use sf classes packages. also think sf easier learn use sp.”future come, ’s tough question anymore. think major spatial packages support sf package, sf largely becomes standard handling vector data \\(R\\)31. Thus, lecture note cover use sp .sf several advantages sp package (Pebesma 2018).32 First, cut tie sp ESRI shapefile system, somewhat loose way representing spatial data. Instead, uses simple feature access, open standard supported Open Geospatial Consortium (OGC). Another important benefit compatibility tidyverse package, includes widely popular packages like ggplot2 dplyr. Consequently, map-making ggplot() data wrangling family dplyr functions come natural many \\(R\\) users. sp objects different slots spatial information attributes data, amenable dplyr way data transformation.","code":""},{"path":"vector-basics.html","id":"direction-for-replication-1","chapter":"2 Vector Data Handling with sf","heading":"Direction for replication","text":"DatasetsAll datasets need import available . chapter, path files set relative working directory (hidden). run codes without mess paths files, follow steps:set folder (folder) working directory using setwd()create folder called “Data” inside folder designated working directory (created “Data” folder replicate demonstrations Chapter 1, skip step)download pertinent datasets hereplace files downloaded folder “Data” folderPackagesRun following code install load (already installed) pacman package, install load (already installed) listed package inside pacman::p_load() function.","code":"\nif (!require(\"pacman\")) install.packages(\"pacman\")\npacman::p_load(\n  sf, # vector data operations\n  dplyr, # data wrangling\n  data.table, # data wrangling\n  tmap, # make maps\n  mapview # create an interactive map\n)"},{"path":"vector-basics.html","id":"spatial-data-structure","chapter":"2 Vector Data Handling with sf","heading":"2.1 Spatial Data Structure","text":"learn sf package stores spatial data along definition three key sf object classes: simple feature geometry (sfg), simple feature geometry list-column (sfc), simple feature (sf). sf package provides simply way storing geographic information attributes geographic units single dataset. special type dataset called simple feature (sf). best take look example see achieved. use North Carolina county boundaries county attributes (Figure 2.1).\nFigure 2.1: North Carolina county boundary\ncan see , dataset class sf (data.frame time).Now, let’s take look inside nc.Just like regular data.frame, see number variables (attributes) except variable called geometry end. row represents single geographic unit (, county). Ashe County (1st row) area \\(0.114\\), FIPS code \\(37009\\), . entry geometry column first row represents geographic information Ashe County. entry geometry column simple feature geometry (sfg), \\(R\\) object represents geographic information single geometric feature (county example). different types sfgs (POINT, LINESTRING, POLYGON, MULTIPOLYGON, etc). , sfgs representing counties NC type MULTIPOLYGON. Let’s take look inside sfg Ashe County using st_geometry().can see, sfg consists number points (pairs two numbers). Connecting points order stored delineates Ashe County boundary.take closer look different types sfg next section.Finally, geometry variable list individual sfgs, called simple feature geometry list-column (sfc).Elements geometry list-column allowed different nature elements33. nc data, elements (sfgs) geometry column MULTIPOLYGON. However, also LINESTRING POINT objects mixed MULTIPOLYGONS objects single sf object like.","code":"\n#--- a dataset that comes with the sf package ---#\nnc <- st_read(system.file(\"shape/nc.shp\", package = \"sf\"))Reading layer `nc' from data source \n  `/Library/Frameworks/R.framework/Versions/4.1/Resources/library/sf/shape/nc.shp' \n  using driver `ESRI Shapefile'\nSimple feature collection with 100 features and 14 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -84.32385 ymin: 33.88199 xmax: -75.45698 ymax: 36.58965\nGeodetic CRS:  NAD27\nclass(nc)[1] \"sf\"         \"data.frame\"\n#--- take a look at the data ---#\nhead(nc)Simple feature collection with 6 features and 14 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -81.74107 ymin: 36.07282 xmax: -75.77316 ymax: 36.58965\nGeodetic CRS:  NAD27\n   AREA PERIMETER CNTY_ CNTY_ID        NAME  FIPS FIPSNO CRESS_ID BIR74 SID74\n1 0.114     1.442  1825    1825        Ashe 37009  37009        5  1091     1\n2 0.061     1.231  1827    1827   Alleghany 37005  37005        3   487     0\n3 0.143     1.630  1828    1828       Surry 37171  37171       86  3188     5\n4 0.070     2.968  1831    1831   Currituck 37053  37053       27   508     1\n5 0.153     2.206  1832    1832 Northampton 37131  37131       66  1421     9\n6 0.097     1.670  1833    1833    Hertford 37091  37091       46  1452     7\n  NWBIR74 BIR79 SID79 NWBIR79                       geometry\n1      10  1364     0      19 MULTIPOLYGON (((-81.47276 3...\n2      10   542     3      12 MULTIPOLYGON (((-81.23989 3...\n3     208  3616     6     260 MULTIPOLYGON (((-80.45634 3...\n4     123   830     2     145 MULTIPOLYGON (((-76.00897 3...\n5    1066  1606     3    1197 MULTIPOLYGON (((-77.21767 3...\n6     954  1838     5    1237 MULTIPOLYGON (((-76.74506 3...\nst_geometry(nc[1, ])[[1]][[1]][[1]]\n           [,1]     [,2]\n [1,] -81.47276 36.23436\n [2,] -81.54084 36.27251\n [3,] -81.56198 36.27359\n [4,] -81.63306 36.34069\n [5,] -81.74107 36.39178\n [6,] -81.69828 36.47178\n [7,] -81.70280 36.51934\n [8,] -81.67000 36.58965\n [9,] -81.34530 36.57286\n[10,] -81.34754 36.53791\n[11,] -81.32478 36.51368\n[12,] -81.31332 36.48070\n[13,] -81.26624 36.43721\n[14,] -81.26284 36.40504\n[15,] -81.24069 36.37942\n[16,] -81.23989 36.36536\n[17,] -81.26424 36.35241\n[18,] -81.32899 36.36350\n[19,] -81.36137 36.35316\n[20,] -81.36569 36.33905\n[21,] -81.35413 36.29972\n[22,] -81.36745 36.27870\n[23,] -81.40639 36.28505\n[24,] -81.41233 36.26729\n[25,] -81.43104 36.26072\n[26,] -81.45289 36.23959\n[27,] -81.47276 36.23436\nplot(st_geometry(nc[1, ]))\ndplyr::select(nc, geometry)Simple feature collection with 100 features and 0 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -84.32385 ymin: 33.88199 xmax: -75.45698 ymax: 36.58965\nGeodetic CRS:  NAD27\nFirst 10 features:\n                         geometry\n1  MULTIPOLYGON (((-81.47276 3...\n2  MULTIPOLYGON (((-81.23989 3...\n3  MULTIPOLYGON (((-80.45634 3...\n4  MULTIPOLYGON (((-76.00897 3...\n5  MULTIPOLYGON (((-77.21767 3...\n6  MULTIPOLYGON (((-76.74506 3...\n7  MULTIPOLYGON (((-76.00897 3...\n8  MULTIPOLYGON (((-76.56251 3...\n9  MULTIPOLYGON (((-78.30876 3...\n10 MULTIPOLYGON (((-80.02567 3..."},{"path":"vector-basics.html","id":"simple-feature-geometry-simple-feature-geometry-list-column-and-simple-feature","chapter":"2 Vector Data Handling with sf","heading":"2.2 Simple feature geometry, simple feature geometry list-column, and simple feature","text":", learn different types sfg constructed. also learn create sfc sf sfg scratch.34","code":""},{"path":"vector-basics.html","id":"simple-feature-geometry-sfg","chapter":"2 Vector Data Handling with sf","heading":"2.2.1 Simple feature geometry (sfg)","text":"sf package uses class sfg (simple feature geometry) objects represent geometry single geometric feature (say, city point, river line, county school district polygons). different types sfgs. example feature types commonly encounter economist35:POINT: area-less feature represents point (e.g., well, city, farmland)LINESTRING: (e.g., tributary river)MULTILINESTRING: (e.g., river one tributary)POLYGON: geometry positive area (e.g., county, state, country)MULTIPOLYGON: collection polygons represent single object (e.g., countries islands: U.S., Japan)POINT simplest geometry type represented vector two36 numeric values. example shows POINT feature can made scratch:st_point() function creates POINT object supplied vector two numeric values. check class newly created object,can see ’s indeed POINT object. , ’s also sfg object. , a_point sfg object type POINT.LINESTRING objects represented sequence points:s1 matrix row represents point. applying st_linestring() function s1, create LINESTRING object. Let’s see line looks like.can see, pair consecutive points matrix connected straight line form line.POLYGON similar LINESTRING manner represented.Just like LINESTRING object created earlier, POLYGON represented collection points. biggest difference need positive area enclosed lines connecting points. , point first last points close loop: , ’s c(0,0). POLYGON can holes . first matrix list becomes exterior ring, subsequent matrices holes within exterior ring.can create MULTIPOLYGON object similar manner. difference supply list lists matrices, inner list representing polygon. example :list(p1,p2) list(p3) represents polygon. supply list lists st_multipolygon() function make MULTIPOLYGON object.","code":"\n#--- create a POINT ---#\na_point <- st_point(c(2, 1))\n#--- check the class of the object ---#\nclass(a_point)[1] \"XY\"    \"POINT\" \"sfg\"  \n#--- collection of points in a matrix form ---#\ns1 <- rbind(c(2, 3), c(3, 4), c(3, 5), c(1, 5))\n\n#--- see what s1 looks like ---#\ns1     [,1] [,2]\n[1,]    2    3\n[2,]    3    4\n[3,]    3    5\n[4,]    1    5\n#--- create a \"LINESTRING\" ---#\na_linestring <- st_linestring(s1)\n\n#--- check the class ---#\nclass(a_linestring)[1] \"XY\"         \"LINESTRING\" \"sfg\"       \nplot(a_linestring)\n#--- collection of points in a matrix form ---#\np1 <- rbind(c(0, 0), c(3, 0), c(3, 2), c(2, 5), c(1, 3), c(0, 0))\n\n#--- see what s1 looks like ---#\np1     [,1] [,2]\n[1,]    0    0\n[2,]    3    0\n[3,]    3    2\n[4,]    2    5\n[5,]    1    3\n[6,]    0    0\n#--- create a \"POLYGON\" ---#\na_polygon <- st_polygon(list(p1))\n\n#--- check the class ---#\nclass(a_polygon)[1] \"XY\"      \"POLYGON\" \"sfg\"    \n#--- see what it looks like ---#\nplot(a_polygon)\n#--- a hole within p1 ---#\np2 <- rbind(c(1, 1), c(1, 2), c(2, 2), c(1, 1))\n\n#--- create a polygon with hole ---#\na_plygon_with_a_hole <- st_polygon(list(p1, p2))\n\n#--- see what it looks like ---#\nplot(a_plygon_with_a_hole)\n#--- second polygon ---#\np3 <- rbind(c(4, 0), c(5, 0), c(5, 3), c(4, 2), c(4, 0))\n\n#--- create a multipolygon ---#\na_multipolygon <- st_multipolygon(list(list(p1, p2), list(p3)))\n\n#--- see what it looks like ---#\nplot(a_multipolygon)"},{"path":"vector-basics.html","id":"create-simple-feature-geometry-list-column-sfc-and-simple-feature-sf-from-scratch","chapter":"2 Vector Data Handling with sf","heading":"2.2.2 Create simple feature geometry list-column (sfc) and simple feature (sf) from scratch","text":"make simple feature geometry list-column (sfc), can simply supply list sfg st_sfc() function follows:create sf object, first add sfc column data.frame.point, yet recognized sf R yet.can register sf object using st_as_sf().can see sf_ex now recognized also sf object.","code":"\n#--- create an sfc ---#\nsfc_ex <- st_sfc(list(a_point, a_linestring, a_polygon, a_multipolygon))\n#--- create a data.frame ---#\ndf_ex <- data.frame(name = c(\"A\", \"B\", \"C\", \"D\"))\n\n#--- add the sfc as a column ---#\ndf_ex$geometry <- sfc_ex\n\n#--- take a look ---#\ndf_ex  name                       geometry\n1    A                    POINT (2 1)\n2    B LINESTRING (2 3, 3 4, 3 5, ...\n3    C POLYGON ((0 0, 3 0, 3 2, 2 ...\n4    D MULTIPOLYGON (((0 0, 3 0, 3...\n#--- see what it looks like (this is not an sf object yet) ---#\nclass(df_ex)[1] \"data.frame\"\n#--- let R recognize the data frame as sf ---#\nsf_ex <- st_as_sf(df_ex)\n\n#--- see what it looks like ---#\nsf_exSimple feature collection with 4 features and 1 field\nGeometry type: GEOMETRY\nDimension:     XY\nBounding box:  xmin: 0 ymin: 0 xmax: 5 ymax: 5\nCRS:           NA\n  name                       geometry\n1    A                    POINT (2 1)\n2    B LINESTRING (2 3, 3 4, 3 5, ...\n3    C POLYGON ((0 0, 3 0, 3 2, 2 ...\n4    D MULTIPOLYGON (((0 0, 3 0, 3...\n#--- check the class ---#\nclass(sf_ex)[1] \"sf\"         \"data.frame\""},{"path":"vector-basics.html","id":"reading-and-writing-vector-data","chapter":"2 Vector Data Handling with sf","heading":"2.3 Reading and writing vector data","text":"vast majority people still use ArcGIS handle spatial data, system storing spatial data37 called shapefile. , chances collaborators use shapefiles. Moreover, many GIS data online available shapefiles. , important learn read write shapefiles.","code":""},{"path":"vector-basics.html","id":"reading-a-shapefile","chapter":"2 Vector Data Handling with sf","heading":"2.3.1 Reading a shapefile","text":"can use st_read() read shapefile. reads shapefile turn data sf object. Let’s take look example.Typically, two arguments specify st_read(). first one dsn, basically path folder shapefile want import stored. second one name shapefile. Notice add .shp extension file name: nc, nc.shp.38.","code":"\n#--- read a NE county boundary shapefile ---#\nnc_loaded <- st_read(dsn = \"Data\", \"nc\")"},{"path":"vector-basics.html","id":"writing-to-a-shapefile","chapter":"2 Vector Data Handling with sf","heading":"2.3.2 Writing to a shapefile","text":"Writing sf object shapefile just easy. use st_write() function, first argument name sf object exporting, second name new shapefile. example, code export sf object called nc_loaded nc2.shp (along supporting files).append = FALSE forces writing data file already exists name. Without option, happens.","code":"\nst_write(\n  nc_loaded,\n  dsn = \"Data\",\n  layer = \"nc2\",\n  driver = \"ESRI Shapefile\",\n  append = FALSE\n)\nst_write(\n  nc_loaded,\n  dsn = \"Data\",\n  layer = \"nc2\",\n  driver = \"ESRI Shapefile\"\n)Layer nc2 in dataset Data already exists:\nuse either append=TRUE to append to layer or append=FALSE to overwrite layerError in CPL_write_ogr(obj, dsn, layer, driver, as.character(dataset_options), : Dataset already exists."},{"path":"vector-basics.html","id":"better-alternatives","chapter":"2 Vector Data Handling with sf","heading":"2.3.3 Better alternatives","text":"Now, collaborator using ArcGIS demanding /needs shapefile /work, sure can use command write shapefile. , really need work shapefile system. One alternative data formats considered superior shapefile system GeoPackage39, overcomes various limitations associated shapefile40. Unlike shapefile system, produces single file .gpkg extension.41 Note GeoPackage files can also easily read ArcGIS. , might worthwhile convince collaborators stop using shapefiles start using GeoPackage.better yet, collaborator uses R (going use data), just save rds file using saveRDS(), can course read using readRDS().use rds files can particularly attractive dataset large rds files typically memory efficient shapefiles, eating less disk memory.can see , myth spatial datasets stored shapefiles.","code":"\n#--- write as a gpkg file ---#\nst_write(nc, dsn = \"Data/nc.gpkg\", append = FALSE)\n\n#--- read a gpkg file ---#\nnc <- st_read(\"Data/nc.gpkg\")\n#--- save as an rds ---#\nsaveRDS(nc, \"Data/nc_county.rds\")\n\n#--- read an rds ---#\nnc <- readRDS(\"Data/nc_county.rds\")"},{"path":"vector-basics.html","id":"projection-with-a-different-coordinate-reference-systems","chapter":"2 Vector Data Handling with sf","heading":"2.4 Projection with a different Coordinate Reference Systems","text":"often need reproject sf using different coordinate reference system (CRS) need CRS sf object interacting (spatial join) mapping . order check current CRS sf object, can use st_crs() function.wkt stands Well Known Text42, one many many formats store CRS information.43 4267 SRID (Spatial Reference System Identifier) defined European Petroleum Survey Group (EPSG) CRS44.transform sf using different CRS, can use EPSG number CRS EPSG number.45 Let’s transform sf WGS 84 (another commonly used GCS), whose EPSG number 4326. can use st_transform() function achieve , first argument sf object transforming second EPSG number new CRS.Notice wkt also altered accordingly reflect change CRS: datum changed WGS 84. Now, let’s transform (reproject) data using NAD83 / UTM zone 17N CRS. EPSG number \\(26917\\).46 , following code job.can see CRS information, projection system now UTM zone 17N.often need change CRS sf object interact (e.g., spatial subsetting, joining, etc) another sf object. case, can extract CRS sf object using st_crs() use transformation.47 , need find EPSG CRS sf object interacting .","code":"\nst_crs(nc)Coordinate Reference System:\n  User input: NAD27 \n  wkt:\nGEOGCRS[\"NAD27\",\n    DATUM[\"North American Datum 1927\",\n        ELLIPSOID[\"Clarke 1866\",6378206.4,294.978698213898,\n            LENGTHUNIT[\"metre\",1]]],\n    PRIMEM[\"Greenwich\",0,\n        ANGLEUNIT[\"degree\",0.0174532925199433]],\n    CS[ellipsoidal,2],\n        AXIS[\"latitude\",north,\n            ORDER[1],\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        AXIS[\"longitude\",east,\n            ORDER[2],\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n    ID[\"EPSG\",4267]]\n#--- transform ---#\nnc_wgs84 <- st_transform(nc, 4326)\n\n#--- check if the transformation was successful ---#\nst_crs(nc_wgs84)Coordinate Reference System:\n  User input: EPSG:4326 \n  wkt:\nGEOGCRS[\"WGS 84\",\n    ENSEMBLE[\"World Geodetic System 1984 ensemble\",\n        MEMBER[\"World Geodetic System 1984 (Transit)\"],\n        MEMBER[\"World Geodetic System 1984 (G730)\"],\n        MEMBER[\"World Geodetic System 1984 (G873)\"],\n        MEMBER[\"World Geodetic System 1984 (G1150)\"],\n        MEMBER[\"World Geodetic System 1984 (G1674)\"],\n        MEMBER[\"World Geodetic System 1984 (G1762)\"],\n        ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n            LENGTHUNIT[\"metre\",1]],\n        ENSEMBLEACCURACY[2.0]],\n    PRIMEM[\"Greenwich\",0,\n        ANGLEUNIT[\"degree\",0.0174532925199433]],\n    CS[ellipsoidal,2],\n        AXIS[\"geodetic latitude (Lat)\",north,\n            ORDER[1],\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        AXIS[\"geodetic longitude (Lon)\",east,\n            ORDER[2],\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n    USAGE[\n        SCOPE[\"Horizontal component of 3D system.\"],\n        AREA[\"World.\"],\n        BBOX[-90,-180,90,180]],\n    ID[\"EPSG\",4326]]\n#--- transform ---#\nnc_utm17N <- st_transform(nc_wgs84, 26917)\n\n#--- check if the transformation was successful ---#\nst_crs(nc_utm17N)Coordinate Reference System:\n  User input: EPSG:26917 \n  wkt:\nPROJCRS[\"NAD83 / UTM zone 17N\",\n    BASEGEOGCRS[\"NAD83\",\n        DATUM[\"North American Datum 1983\",\n            ELLIPSOID[\"GRS 1980\",6378137,298.257222101,\n                LENGTHUNIT[\"metre\",1]]],\n        PRIMEM[\"Greenwich\",0,\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        ID[\"EPSG\",4269]],\n    CONVERSION[\"UTM zone 17N\",\n        METHOD[\"Transverse Mercator\",\n            ID[\"EPSG\",9807]],\n        PARAMETER[\"Latitude of natural origin\",0,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8801]],\n        PARAMETER[\"Longitude of natural origin\",-81,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8802]],\n        PARAMETER[\"Scale factor at natural origin\",0.9996,\n            SCALEUNIT[\"unity\",1],\n            ID[\"EPSG\",8805]],\n        PARAMETER[\"False easting\",500000,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8806]],\n        PARAMETER[\"False northing\",0,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8807]]],\n    CS[Cartesian,2],\n        AXIS[\"(E)\",east,\n            ORDER[1],\n            LENGTHUNIT[\"metre\",1]],\n        AXIS[\"(N)\",north,\n            ORDER[2],\n            LENGTHUNIT[\"metre\",1]],\n    USAGE[\n        SCOPE[\"Engineering survey, topographic mapping.\"],\n        AREA[\"North America - between 84°W and 78°W - onshore and offshore. Canada - Nunavut; Ontario; Quebec. United States (USA) - Florida; Georgia; Kentucky; Maryland; Michigan; New York; North Carolina; Ohio; Pennsylvania; South Carolina; Tennessee; Virginia; West Virginia.\"],\n        BBOX[23.81,-84,84,-78]],\n    ID[\"EPSG\",26917]]\n#--- transform ---#\nnc_utm17N_2 <- st_transform(nc_wgs84, st_crs(nc_utm17N))\n\n#--- check if the transformation was successful ---#\nst_crs(nc_utm17N_2)Coordinate Reference System:\n  User input: EPSG:26917 \n  wkt:\nPROJCRS[\"NAD83 / UTM zone 17N\",\n    BASEGEOGCRS[\"NAD83\",\n        DATUM[\"North American Datum 1983\",\n            ELLIPSOID[\"GRS 1980\",6378137,298.257222101,\n                LENGTHUNIT[\"metre\",1]]],\n        PRIMEM[\"Greenwich\",0,\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        ID[\"EPSG\",4269]],\n    CONVERSION[\"UTM zone 17N\",\n        METHOD[\"Transverse Mercator\",\n            ID[\"EPSG\",9807]],\n        PARAMETER[\"Latitude of natural origin\",0,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8801]],\n        PARAMETER[\"Longitude of natural origin\",-81,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8802]],\n        PARAMETER[\"Scale factor at natural origin\",0.9996,\n            SCALEUNIT[\"unity\",1],\n            ID[\"EPSG\",8805]],\n        PARAMETER[\"False easting\",500000,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8806]],\n        PARAMETER[\"False northing\",0,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8807]]],\n    CS[Cartesian,2],\n        AXIS[\"(E)\",east,\n            ORDER[1],\n            LENGTHUNIT[\"metre\",1]],\n        AXIS[\"(N)\",north,\n            ORDER[2],\n            LENGTHUNIT[\"metre\",1]],\n    USAGE[\n        SCOPE[\"Engineering survey, topographic mapping.\"],\n        AREA[\"North America - between 84°W and 78°W - onshore and offshore. Canada - Nunavut; Ontario; Quebec. United States (USA) - Florida; Georgia; Kentucky; Maryland; Michigan; New York; North Carolina; Ohio; Pennsylvania; South Carolina; Tennessee; Virginia; West Virginia.\"],\n        BBOX[23.81,-84,84,-78]],\n    ID[\"EPSG\",26917]]"},{"path":"vector-basics.html","id":"quick-and-interactive-view-of-an-sf-object","chapter":"2 Vector Data Handling with sf","heading":"2.5 Quick and interactive view of an sf object","text":"","code":""},{"path":"vector-basics.html","id":"quick-view","chapter":"2 Vector Data Handling with sf","heading":"2.5.1 Quick view","text":"easiest way visualize sf object use plot():\nFigure 2.2: Quick Visualization sf object\ncan see, plot() create map variable spatial units color-differentiated based values variable. creating elaborate maps publication-quality, see Chapter 8.","code":"\nplot(nc)"},{"path":"vector-basics.html","id":"interactive-view","chapter":"2 Vector Data Handling with sf","heading":"2.5.2 Interactive view","text":"Sometimes useful able tell certain spatial objects values associated map. tmap_leaflet() function tmap package can create interactive map can point spatial object associated information revealed map. Let’s use North Carolina county map example :\nFigure 2.3: Interactive map North Carolina counties\ncan see, put cursor polygon (county) click , information pops .Alternatively, use tmap package create interactive maps. can first create static map following syntax like :creates static map nc county boundaries drawn:, can apply tmap_leaflet() static map interactive view map:also change view mode tmap objects view mode using tmap_mode(\"view\") simply evaluate tm_nc_polygons.Note change view mode “view,” evaluation tmap objects become interactive. prefer first option, need revert view mode back “plot” tmap_mode(\"plot\") don’t want interactive views.","code":"\n#--- NOT RUN (for polygons) ---#\ntm_shape(sf) +\n  tm_polygons()\n\n#--- NOT RUN (for points) ---#\ntm_shape(sf) +\n  tm_symbols()\n(\n  tm_nc_polygons <- tm_shape(nc) + tm_polygons()\n)\ntmap_leaflet(tm_nc_polygons)\n#--- change to the \"view\" mode ---#\ntmap_mode(\"view\")\n\n#--- now you have an interactive biew ---#\ntm_nc_polygons"},{"path":"vector-basics.html","id":"turning-a-data.frame-of-points-into-an-sf","chapter":"2 Vector Data Handling with sf","heading":"2.6 Turning a data.frame of points into an sf","text":"Often times, dataset geographic coordinates variables csv formats, recognized spatial dataset R immediately read R. case, need identify variables represent geographic coordinates data set, create sf . Fortunately, easy using st_as_sf() function. Let’s first read dataset (irrigation wells Nebraska):can see data sf object. dataset, longdd latdd represent longitude latitude, respectively. now turn dataset sf object:Note CRS wells_sf NA. Obviously, \\(R\\) know reference system without telling . know48 geographic coordinates wells data NAD 83 (\\(epsg=4269\\)) dataset. , can assign right CRS using either st_set_crs() st_crs().","code":"\n#--- read irrigation well registration data ---#\n(\n  wells <- readRDS(\"Data/well_registration.rds\")\n)        wellid ownerid        nrdname acres  regdate section     longdd\n     1:      2  106106 Central Platte   160 12/30/55       3  -99.58401\n     2:      3   14133   South Platte    46  4/29/31       8 -102.62495\n     3:      4   14133   South Platte    46  4/29/31       8 -102.62495\n     4:      5   14133   South Platte    46  4/29/31       8 -102.62495\n     5:      6   15837 Central Platte   160  8/29/32      20  -99.62580\n    ---                                                                \n105818: 244568  135045 Upper Big Blue    NA  8/26/16      30  -97.58872\n105819: 244569  105428    Little Blue    NA  8/26/16      24  -97.60752\n105820: 244570  135045 Upper Big Blue    NA  8/26/16      30  -97.58294\n105821: 244571  135045 Upper Big Blue    NA  8/26/16      25  -97.59775\n105822: 244572  105428    Little Blue    NA  8/26/16      15  -97.64086\n           latdd\n     1: 40.69825\n     2: 41.11699\n     3: 41.11699\n     4: 41.11699\n     5: 40.73268\n    ---         \n105818: 40.89017\n105819: 40.13257\n105820: 40.88722\n105821: 40.89639\n105822: 40.13380\n#--- check the class ---#\nclass(wells)[1] \"data.table\" \"data.frame\"\n#--- recognize it as an sf ---#\nwells_sf <- st_as_sf(wells, coords = c(\"longdd\", \"latdd\"))\n\n#--- take a look at the data ---#\nhead(wells_sf[, 1:5])Simple feature collection with 6 features and 5 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: -102.6249 ymin: 40.69824 xmax: -99.58401 ymax: 41.11699\nCRS:           NA\n  wellid ownerid        nrdname acres  regdate                   geometry\n1      2  106106 Central Platte   160 12/30/55 POINT (-99.58401 40.69825)\n2      3   14133   South Platte    46  4/29/31 POINT (-102.6249 41.11699)\n3      4   14133   South Platte    46  4/29/31 POINT (-102.6249 41.11699)\n4      5   14133   South Platte    46  4/29/31 POINT (-102.6249 41.11699)\n5      6   15837 Central Platte   160  8/29/32  POINT (-99.6258 40.73268)\n6      7   90248 Central Platte   120  2/15/35 POINT (-99.64524 40.73164)\n#--- set CRS ---#\nwells_sf <- st_set_crs(wells_sf, 4269)\n\n#--- or this ---#\nst_crs(wells_sf) <- 4269\n\n#--- see the change ---#\nhead(wells_sf[, 1:5])Simple feature collection with 6 features and 5 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: -102.6249 ymin: 40.69824 xmax: -99.58401 ymax: 41.11699\nGeodetic CRS:  NAD83\n  wellid ownerid        nrdname acres  regdate                   geometry\n1      2  106106 Central Platte   160 12/30/55 POINT (-99.58401 40.69825)\n2      3   14133   South Platte    46  4/29/31 POINT (-102.6249 41.11699)\n3      4   14133   South Platte    46  4/29/31 POINT (-102.6249 41.11699)\n4      5   14133   South Platte    46  4/29/31 POINT (-102.6249 41.11699)\n5      6   15837 Central Platte   160  8/29/32  POINT (-99.6258 40.73268)\n6      7   90248 Central Platte   120  2/15/35 POINT (-99.64524 40.73164)"},{"path":"vector-basics.html","id":"conv_sp","chapter":"2 Vector Data Handling with sf","heading":"2.7 Conversion to and from sp objects","text":"may find instances sp objects necessary desirable.49 case, good know convert sf object sp object, vice versa. can convert sf object sp counterpart using (sf_object, \"Spatial\"):can see wells_sp class SpatialPointsDataFrame, points data.frame supported sp package. syntax works converting sf polygons SpatialPolygonsDataFrame well50.can revert wells_sp back sf object using st_as_sf() function, follows:cover use sp package benefit learning become marginal compared sf just introduced years back51.","code":"\n#--- conversion ---#\nwells_sp <- as(wells_sf, \"Spatial\")\n\n#--- check the class ---#\nclass(wells_sp)[1] \"SpatialPointsDataFrame\"\nattr(,\"package\")\n[1] \"sp\"\n#--- revert back to sf ---#\nwells_sf <- st_as_sf(wells_sp)\n\n#--- check the class ---#\nclass(wells_sf)[1] \"sf\"         \"data.frame\""},{"path":"vector-basics.html","id":"non-spatial-transformation-of-sf","chapter":"2 Vector Data Handling with sf","heading":"2.8 Non-spatial transformation of sf","text":"","code":""},{"path":"vector-basics.html","id":"using-dplyr","chapter":"2 Vector Data Handling with sf","heading":"2.8.1 Using dplyr","text":"important feature sf object basically data.frame geometric information stored variable (column). means transforming sf object works just like transforming data.frame. Basically, everything can data.frame, can sf well. code just provides example basic operations including dplyr::select(), dplyr::filter(), dplyr::mutate() action sf object just confirm dplyr operations works sf object just like data.frame.Notice geometry column retained dplyr::select() even tell R keep .Let’s apply dplyr::select(), dplyr::filter(), dplyr::mutate() dataset.Now, let’s try get summary variable group using group_by() summarize() functions. , use first 100 observations dplyr::summarize() takes just long., can summarize sf group using dplyr::group_by() dplyr::summarize(). One interesting change happened geometry variable. NRD now multipoint sfg, combination wells (points) located inside NRD. Now, hard imagine need summarized geometries group summary. Moreover, slow operation. lots free time, try running code wells_sf instead wells_sf[1:100, ]. never waited finish running long long time. advised simply drop geometry turn sf object data.frame (tibble, data.table) first group summary.can now group summary quickly:","code":"\n#--- here is what the data looks like ---#\ndplyr::select(wells_sf, wellid, nrdname, acres, regdate, nrdname)Simple feature collection with 105822 features and 4 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: -104.0531 ymin: 40.00161 xmax: -96.87681 ymax: 41.85942\nGeodetic CRS:  NAD83\nFirst 10 features:\n   wellid           nrdname acres  regdate                   geometry\n1       2    Central Platte   160 12/30/55 POINT (-99.58401 40.69825)\n2       3      South Platte    46  4/29/31 POINT (-102.6249 41.11699)\n3       4      South Platte    46  4/29/31 POINT (-102.6249 41.11699)\n4       5      South Platte    46  4/29/31 POINT (-102.6249 41.11699)\n5       6    Central Platte   160  8/29/32  POINT (-99.6258 40.73268)\n6       7    Central Platte   120  2/15/35 POINT (-99.64524 40.73164)\n7       8      South Platte   113   8/7/37 POINT (-103.5257 41.24492)\n8      10      South Platte   160   5/4/38 POINT (-103.0284 41.13243)\n9      11 Middle Republican   807   5/6/38  POINT (-101.1193 40.3527)\n10     12 Middle Republican   148 11/29/77 POINT (-101.1146 40.35631)\n#--- do some transformations ---#\nwells_sf %>%\n  #--- select variables (geometry will always remain after select) ---#\n  dplyr::select(wellid, nrdname, acres, regdate, nrdname) %>%\n  #--- removes observations with acre < 30  ---#\n  dplyr::filter(acres > 30) %>%\n  #--- hectare instead of acre ---#\n  dplyr::mutate(hectare = acres * 0.404686)Simple feature collection with 63271 features and 5 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: -104.0529 ymin: 40.00161 xmax: -96.87681 ymax: 41.73599\nGeodetic CRS:  NAD83\nFirst 10 features:\n   wellid           nrdname acres  regdate                   geometry   hectare\n1       2    Central Platte   160 12/30/55 POINT (-99.58401 40.69825)  64.74976\n2       3      South Platte    46  4/29/31 POINT (-102.6249 41.11699)  18.61556\n3       4      South Platte    46  4/29/31 POINT (-102.6249 41.11699)  18.61556\n4       5      South Platte    46  4/29/31 POINT (-102.6249 41.11699)  18.61556\n5       6    Central Platte   160  8/29/32  POINT (-99.6258 40.73268)  64.74976\n6       7    Central Platte   120  2/15/35 POINT (-99.64524 40.73164)  48.56232\n7       8      South Platte   113   8/7/37 POINT (-103.5257 41.24492)  45.72952\n8      10      South Platte   160   5/4/38 POINT (-103.0284 41.13243)  64.74976\n9      11 Middle Republican   807   5/6/38  POINT (-101.1193 40.3527) 326.58160\n10     12 Middle Republican   148 11/29/77 POINT (-101.1146 40.35631)  59.89353\n#--- summary by group ---#\nwells_by_nrd <-\n  wells_sf[1:100, ] %>%\n  #--- group by nrdname ---#\n  dplyr::group_by(nrdname) %>%\n  #--- summarize ---#\n  dplyr::summarize(tot_acres = sum(acres, na.rm = TRUE))\n\n#--- take a look ---#\nwells_by_nrdSimple feature collection with 8 features and 2 fields\nGeometry type: MULTIPOINT\nDimension:     XY\nBounding box:  xmin: -103.6467 ymin: 40.0475 xmax: -97.89758 ymax: 41.24492\nGeodetic CRS:  NAD83\n# A tibble: 8 × 3\n  nrdname           tot_acres                                           geometry\n  <chr>                 <dbl>                                   <MULTIPOINT [°]>\n1 Central Platte        6690. ((-99.00983 40.73009), (-99.03167 40.71927), (-99…\n2 Lower Republican       350  ((-99.30403 40.05116), (-99.21946 40.05126), (-99…\n3 Middle Republican     1396  ((-100.9216 40.18866), (-101.1663 40.36351), (-10…\n4 South Platte          1381  ((-103.6467 41.2375), (-103.6128 41.23761), (-103…\n5 Tri-Basin              480  ((-98.94967 40.64285), (-98.94906 40.65061), (-98…\n6 Twin Platte           1098. ((-100.9571 41.18633), (-100.8659 41.14635), (-10…\n7 Upper Big Blue         330        ((-97.89758 40.86156), (-97.89998 40.86336))\n8 Upper Republican       480         ((-101.8858 40.47776), (-101.6501 40.6012))\n#--- remove geometry ---#\nwells_no_longer_sf <- st_drop_geometry(wells_sf)\n\n#--- take a look ---#\nhead(wells_no_longer_sf)  wellid ownerid        nrdname acres  regdate section\n1      2  106106 Central Platte   160 12/30/55       3\n2      3   14133   South Platte    46  4/29/31       8\n3      4   14133   South Platte    46  4/29/31       8\n4      5   14133   South Platte    46  4/29/31       8\n5      6   15837 Central Platte   160  8/29/32      20\n6      7   90248 Central Platte   120  2/15/35      19\nwells_no_longer_sf %>%\n  #--- group by nrdname ---#\n  dplyr::group_by(nrdname) %>%\n  #--- summarize ---#\n  dplyr::summarize(tot_acres = sum(acres, na.rm = TRUE))# A tibble: 9 × 2\n  nrdname           tot_acres\n  <chr>                 <dbl>\n1 Central Platte     1890918.\n2 Little Blue         995900.\n3 Lower Republican    543079.\n4 Middle Republican   443472.\n5 South Platte        216109.\n6 Tri-Basin           847058.\n7 Twin Platte         452678.\n8 Upper Big Blue     1804782.\n9 Upper Republican    551906."},{"path":"vector-basics.html","id":"using-data.table","chapter":"2 Vector Data Handling with sf","heading":"2.8.2 Using data.table","text":"data.table package provides data wrangling options extremely fast (see various benchmark results). particularly shines datasets large much faster dplyr. However, naturally integrated workflow involving sf objects dplyr can. Let’s convert sf object points data.table object using data.table().see wells_dt longer sf object, geometry column still remains data.convert sf polygons data.table, geometry column appears lost geometry information entries just <XY[number]>., geometry information still can see :try run sf operations data.table obejct geometry colum, course give error. Like thisHowever, can apply sf spatial operations geometry like :Finally, easy revert data.table object back sf object using st_as_sf() function., means need fast data transformation, can first turn sf data.table, transform data using data.table functionality, revert back sf. However, economists, geometry variable interest sense never enters econometric models. us, geographic information contained geometry variable just glue tie two datasets together geographic referencing. get values spatial variables interest, point keeping data sf object. Personally, whenever longer need carry around geometry variable, immediately turn sf object data.table fast data transformation especially data large.know dtplyr package (takes advantage speed data.table can keep using dplyr syntax functions) may wonder works well sf objects. Nope:way, package awesome really love dplyr, want speed data.table. dtplyr course slightly slower data.table internal translations dplyr language data.table language happen first.52","code":"\n#--- convert an sf to data.table ---#\n(\n  wells_dt <- data.table(wells_sf)\n)        wellid ownerid        nrdname acres  regdate section\n     1:      2  106106 Central Platte   160 12/30/55       3\n     2:      3   14133   South Platte    46  4/29/31       8\n     3:      4   14133   South Platte    46  4/29/31       8\n     4:      5   14133   South Platte    46  4/29/31       8\n     5:      6   15837 Central Platte   160  8/29/32      20\n    ---                                                     \n105818: 244568  135045 Upper Big Blue    NA  8/26/16      30\n105819: 244569  105428    Little Blue    NA  8/26/16      24\n105820: 244570  135045 Upper Big Blue    NA  8/26/16      30\n105821: 244571  135045 Upper Big Blue    NA  8/26/16      25\n105822: 244572  105428    Little Blue    NA  8/26/16      15\n                          geometry\n     1: POINT (-99.58401 40.69825)\n     2: POINT (-102.6249 41.11699)\n     3: POINT (-102.6249 41.11699)\n     4: POINT (-102.6249 41.11699)\n     5:  POINT (-99.6258 40.73268)\n    ---                           \n105818: POINT (-97.58872 40.89017)\n105819: POINT (-97.60752 40.13257)\n105820: POINT (-97.58294 40.88722)\n105821: POINT (-97.59775 40.89639)\n105822:  POINT (-97.64086 40.1338)\n#--- check the class ---#\nclass(wells_dt)[1] \"data.table\" \"data.frame\"\n(\n  nc_dt <- data.table(nc) %>% head()\n)    AREA PERIMETER CNTY_ CNTY_ID        NAME  FIPS FIPSNO CRESS_ID BIR74 SID74\n1: 0.114     1.442  1825    1825        Ashe 37009  37009        5  1091     1\n2: 0.061     1.231  1827    1827   Alleghany 37005  37005        3   487     0\n3: 0.143     1.630  1828    1828       Surry 37171  37171       86  3188     5\n4: 0.070     2.968  1831    1831   Currituck 37053  37053       27   508     1\n5: 0.153     2.206  1832    1832 Northampton 37131  37131       66  1421     9\n6: 0.097     1.670  1833    1833    Hertford 37091  37091       46  1452     7\n   NWBIR74 BIR79 SID79 NWBIR79 geometry\n1:      10  1364     0      19  <XY[1]>\n2:      10   542     3      12  <XY[1]>\n3:     208  3616     6     260  <XY[1]>\n4:     123   830     2     145  <XY[3]>\n5:    1066  1606     3    1197  <XY[1]>\n6:     954  1838     5    1237  <XY[1]>\n#--- take a look at what's inside the geometry column ---#\nnc_dt$geometryGeometry set for 6 features \nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -84.32385 ymin: 33.88199 xmax: -75.45698 ymax: 36.58965\nGeodetic CRS:  NAD27\nFirst 5 geometries:\n#--- check the class of the geometry column ---#\nclass(nc_dt$geometry)[1] \"sfc_MULTIPOLYGON\" \"sfc\"             \nst_buffer(nc_dt, dist = 2)Error in UseMethod(\"st_buffer\"): no applicable method for 'st_buffer' applied to an object of class \"c('data.table', 'data.frame')\"\nnc_dt$geometry %>%\n  st_buffer(dist = 2) %>%\n  head()Geometry set for 6 features \nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -81.74315 ymin: 36.07122 xmax: -75.77203 ymax: 36.59095\nGeodetic CRS:  NAD27\nFirst 5 geometries:\n#--- wells ---#\n(\n  wells_sf_again <- st_as_sf(wells_dt)\n)Simple feature collection with 105822 features and 6 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: -104.0531 ymin: 40.00161 xmax: -96.87681 ymax: 41.85942\nGeodetic CRS:  NAD83\nFirst 10 features:\n   wellid ownerid           nrdname acres  regdate section\n1       2  106106    Central Platte   160 12/30/55       3\n2       3   14133      South Platte    46  4/29/31       8\n3       4   14133      South Platte    46  4/29/31       8\n4       5   14133      South Platte    46  4/29/31       8\n5       6   15837    Central Platte   160  8/29/32      20\n6       7   90248    Central Platte   120  2/15/35      19\n7       8   48113      South Platte   113   8/7/37      28\n8      10   17073      South Platte   160   5/4/38       2\n9      11   98432 Middle Republican   807   5/6/38      36\n10     12   79294 Middle Republican   148 11/29/77      31\n                     geometry\n1  POINT (-99.58401 40.69825)\n2  POINT (-102.6249 41.11699)\n3  POINT (-102.6249 41.11699)\n4  POINT (-102.6249 41.11699)\n5   POINT (-99.6258 40.73268)\n6  POINT (-99.64524 40.73164)\n7  POINT (-103.5257 41.24492)\n8  POINT (-103.0284 41.13243)\n9   POINT (-101.1193 40.3527)\n10 POINT (-101.1146 40.35631)\n#--- nc polygons ---#\n(\n  nc_sf_again <- st_as_sf(nc_dt)\n)Simple feature collection with 6 features and 14 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -84.32385 ymin: 33.88199 xmax: -75.45698 ymax: 36.58965\nGeodetic CRS:  NAD27\n   AREA PERIMETER CNTY_ CNTY_ID        NAME  FIPS FIPSNO CRESS_ID BIR74 SID74\n1 0.114     1.442  1825    1825        Ashe 37009  37009        5  1091     1\n2 0.061     1.231  1827    1827   Alleghany 37005  37005        3   487     0\n3 0.143     1.630  1828    1828       Surry 37171  37171       86  3188     5\n4 0.070     2.968  1831    1831   Currituck 37053  37053       27   508     1\n5 0.153     2.206  1832    1832 Northampton 37131  37131       66  1421     9\n6 0.097     1.670  1833    1833    Hertford 37091  37091       46  1452     7\n  NWBIR74 BIR79 SID79 NWBIR79                       geometry\n1      10  1364     0      19 MULTIPOLYGON (((-81.47276 3...\n2      10   542     3      12 MULTIPOLYGON (((-81.23989 3...\n3     208  3616     6     260 MULTIPOLYGON (((-80.45634 3...\n4     123   830     2     145 MULTIPOLYGON (((-76.00897 3...\n5    1066  1606     3    1197 MULTIPOLYGON (((-77.21767 3...\n6     954  1838     5    1237 MULTIPOLYGON (((-76.74506 3...\nlibrary(dtplyr)\n\n#--- convert an \"lazy\" data.table ---#\nwells_ldt <- lazy_dt(wells_sf)\n\n#--- try  ---#\nst_buffer(wells_ldt, dist = 2)Error in UseMethod(\"st_buffer\"): no applicable method for 'st_buffer' applied to an object of class \"c('dtplyr_step_first', 'dtplyr_step')\""},{"path":"vector-basics.html","id":"non-interactive-geometrical-operations","chapter":"2 Vector Data Handling with sf","heading":"2.9 Non-interactive geometrical operations","text":"various geometrical operations particularly useful economists. , commonly used geometrical operations introduced53. can see practical use functions Chapter 1.4.","code":""},{"path":"vector-basics.html","id":"st_buffer","chapter":"2 Vector Data Handling with sf","heading":"2.9.1 st_buffer","text":"st_buffer() creates buffer around points, lines, border polygons.Let’s create buffers around points. First, read well locations data.spatial distribution wells (Figure 2.4).\nFigure 2.4: Map wells\nLet’s create buffers around wells.can see, many circles around wells radius \\(1,600\\) meters (Figure 2.5).\nFigure 2.5: Buffers around wells\npractical application buffer creation can seen Chapter 1.1.now create buffers around polygons. First, read NE county boundary data select three counties (Chase, Dundy, Perkins).look like (Figure 2.6):\nFigure 2.6: Map three counties\nfollowing code creates buffers around polygons (see results Figure 2.7):\nFigure 2.7: Buffers around three counties\nexample, can useful identify observations close border political boundaries want take advantage spatial discontinuity policies across adjacent political boundaries.","code":"\n#--- read wells location data ---#\nurnrd_wells_sf <-\n  readRDS(\"Data/urnrd_wells.rds\") %>%\n  #--- project to UTM 14N WGS 84 ---#\n  st_transform(32614)\ntm_shape(urnrd_wells_sf) +\n  tm_symbols(col = \"red\", size = 0.1) +\n  tm_layout(frame = FALSE)\n#--- create a one-mile buffer around the wells ---#\nwells_buffer <- st_buffer(urnrd_wells_sf, dist = 1600)\ntm_shape(wells_buffer) +\n  tm_polygons(alpha = 0) +\n  tm_shape(urnrd_wells_sf) +\n  tm_symbols(col = \"red\", size = 0.1) +\n  tm_layout(frame = NA)\nNE_counties <-\n  readRDS(\"Data/NE_county_borders.rds\") %>%\n  filter(NAME %in% c(\"Perkins\", \"Dundy\", \"Chase\")) %>%\n  st_transform(32614)\ntm_shape(NE_counties) +\n  tm_polygons(\"NAME\", palette = \"RdYlGn\", contrast = .3, title = \"County\") +\n  tm_layout(frame = NA)\nNE_buffer <- st_buffer(NE_counties, dist = 2000)\ntm_shape(NE_buffer) +\n  tm_polygons(col = \"blue\", alpha = 0.2) +\n  tm_shape(NE_counties) +\n  tm_polygons(\"NAME\", palette = \"RdYlGn\", contrast = .3, title = \"County\") +\n  tm_layout(\n    legend.outside = TRUE,\n    frame = FALSE\n  )"},{"path":"vector-basics.html","id":"st_area","chapter":"2 Vector Data Handling with sf","heading":"2.9.2 st_area","text":"st_area() function calculates area polygons.Now, can see , default class results st_area() units, accept numerical operations., let’s turn double.st_area() useful want find area-weighted average characteristics spatially joining two polygon layers using st_intersection() function (See Chapter 3.3.3).","code":"\n#--- generate area by polygon ---#\n(\n  NE_counties <- mutate(NE_counties, area = st_area(NE_counties))\n)Simple feature collection with 3 features and 10 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 239494.1 ymin: 4430632 xmax: 310778.1 ymax: 4543676\nProjected CRS: WGS 84 / UTM zone 14N\n  STATEFP COUNTYFP COUNTYNS       AFFGEOID GEOID    NAME LSAD      ALAND\n1      31      135 00835889 0500000US31135 31135 Perkins   06 2287828025\n2      31      029 00835836 0500000US31029 31029   Chase   06 2316533447\n3      31      057 00835850 0500000US31057 31057   Dundy   06 2381956151\n   AWATER                       geometry             area\n1 2840176 MULTIPOLYGON (((243340.2 45... 2302174854 [m^2]\n2 7978172 MULTIPOLYGON (((241201.4 44... 2316908196 [m^2]\n3 3046331 MULTIPOLYGON (((240811.3 44... 2389890530 [m^2]\nclass(NE_counties$area)[1] \"units\"\n(\n  NE_counties <- mutate(NE_counties, area = as.numeric(area))\n)Simple feature collection with 3 features and 10 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 239494.1 ymin: 4430632 xmax: 310778.1 ymax: 4543676\nProjected CRS: WGS 84 / UTM zone 14N\n  STATEFP COUNTYFP COUNTYNS       AFFGEOID GEOID    NAME LSAD      ALAND\n1      31      135 00835889 0500000US31135 31135 Perkins   06 2287828025\n2      31      029 00835836 0500000US31029 31029   Chase   06 2316533447\n3      31      057 00835850 0500000US31057 31057   Dundy   06 2381956151\n   AWATER                       geometry       area\n1 2840176 MULTIPOLYGON (((243340.2 45... 2302174854\n2 7978172 MULTIPOLYGON (((241201.4 44... 2316908196\n3 3046331 MULTIPOLYGON (((240811.3 44... 2389890530"},{"path":"vector-basics.html","id":"st_centroid","chapter":"2 Vector Data Handling with sf","heading":"2.9.3 st_centroid","text":"st_centroid() function finds centroid polygon.’s map output (Figure 2.8).\nFigure 2.8: centroids polygons\ncan useful creating map labels centroid polygons tend good place place labels (Figure 2.9).54\nFigure 2.9: County names placed centroids counties\nmay also useful need calculate “distance” polygons.","code":"\n#--- create centroids ---#\n(\n  NE_centroids <- st_centroid(NE_counties)\n)Simple feature collection with 3 features and 10 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 271156.7 ymin: 4450826 xmax: 276594.1 ymax: 4525635\nProjected CRS: WGS 84 / UTM zone 14N\n  STATEFP COUNTYFP COUNTYNS       AFFGEOID GEOID    NAME LSAD      ALAND\n1      31      135 00835889 0500000US31135 31135 Perkins   06 2287828025\n2      31      029 00835836 0500000US31029 31029   Chase   06 2316533447\n3      31      057 00835850 0500000US31057 31057   Dundy   06 2381956151\n   AWATER                 geometry       area\n1 2840176 POINT (276594.1 4525635) 2302174854\n2 7978172 POINT (271469.9 4489429) 2316908196\n3 3046331 POINT (271156.7 4450826) 2389890530\ntm_shape(NE_counties) +\n  tm_polygons() +\n  tm_shape(NE_centroids) +\n  tm_symbols(size = 0.5) +\n  tm_layout(\n    legend.outside = TRUE,\n    frame = FALSE\n  )\ntm_shape(NE_counties) +\n  tm_polygons() +\n  tm_shape(NE_centroids) +\n  tm_text(\"NAME\") +\n  tm_layout(\n    legend.outside = TRUE,\n    frame = FALSE\n  )"},{"path":"vector-basics.html","id":"st_length","chapter":"2 Vector Data Handling with sf","heading":"2.9.4 st_length","text":"can use st_length() calculate great circle distances55 LINESTRING MULTILINESTRING represented geodetic coordinates. hand, projected use Cartesian coordinate system, calculate Euclidean distance. use U.S. railroad data demonstration.uses geodetic coordinate system. Let’s calculate great circle distance lines (Chapter 1.4 practical use case function).","code":"\n#--- import US railroad data and take only the first 10 of it ---#\n(\n  a_railroad <- rail_roads <- st_read(dsn = \"Data\", layer = \"tl_2015_us_rails\")[1:10, ]\n)Reading layer `tl_2015_us_rails' from data source \n  `/Users/tmieno2/Dropbox/TeachingUNL/R_as_GIS/Data' using driver `ESRI Shapefile'\nSimple feature collection with 180958 features and 3 fields\nGeometry type: MULTILINESTRING\nDimension:     XY\nBounding box:  xmin: -165.4011 ymin: 17.95174 xmax: -65.74931 ymax: 65.00006\nGeodetic CRS:  NAD83Simple feature collection with 10 features and 3 fields\nGeometry type: MULTILINESTRING\nDimension:     XY\nBounding box:  xmin: -79.74031 ymin: 35.0571 xmax: -79.2377 ymax: 35.51776\nGeodetic CRS:  NAD83\n      LINEARID                    FULLNAME MTFCC                       geometry\n1  11020239500       Norfolk Southern Rlwy R1011 MULTILINESTRING ((-79.47058...\n2  11020239501       Norfolk Southern Rlwy R1011 MULTILINESTRING ((-79.46687...\n3  11020239502       Norfolk Southern Rlwy R1011 MULTILINESTRING ((-79.66819...\n4  11020239503       Norfolk Southern Rlwy R1011 MULTILINESTRING ((-79.46687...\n5  11020239504       Norfolk Southern Rlwy R1011 MULTILINESTRING ((-79.74031...\n6  11020239575      Seaboard Coast Line RR R1011 MULTILINESTRING ((-79.43695...\n7  11020239576      Seaboard Coast Line RR R1011 MULTILINESTRING ((-79.47852...\n8  11020239577      Seaboard Coast Line RR R1011 MULTILINESTRING ((-79.43695...\n9  11020239589    Aberdeen and Rockfish RR R1011 MULTILINESTRING ((-79.38736...\n10 11020239591 Aberdeen and Briar Patch RR R1011 MULTILINESTRING ((-79.53848...\n#--- check CRS ---#\nst_crs(a_railroad)Coordinate Reference System:\n  User input: NAD83 \n  wkt:\nGEOGCRS[\"NAD83\",\n    DATUM[\"North American Datum 1983\",\n        ELLIPSOID[\"GRS 1980\",6378137,298.257222101,\n            LENGTHUNIT[\"metre\",1]]],\n    PRIMEM[\"Greenwich\",0,\n        ANGLEUNIT[\"degree\",0.0174532925199433]],\n    CS[ellipsoidal,2],\n        AXIS[\"latitude\",north,\n            ORDER[1],\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        AXIS[\"longitude\",east,\n            ORDER[2],\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n    ID[\"EPSG\",4269]]\n(\n  a_railroad <- mutate(a_railroad, length = st_length(a_railroad))\n)Simple feature collection with 10 features and 4 fields\nGeometry type: MULTILINESTRING\nDimension:     XY\nBounding box:  xmin: -79.74031 ymin: 35.0571 xmax: -79.2377 ymax: 35.51776\nGeodetic CRS:  NAD83\n      LINEARID                    FULLNAME MTFCC                       geometry\n1  11020239500       Norfolk Southern Rlwy R1011 MULTILINESTRING ((-79.47058...\n2  11020239501       Norfolk Southern Rlwy R1011 MULTILINESTRING ((-79.46687...\n3  11020239502       Norfolk Southern Rlwy R1011 MULTILINESTRING ((-79.66819...\n4  11020239503       Norfolk Southern Rlwy R1011 MULTILINESTRING ((-79.46687...\n5  11020239504       Norfolk Southern Rlwy R1011 MULTILINESTRING ((-79.74031...\n6  11020239575      Seaboard Coast Line RR R1011 MULTILINESTRING ((-79.43695...\n7  11020239576      Seaboard Coast Line RR R1011 MULTILINESTRING ((-79.47852...\n8  11020239577      Seaboard Coast Line RR R1011 MULTILINESTRING ((-79.43695...\n9  11020239589    Aberdeen and Rockfish RR R1011 MULTILINESTRING ((-79.38736...\n10 11020239591 Aberdeen and Briar Patch RR R1011 MULTILINESTRING ((-79.53848...\n           length\n1    662.0043 [m]\n2    658.1026 [m]\n3  19953.8571 [m]\n4  13880.5060 [m]\n5   7183.7286 [m]\n6   1062.4649 [m]\n7   7832.3375 [m]\n8  31773.7700 [m]\n9   4544.6795 [m]\n10 17101.8581 [m]"},{"path":"int-vv.html","id":"int-vv","chapter":"3 Spatial Interactions of Vector Data: Subsetting and Joining","heading":"3 Spatial Interactions of Vector Data: Subsetting and Joining","text":"","code":""},{"path":"int-vv.html","id":"before-you-start-2","chapter":"3 Spatial Interactions of Vector Data: Subsetting and Joining","heading":"Before you start","text":"chapter learn spatial interactions two spatial objects. first look topological relations two spatial objects (spatially related ): specifically, st_intersects() st_is_within_distance(). st_intersects() particularly important far common topological relation economists use also default topological relation sf uses spatial subsetting spatial joining.follow spatial subsetting: filtering spatial data geographic features another spatial data. Finally, learn spatial joining. Spatial joining act assigning attribute values one spatial data another spatial data based two spatial datasets spatially related (topological relation). important spatial operation economists want use spatial variables econometric analysis. used sp package, operations akin sp::().","code":""},{"path":"int-vv.html","id":"direction-for-replication-2","chapter":"3 Spatial Interactions of Vector Data: Subsetting and Joining","heading":"Direction for replication","text":"DatasetsAll datasets need import available . chapter, path files set relative working directory (hidden). run codes without mess paths files, follow steps:set folder (folder) working directory using setwd()create folder called “Data” inside folder designated working directory (created “Data” folder previously, skip step)download pertinent datasets hereplace files downloaded folder “Data” folderPackagesRun following code install load (already installed) pacman package, install load (already installed) listed package inside pacman::p_load() function.","code":"\nif (!require(\"pacman\")) install.packages(\"pacman\")\npacman::p_load(\n  sf, # vector data operations\n  dplyr, # data wrangling\n  data.table, # data wrangling\n  ggplot2, # for map creation\n  tmap # for map creation\n)"},{"path":"int-vv.html","id":"topo","chapter":"3 Spatial Interactions of Vector Data: Subsetting and Joining","heading":"3.1 Topological relations","text":"learn spatial subsetting joining, first look topological relations. Topological relations refer way multiple spatial objects spatially related one another. can identify various types spatial relations using sf package. main focus intersections spatial objects, can found using st_intersects()56. also briefly cover st_is_within_distance(). interested topological relations, run ?geos_binary_pred.first create sf objects going use illustrations.POINTSLINESPOLYGONSFigure 3.1 shows look:\nFigure 3.1: Visualization points, lines, polygons\n","code":"\n#--- create points ---#\npoint_1 <- st_point(c(2, 2))\npoint_2 <- st_point(c(1, 1))\npoint_3 <- st_point(c(1, 3))\n\n#--- combine the points to make a single  sf of points ---#\n(\npoints <- list(point_1, point_2, point_3) %>% \n  st_sfc() %>% \n  st_as_sf() %>% \n  mutate(point_name = c(\"point 1\", \"point 2\", \"point 3\"))\n)Simple feature collection with 3 features and 1 field\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 1 ymin: 1 xmax: 2 ymax: 3\nCRS:           NA\n            x point_name\n1 POINT (2 2)    point 1\n2 POINT (1 1)    point 2\n3 POINT (1 3)    point 3\n#--- create points ---#\nline_1 <- st_linestring(rbind(c(0, 0), c(2.5, 0.5)))\nline_2 <- st_linestring(rbind(c(1.5, 0.5), c(2.5, 2)))\n\n#--- combine the points to make a single  sf of points ---#\n(\nlines <- list(line_1, line_2) %>% \n  st_sfc() %>% \n  st_as_sf() %>% \n  mutate(line_name = c(\"line 1\", \"line 2\"))\n)Simple feature collection with 2 features and 1 field\nGeometry type: LINESTRING\nDimension:     XY\nBounding box:  xmin: 0 ymin: 0 xmax: 2.5 ymax: 2\nCRS:           NA\n                            x line_name\n1   LINESTRING (0 0, 2.5 0.5)    line 1\n2 LINESTRING (1.5 0.5, 2.5 2)    line 2\n#--- create polygons ---#\npolygon_1 <- st_polygon(list(\n  rbind(c(0, 0), c(2, 0), c(2, 2), c(0, 2), c(0, 0)) \n))\n\npolygon_2 <- st_polygon(list(\n  rbind(c(0.5, 1.5), c(0.5, 3.5), c(2.5, 3.5), c(2.5, 1.5), c(0.5, 1.5)) \n))\n\npolygon_3 <- st_polygon(list(\n  rbind(c(0.5, 2.5), c(0.5, 3.2), c(2.3, 3.2), c(2, 2), c(0.5, 2.5)) \n))\n\n#--- combine the polygons to make an sf of polygons ---#\n(\npolygons <- list(polygon_1, polygon_2, polygon_3) %>% \n  st_sfc() %>% \n  st_as_sf() %>% \n  mutate(polygon_name = c(\"polygon 1\", \"polygon 2\", \"polygon 3\"))\n)Simple feature collection with 3 features and 1 field\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 0 ymin: 0 xmax: 2.5 ymax: 3.5\nCRS:           NA\n                               x polygon_name\n1 POLYGON ((0 0, 2 0, 2 2, 0 ...    polygon 1\n2 POLYGON ((0.5 1.5, 0.5 3.5,...    polygon 2\n3 POLYGON ((0.5 2.5, 0.5 3.2,...    polygon 3\nggplot() +\n  geom_sf(data = polygons, aes(fill = polygon_name), alpha = 0.3) +\n  scale_fill_discrete(name = \"Polygons\") +\n  geom_sf(data = lines, aes(color = line_name)) +\n  scale_color_discrete(name = \"Lines\") + \n  geom_sf(data = points, aes(shape = point_name), size = 3) +\n  scale_shape_discrete(name = \"Points\")  "},{"path":"int-vv.html","id":"st_intersects","chapter":"3 Spatial Interactions of Vector Data: Subsetting and Joining","heading":"3.1.1 st_intersects()","text":"function identifies sfg object sf (sfc) intersects sfg object(s) another sf. example, can use function identify well located within county. st_intersects() commonly used topological relation. important understand default topological relation used performing spatial subsetting joining, cover later.points polygonsAs can see, output list polygon(s) points intersect . numbers 1, 2, 3 first row mean 1st (polygon 1), 2nd (polygon 2), 3rd (polygon 3) objects polygons intersect first point (point 1) points object. fact point 1 considered intersecting polygon 2 means area inside border considered part polygon (course).like results st_intersects() matrix form boolean values filling matrix, can add sparse = FALSE option.lines polygonsThe output list polygon(s) lines intersect .polygons polygonsFor polygons vs polygons interaction, st_intersects() identifies polygons either touches (even point like polygons 1 3) share area.","code":"\nst_intersects(points, polygons)Sparse geometry binary predicate list of length 3, where the predicate\nwas `intersects'\n 1: 1, 2, 3\n 2: 1\n 3: 2, 3\nst_intersects(points, polygons, sparse = FALSE)      [,1]  [,2]  [,3]\n[1,]  TRUE  TRUE  TRUE\n[2,]  TRUE FALSE FALSE\n[3,] FALSE  TRUE  TRUE\nst_intersects(lines, polygons)Sparse geometry binary predicate list of length 2, where the predicate\nwas `intersects'\n 1: 1\n 2: 1, 2\nst_intersects(polygons, polygons)Sparse geometry binary predicate list of length 3, where the predicate\nwas `intersects'\n 1: 1, 2, 3\n 2: 1, 2, 3\n 3: 1, 2, 3"},{"path":"int-vv.html","id":"st_is_within_distance","chapter":"3 Spatial Interactions of Vector Data: Subsetting and Joining","heading":"3.1.2 st_is_within_distance()","text":"function identifies whether two spatial objects within distance specify name suggests57.Let’s first create two sets points.spatially distributed (Figure 3.2). Instead circles points, corresponding id (equivalently row number ) values displayed.\nFigure 3.2: locations set points\nwant know blue points (points_set_2) located within 0.2 red points (points_set_1). following figure (Figure 3.3) gives us answer visually.\nFigure 3.3: blue points within 0.2 radius red points\nConfirm visual inspection results outcome following code using st_is_within_distance() function.","code":"\nset.seed(38424738)\n\npoints_set_1 <- lapply(1:5, function(x) st_point(runif(2))) %>% \n  st_sfc() %>% st_as_sf() %>% \n  mutate(id = 1:nrow(.))\n\npoints_set_2 <- lapply(1:5, function(x) st_point(runif(2))) %>% \n  st_sfc() %>% st_as_sf() %>% \n  mutate(id = 1:nrow(.))\nggplot() +\n  geom_sf_text(data = points_set_1, aes(label = id), color = \"red\") +\n  geom_sf_text(data = points_set_2, aes(label = id), color = \"blue\") \n#--- create 0.2 buffers around points in points_set_1 ---#\nbuffer_1 <- st_buffer(points_set_1, dist = 0.2)\n\nggplot() +\n  geom_sf(data = buffer_1, color = \"red\", fill = NA) +\n  geom_sf_text(data = points_set_1, aes(label = id), color = \"red\") +\n  geom_sf_text(data = points_set_2, aes(label = id), color = \"blue\") \nst_is_within_distance(points_set_1, points_set_2, dist = 0.2)Sparse geometry binary predicate list of length 5, where the predicate\nwas `is_within_distance'\n 1: 1\n 2: (empty)\n 3: (empty)\n 4: (empty)\n 5: 3"},{"path":"int-vv.html","id":"spatial-subsetting-or-flagging","chapter":"3 Spatial Interactions of Vector Data: Subsetting and Joining","heading":"3.2 Spatial Subsetting (or Flagging)","text":"Spatial subsetting refers operations narrow geographic scope spatial object (source data) based another spatial object (target data). illustrate spatial subsetting using Kansas county borders, boundary High-Plains Aquifer (HPA), agricultural irrigation wells Kansas.First, let’s import files use section.","code":"\n#--- Kansas county borders ---#\nKS_counties <- readRDS(\"Data/KS_county_borders.rds\")\n\n#--- HPA boundary ---#\nhpa <- st_read(dsn = \"Data\", layer = \"hp_bound2010\") %>%\n  .[1, ] %>%\n  st_transform(st_crs(KS_counties))\n\n#--- all the irrigation wells in KS ---#\nKS_wells <- readRDS(\"Data/Kansas_wells.rds\") %>%\n  st_transform(st_crs(KS_counties))\n\n#--- US railroad ---#\nrail_roads <- st_read(dsn = \"Data\", layer = \"tl_2015_us_rails\") %>%\n  st_transform(st_crs(KS_counties)) "},{"path":"int-vv.html","id":"polygons-source-vs-polygons-target","chapter":"3 Spatial Interactions of Vector Data: Subsetting and Joining","heading":"3.2.1 polygons (source) vs polygons (target)","text":"following map (Figure 3.4) shows Kansas portion HPA Kansas counties.58\nFigure 3.4: Kansas portion High-Plains Aquifer Kansas counties\ngoal select counties intersect HPA boundary. subsetting data.frame specifying row numbers like select, can doSpatial subsetting sf objects works similar syntax:subsetting sf_1 based sf_2. Instead row numbers, provide another sf object place. following code spatially subsets Kansas counties based HPA boundary.See results Figure 3.5.\nFigure 3.5: results spatially subsetting Kansas counties based HPA boundary\ncan see counties intersect HPA boundary remained. use syntax sf_1[sf_2, ], default underlying topological relations st_intersects(). , object sf_1 intersects objects sf_2 even slightly, remain subsetting.can specify spatial operation used option inFor example, want counties completely within HPA boundary, can following (map results Figure 3.6):\nFigure 3.6: Kansas counties completely within HPA boundary\nSometimes, just want flag whether two spatial objects intersect , instead dropping non-overlapping observations. case, can use st_intersects().","code":"\n#--- add US counties layer ---#\ntm_shape(KS_counties) +\n  tm_polygons() +\n#--- add High-Plains Aquifer layer ---#\ntm_shape(hpa) +\n  tm_fill(col = \"blue\", alpha = 0.3)#--- this does not run ---#\ndata.frame[vector of row numbers, ]\n#--- this does not run ---#\nsf_1[sf_2, ]\ncounties_in_hpa <- KS_counties[hpa, ]\n#--- add US counties layer ---#\ntm_shape(counties_in_hpa) +\n  tm_polygons() +\n#--- add High-Plains Aquifer layer ---#\ntm_shape(hpa) +\n  tm_fill(col = \"blue\", alpha = 0.3)\n#--- this does not run ---#\nsf_1[sf_2, op = topological_relation_type] \ncounties_within_hpa <- KS_counties[hpa, , op = st_within]\n#--- add US counties layer ---#\ntm_shape(counties_within_hpa) +\n  tm_polygons() +\n#--- add High-Plains Aquifer layer ---#\ntm_shape(hpa) +\n  tm_fill(col = \"blue\", alpha = 0.3)\n#--- check the intersections of HPA and counties  ---#\nintersects_hpa <- st_intersects(KS_counties, hpa, sparse = FALSE)\n\n#--- take a look ---#\nhead(intersects_hpa)      [,1]\n[1,] FALSE\n[2,]  TRUE\n[3,] FALSE\n[4,]  TRUE\n[5,]  TRUE\n[6,]  TRUE\n#--- assign the index as a variable ---#\nKS_counties <- mutate(KS_counties, intersects_hpa  = intersects_hpa)\n\n#--- take a look ---#\ndplyr::select(KS_counties, COUNTYFP, intersects_hpa)Simple feature collection with 105 features and 2 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -102.0517 ymin: 36.99308 xmax: -94.59193 ymax: 40.00308\nGeodetic CRS:  NAD83\nFirst 10 features:\n   COUNTYFP intersects_hpa                       geometry\n1       133          FALSE MULTIPOLYGON (((-95.5255 37...\n2       075           TRUE MULTIPOLYGON (((-102.0446 3...\n3       123          FALSE MULTIPOLYGON (((-98.48738 3...\n4       189           TRUE MULTIPOLYGON (((-101.5566 3...\n5       155           TRUE MULTIPOLYGON (((-98.47279 3...\n6       129           TRUE MULTIPOLYGON (((-102.0419 3...\n7       073          FALSE MULTIPOLYGON (((-96.52278 3...\n8       023           TRUE MULTIPOLYGON (((-102.0517 4...\n9       089           TRUE MULTIPOLYGON (((-98.50445 4...\n10      059          FALSE MULTIPOLYGON (((-95.50827 3..."},{"path":"int-vv.html","id":"points-source-vs-polygons-target","chapter":"3 Spatial Interactions of Vector Data: Subsetting and Joining","heading":"3.2.2 points (source) vs polygons (target)","text":"following map (Figure 3.7) shows Kansas portion HPA irrigation wells Kansas.\nFigure 3.7: map Kansas irrigation wells HPA\ncan select wells reside within HPA boundary using syntax example.can see Figure 3.8 , wells inside (intersect ) HPA remained default topological relation st_intersects().\nFigure 3.8: map Kansas irrigation wells HPA\njust want flag wells intersects HPA instead dropping non-intersecting wells, use st_intersects():","code":"\ntm_shape(KS_wells) +\n  tm_symbols(size = 0.1) +\ntm_shape(hpa) +\n  tm_polygons(col = \"blue\", alpha = 0.1) \nKS_wells_in_hpa <- KS_wells[hpa, ]\ntm_shape(KS_wells_in_hpa) +\n  tm_symbols(size = 0.1) +\ntm_shape(hpa) +\n  tm_polygons(col = \"blue\", alpha = 0.1) \n(\nKS_wells <- mutate(KS_wells, in_hpa = st_intersects(KS_wells, hpa, sparse = FALSE))\n)Simple feature collection with 37647 features and 3 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: -102.0495 ymin: 36.99552 xmax: -94.62089 ymax: 40.00199\nGeodetic CRS:  NAD83\nFirst 10 features:\n   site    af_used                   geometry in_hpa\n1     1 232.099948 POINT (-100.4423 37.52046)   TRUE\n2     3  13.183940 POINT (-100.7118 39.91526)   TRUE\n3     5  99.187052 POINT (-99.15168 38.48849)   TRUE\n4     7   0.000000 POINT (-101.8995 38.78077)   TRUE\n5     8 145.520499  POINT (-100.7122 38.0731)   TRUE\n6     9   3.614535 POINT (-97.70265 39.04055)  FALSE\n7    11 188.423543 POINT (-101.7114 39.55035)   TRUE\n8    12  77.335960 POINT (-95.97031 39.16121)  FALSE\n9    15   0.000000 POINT (-98.30759 38.26787)   TRUE\n10   17 167.819034 POINT (-100.2785 37.71539)   TRUE"},{"path":"int-vv.html","id":"lines_polygons","chapter":"3 Spatial Interactions of Vector Data: Subsetting and Joining","heading":"3.2.3 lines (source) vs polygons (target)","text":"following map (Figure 3.9) shows Kansas counties U.S. railroads.\nFigure 3.9: U.S. railroads Kansas county boundaries\ncan select railroads intersect Kansas.can see Figure 3.10 , railroads intersect Kansas selected. Note lines go beyond Kansas boundary also selected. Remember, default st_intersect(). like lines beyond state boundary cut intersecting parts lines remain, use st_intersection(), explained Chapter 3.4.1.\nFigure 3.10: Railroads intersect Kansas county boundaries\nUnlike previous two cases, multiple objects (lines) checked multiple objects (polygons) intersection59. Therefore, use strategy took returning vector TRUE FALSE using sparse = TRUE option. , count number intersecting counties assign TRUE number greater 0.","code":"\ntm_shape(rail_roads) +\n  tm_lines(col = \"blue\") +\ntm_shape(KS_counties) +\n  tm_polygons(alpha = 0)  +\n  tm_layout(frame = FALSE) \nrailroads_KS <- rail_roads[KS_counties, ]\ntm_shape(railroads_KS) +\n  tm_lines(col = \"blue\") +\ntm_shape(KS_counties) +\n  tm_polygons(alpha = 0)  +\n  tm_layout(frame = FALSE) \n#--- check the number of intersecting KS counties ---#\nint_mat <- st_intersects(rail_roads, KS_counties) %>% \n  lapply(length) %>% \n  unlist() \n\n#--- railroads ---#\nrail_roads <- mutate(rail_roads, intersect_ks  = int_mat > 0)\n\n#--- take a look ---#\ndplyr::select(rail_roads, LINEARID, intersect_ks)Simple feature collection with 180958 features and 2 fields\nGeometry type: MULTILINESTRING\nDimension:     XY\nBounding box:  xmin: -165.4011 ymin: 17.95174 xmax: -65.74931 ymax: 65.00006\nGeodetic CRS:  NAD83\nFirst 10 features:\n      LINEARID intersect_ks                       geometry\n1  11020239500        FALSE MULTILINESTRING ((-79.47058...\n2  11020239501        FALSE MULTILINESTRING ((-79.46687...\n3  11020239502        FALSE MULTILINESTRING ((-79.66819...\n4  11020239503        FALSE MULTILINESTRING ((-79.46687...\n5  11020239504        FALSE MULTILINESTRING ((-79.74031...\n6  11020239575        FALSE MULTILINESTRING ((-79.43695...\n7  11020239576        FALSE MULTILINESTRING ((-79.47852...\n8  11020239577        FALSE MULTILINESTRING ((-79.43695...\n9  11020239589        FALSE MULTILINESTRING ((-79.38736...\n10 11020239591        FALSE MULTILINESTRING ((-79.53848..."},{"path":"int-vv.html","id":"polygons_points","chapter":"3 Spatial Interactions of Vector Data: Subsetting and Joining","heading":"3.2.4 polygons (source) vs points (target)","text":"following map (Figure 3.11) shows Kansas counties irrigation wells Kansas overlie HPA.\nFigure 3.11: Kansas county boundaries wells overlie HPA\ncan select counties intersect least one well.can see Figure 3.12 , counties intersect least one well remained.\nFigure 3.12: Counties least one well\nflag counties least one well, use st_intersects() follows:","code":"\ntm_shape(KS_counties) +\n  tm_polygons(alpha = 0) +\ntm_shape(KS_wells_in_hpa) +\n  tm_symbols(size = 0.1) +\n  tm_layout(frame = FALSE)\nKS_counties_intersected <- KS_counties[KS_wells_in_hpa, ]  \ntm_shape(KS_counties) +\n  tm_polygons(col = NA) +\ntm_shape(KS_counties_intersected) +\n  tm_polygons(col =\"blue\", alpha = 0.6) +\n  tm_layout(frame = FALSE)\nint_mat <- st_intersects(KS_counties, KS_wells_in_hpa) %>% \n  lapply(length) %>% \n  unlist()\n\n#--- railroads ---#\nKS_counties <- mutate(KS_counties, intersect_wells  = int_mat > 0)\n\n#--- take a look ---#\ndplyr::select(KS_counties, NAME, COUNTYFP, intersect_wells)Simple feature collection with 105 features and 3 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -102.0517 ymin: 36.99308 xmax: -94.59193 ymax: 40.00308\nGeodetic CRS:  NAD83\nFirst 10 features:\n        NAME COUNTYFP intersect_wells                       geometry\n1     Neosho      133           FALSE MULTIPOLYGON (((-95.5255 37...\n2   Hamilton      075            TRUE MULTIPOLYGON (((-102.0446 3...\n3   Mitchell      123           FALSE MULTIPOLYGON (((-98.48738 3...\n4    Stevens      189            TRUE MULTIPOLYGON (((-101.5566 3...\n5       Reno      155            TRUE MULTIPOLYGON (((-98.47279 3...\n6     Morton      129            TRUE MULTIPOLYGON (((-102.0419 3...\n7  Greenwood      073           FALSE MULTIPOLYGON (((-96.52278 3...\n8   Cheyenne      023            TRUE MULTIPOLYGON (((-102.0517 4...\n9     Jewell      089            TRUE MULTIPOLYGON (((-98.50445 4...\n10  Franklin      059           FALSE MULTIPOLYGON (((-95.50827 3..."},{"path":"int-vv.html","id":"subsetting-to-a-geographic-extent-bounding-box","chapter":"3 Spatial Interactions of Vector Data: Subsetting and Joining","heading":"3.2.5 Subsetting to a geographic extent (bounding box)","text":"can use st_crop() crop spatial objects spatial bounding box (extent) spatial object. bounding box sf rectangle represented minimum maximum x y encompass/contain spatial objects sf. can use st_bbox() find bounding box sf object. Let’s get bounding box KS_wells_in_hpa (irrigation wells Kansas overlie HPA).Visualizing bounding box (Figure 3.13):\nFigure 3.13: bounding box irrigation wells Kansas overlie HPA\nuse bounding box crop sf objects, can consider bounding box single polygon.Let’s crop KS_counties using bbox irrigation wells.cropped data looks like (Figure 3.14):\nFigure 3.14: bounding box irrigation wells Kansas overlie HPA\ncan see, st_crop() operation cut counties right edge bounding box. , st_crop() invasive. like happen want complete original counties least one well, can use subset approach using [, ] converting bounding box sfc follows:subsetted Kansas county data looks like (Figure 3.15):\nFigure 3.15: bounding box irrigation wells Kansas overlie HPA\nNotice difference result operation case used KS_wells_in_hpa directly subset KS_counties shown Figure 3.12. current approach includes counties irrigation wells inside .","code":"\n#--- get the bounding box of KS_wells ---#\n(\nbbox_KS_wells_in_hpa <- st_bbox(KS_wells_in_hpa)  \n)      xmin       ymin       xmax       ymax \n-102.04953   36.99552  -97.33193   40.00199 \n#--- check the class ---#\nclass(bbox_KS_wells_in_hpa)[1] \"bbox\"\ntm_shape(KS_counties) +\n  tm_polygons(alpha = 0) +\ntm_shape(KS_wells_in_hpa) +\n  tm_symbols(size = 0.1) +\ntm_shape(st_as_sfc(bbox_KS_wells_in_hpa)) +\n  tm_borders(col = \"red\") +\n  tm_layout(frame = NA)\nKS_cropped <- st_crop(KS_counties, bbox_KS_wells_in_hpa)  \ntm_shape(KS_counties) +\n  tm_polygons(col = NA) +\ntm_shape(KS_cropped) +\n  tm_polygons(col =\"blue\", alpha = 0.6) +\n  tm_layout(frame = NA)\nKS_complete_counties <- KS_counties[st_as_sfc(bbox_KS_wells_in_hpa), ]  \ntm_shape(KS_counties) +\n  tm_polygons(col = NA) +\ntm_shape(KS_complete_counties) +\n  tm_polygons(col =\"blue\", alpha = 0.6) +\n  tm_layout(frame = NA)"},{"path":"int-vv.html","id":"sp-join","chapter":"3 Spatial Interactions of Vector Data: Subsetting and Joining","heading":"3.3 Spatial Join","text":"spatial join, mean spatial operations involve following:overlay one spatial layer (target layer) onto another spatial layer (source layer)observation target layer\nidentify objects source layer geographically intersects (different topological relation) \nextract values associated intersecting objects source layer (summarize necessary),\nassign extracted value object target layer\nidentify objects source layer geographically intersects (different topological relation) withextract values associated intersecting objects source layer (summarize necessary),assign extracted value object target layerFor economists, probably common motivation using GIS software, ultimate goal include spatially joined variables covariates regression analysis.can classify spatial join four categories type underlying spatial objects:vector-vector: vector data (target) vector data (source)vector-raster: vector data (target) raster data (source)raster-vector: raster data (target) vector data (source)raster-raster: raster data (target) raster data (source)Among four, focus first case. second case discussed Chapter 5. cover third fourth cases course almost always case target data vector data (e.g., city farm fields points, political boundaries polygons, etc).Category 1 can broken different sub categories depending type spatial object (point, line, polygon). , ignore spatial joins involve lines. objects represented lines rarely observation units econometric analysis source data extract values.60 list types spatial joins learn.points (target) polygons (source)polygons (target) points (source)polygons (target) polygons (source)","code":""},{"path":"int-vv.html","id":"case-1-points-target-vs-polygons-source","chapter":"3 Spatial Interactions of Vector Data: Subsetting and Joining","heading":"3.3.1 Case 1: points (target) vs polygons (source)","text":"Case 1, observations (points) target data, finds polygon source data intersects, assign value associated polygon point61. order achieve , can use st_join() function, whose syntax follows:Similar spatial subsetting, default topological relation st_intersects()62.use Kansas irrigation well data (points) Kansas county boundary data (polygons) demonstration. goal assign county-level corn price information Kansas county data wells. First let create add fake county-level corn price variable Kansas county data.map Kansas counties color-differentiated fake corn price (Figure 3.16):\nFigure 3.16: Map county-level fake corn price\nparticular context, following code job:can see Figure 3.17 wells inside county corn price value.\nFigure 3.17: Map wells color-differentiated corn price\n","code":"\n#--- this does not run ---#\nst_join(target_sf, source_sf)\nKS_corn_price <- KS_counties %>%  \n  mutate(corn_price = seq(3.2, 3.9, length = nrow(.))) %>% \n  dplyr::select(COUNTYFP, corn_price)\ntm_shape(KS_corn_price) + \n  tm_polygons(col = \"corn_price\") +\n  tm_layout(frame = FALSE, legend.outside = TRUE)\n#--- spatial join ---#\n(\nKS_wells_County <- st_join(KS_wells, KS_corn_price)\n)Simple feature collection with 37647 features and 5 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: -102.0495 ymin: 36.99552 xmax: -94.62089 ymax: 40.00199\nGeodetic CRS:  NAD83\nFirst 10 features:\n   site    af_used in_hpa COUNTYFP corn_price                   geometry\n1     1 232.099948   TRUE      069   3.556731 POINT (-100.4423 37.52046)\n2     3  13.183940   TRUE      039   3.449038 POINT (-100.7118 39.91526)\n3     5  99.187052   TRUE      165   3.287500 POINT (-99.15168 38.48849)\n4     7   0.000000   TRUE      199   3.644231 POINT (-101.8995 38.78077)\n5     8 145.520499   TRUE      055   3.832692  POINT (-100.7122 38.0731)\n6     9   3.614535  FALSE      143   3.799038 POINT (-97.70265 39.04055)\n7    11 188.423543   TRUE      181   3.590385 POINT (-101.7114 39.55035)\n8    12  77.335960  FALSE      177   3.550000 POINT (-95.97031 39.16121)\n9    15   0.000000   TRUE      159   3.610577 POINT (-98.30759 38.26787)\n10   17 167.819034   TRUE      069   3.556731 POINT (-100.2785 37.71539)\ntm_shape(KS_counties) +\n  tm_polygons() +\ntm_shape(KS_wells_County) +\n  tm_symbols(col = \"corn_price\", size = 0.1) +\n  tm_layout(frame = FALSE, legend.outside = TRUE)"},{"path":"int-vv.html","id":"case-2-polygons-target-vs-points-source","chapter":"3 Spatial Interactions of Vector Data: Subsetting and Joining","heading":"3.3.2 Case 2: polygons (target) vs points (source)","text":"Case 2, observations (polygons) target data, find observations (points) source data intersects, assign values associated points polygon. use function: st_join()63.Suppose now interested county-level analysis like get county-level total groundwater pumping. target data KS_counties, source data KS_wells.can see resulting dataset, unique polygon - point intersecting combinations comprise observations. polygons, many observations number wells intersect polygon. join two layers, can find statistics polygon (county ). Since want groundwater extraction county, following job.course, just easy get types statistics simply modifying summarize() part.However, two-step process can actually done one step using aggregate(), specify want aggregate FUN option follows:Notice mean() function applied columns KS_wells, including site id number. , might want select variables want join apply aggregate() function like :","code":"\n#--- spatial join ---#\nKS_County_wells <- st_join(KS_counties, KS_wells)\n\n#--- take a look ---#\ndplyr::select(KS_County_wells, COUNTYFP, site, af_used)Simple feature collection with 37652 features and 3 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -102.0517 ymin: 36.99308 xmax: -94.59193 ymax: 40.00308\nGeodetic CRS:  NAD83\nFirst 10 features:\n    COUNTYFP  site   af_used                       geometry\n1        133 53861  17.01790 MULTIPOLYGON (((-95.5255 37...\n1.1      133 70592   0.00000 MULTIPOLYGON (((-95.5255 37...\n2        075   328 394.04513 MULTIPOLYGON (((-102.0446 3...\n2.1      075   336  80.65036 MULTIPOLYGON (((-102.0446 3...\n2.2      075   436 568.25359 MULTIPOLYGON (((-102.0446 3...\n2.3      075  1007 215.80416 MULTIPOLYGON (((-102.0446 3...\n2.4      075  1170   0.00000 MULTIPOLYGON (((-102.0446 3...\n2.5      075  1192  77.39120 MULTIPOLYGON (((-102.0446 3...\n2.6      075  1249   0.00000 MULTIPOLYGON (((-102.0446 3...\n2.7      075  1300 320.22612 MULTIPOLYGON (((-102.0446 3...\nKS_County_wells %>% \n  group_by(COUNTYFP) %>% \n  summarize(af_used = sum(af_used, na.rm = TRUE)) Simple feature collection with 105 features and 2 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: -102.0517 ymin: 36.99308 xmax: -94.59193 ymax: 40.00308\nGeodetic CRS:  NAD83\n# A tibble: 105 × 3\n   COUNTYFP af_used                                                     geometry\n   <fct>      <dbl>                                                <POLYGON [°]>\n 1 001           0  ((-95.5255 37.73276, -95.08808 37.73248, -95.07969 37.8198,…\n 2 003           0  ((-95.51897 38.03823, -95.07788 38.03771, -95.06583 38.3899…\n 3 005         771. ((-95.57035 39.41905, -95.18089 39.41922, -94.99785 39.4188…\n 4 007        4972. ((-99.0115 37.38426, -99.00138 37.37502, -99.0003 36.99936,…\n 5 009       61083. ((-99.03241 38.34833, -99.03231 38.26123, -98.91258 38.2610…\n 6 011           0  ((-95.08801 37.67452, -94.61787 37.67311, -94.61789 37.6822…\n 7 013         480. ((-95.78894 39.653, -95.56413 39.65287, -95.33974 39.65298,…\n 8 015         343. ((-97.15333 37.47554, -96.52569 37.4764, -96.5253 37.60701,…\n 9 017           0  ((-96.84077 38.08562, -96.52278 38.08637, -96.3581 38.08582…\n10 019           0  ((-96.52558 36.99868, -96.50029 36.99864, -96.21757 36.9990…\n# … with 95 more rows\n#--- mean ---#\naggregate(KS_wells, KS_counties, FUN = mean)Simple feature collection with 105 features and 3 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -102.0517 ymin: 36.99308 xmax: -94.59193 ymax: 40.00308\nGeodetic CRS:  NAD83\nFirst 10 features:\n       site    af_used    in_hpa                       geometry\n1  62226.50   8.508950 0.0000000 MULTIPOLYGON (((-95.5255 37...\n2  35153.23 175.890557 0.4466292 MULTIPOLYGON (((-102.0446 3...\n3  40086.82  35.465123 0.0000000 MULTIPOLYGON (((-98.48738 3...\n4  40191.67 286.324733 1.0000000 MULTIPOLYGON (((-101.5566 3...\n5  51256.15  46.013373 0.9743976 MULTIPOLYGON (((-98.47279 3...\n6  33033.13 202.612377 1.0000000 MULTIPOLYGON (((-102.0419 3...\n7  29840.40   0.000000 0.0000000 MULTIPOLYGON (((-96.52278 3...\n8  28235.82  94.585634 0.9736842 MULTIPOLYGON (((-102.0517 4...\n9  36180.06  44.033911 0.3000000 MULTIPOLYGON (((-98.50445 4...\n10 40016.00   1.142775 0.0000000 MULTIPOLYGON (((-95.50827 3...\n#--- sum ---#\naggregate(KS_wells, KS_counties, FUN = sum)Simple feature collection with 105 features and 3 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -102.0517 ymin: 36.99308 xmax: -94.59193 ymax: 40.00308\nGeodetic CRS:  NAD83\nFirst 10 features:\n       site      af_used in_hpa                       geometry\n1    124453 1.701790e+01      0 MULTIPOLYGON (((-95.5255 37...\n2  12514550 6.261704e+04    159 MULTIPOLYGON (((-102.0446 3...\n3   1964254 1.737791e+03      0 MULTIPOLYGON (((-98.48738 3...\n4  42442400 3.023589e+05   1056 MULTIPOLYGON (((-101.5566 3...\n5  68068173 6.110576e+04   1294 MULTIPOLYGON (((-98.47279 3...\n6  15756801 9.664610e+04    477 MULTIPOLYGON (((-102.0419 3...\n7    149202 0.000000e+00      0 MULTIPOLYGON (((-96.52278 3...\n8  17167377 5.750807e+04    592 MULTIPOLYGON (((-102.0517 4...\n9   1809003 2.201696e+03     15 MULTIPOLYGON (((-98.50445 4...\n10   160064 4.571102e+00      0 MULTIPOLYGON (((-95.50827 3...\naggregate(dplyr::select(KS_wells, af_used), KS_counties, FUN = mean)Simple feature collection with 105 features and 1 field\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -102.0517 ymin: 36.99308 xmax: -94.59193 ymax: 40.00308\nGeodetic CRS:  NAD83\nFirst 10 features:\n      af_used                       geometry\n1    8.508950 MULTIPOLYGON (((-95.5255 37...\n2  175.890557 MULTIPOLYGON (((-102.0446 3...\n3   35.465123 MULTIPOLYGON (((-98.48738 3...\n4  286.324733 MULTIPOLYGON (((-101.5566 3...\n5   46.013373 MULTIPOLYGON (((-98.47279 3...\n6  202.612377 MULTIPOLYGON (((-102.0419 3...\n7    0.000000 MULTIPOLYGON (((-96.52278 3...\n8   94.585634 MULTIPOLYGON (((-102.0517 4...\n9   44.033911 MULTIPOLYGON (((-98.50445 4...\n10   1.142775 MULTIPOLYGON (((-95.50827 3..."},{"path":"int-vv.html","id":"polygon-polygon","chapter":"3 Spatial Interactions of Vector Data: Subsetting and Joining","heading":"3.3.3 Case 3: polygons (target) vs polygons (source)","text":"case, st_join(target_sf, source_sf) return unique intersecting polygon-polygon combinations information polygon source_sf attached.use county-level corn acres Iowa 2018 USDA NASS64 Hydrologic Units65 objective find corn acres HUC units based county-level corn acres data66.first import Iowa corn acre data:map Iowa counties color-differentiated corn acres (Figure 3.18):\nFigure 3.18: Map Iowa counties color-differentiated corn planted acreage\nNow import HUC units data:map HUC units (Figure 3.19):\nFigure 3.19: Map HUC units intersect Iowa state boundary\nmap Iowa counties HUC units superimposed top (Figure 3.20):\nFigure 3.20: Map HUC units superimposed counties Iowas\nSpatial joining produce following.intersecting HUC-county combinations becomes observation resulting geometry geometry HUC unit. see , let’s take look one HUC units.HUC unit HUC_CODE ==10170203 intersects four County.Figure 3.21 shows map four observations.\nFigure 3.21: Map HUC unit\n, four observations identical geometry, geometry HUC unit, meaning st_join() leave information nature intersection HUC unit four counties. , remember default option st_intersects(), checks whether spatial objects intersect , nothing . just calculating simple average corn acres ignoring degree spatial overlaps, just fine. However, like calculate area-weighted average, sufficient information. see find area-weighted average next subsection.","code":"\n#--- IA boundary ---#\nIA_corn <- readRDS(\"Data/IA_corn.rds\")\n\n#--- take a look ---#\nIA_cornSimple feature collection with 93 features and 3 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 203228.6 ymin: 4470941 xmax: 736832.9 ymax: 4822687\nProjected CRS: NAD83 / UTM zone 15N\nFirst 10 features:\n   county_code year  acres                       geometry\n1          083 2018 183500 MULTIPOLYGON (((458997 4711...\n2          141 2018 167000 MULTIPOLYGON (((267700.8 47...\n3          081 2018 184500 MULTIPOLYGON (((421231.2 47...\n4          019 2018 189500 MULTIPOLYGON (((575285.6 47...\n5          023 2018 165500 MULTIPOLYGON (((497947.5 47...\n6          195 2018 111500 MULTIPOLYGON (((459791.6 48...\n7          063 2018 110500 MULTIPOLYGON (((345214.3 48...\n8          027 2018 183000 MULTIPOLYGON (((327408.5 46...\n9          121 2018  70000 MULTIPOLYGON (((396378.1 45...\n10         077 2018 107000 MULTIPOLYGON (((355180.1 46...\n#--- here is the map ---#\ntm_shape(IA_corn) +\n  tm_polygons(col = \"acres\") +\n  tm_layout(frame = FALSE, legend.outside = TRUE)\n#--- import HUC units ---#\nHUC_IA <- st_read(dsn = \"Data\", layer = \"huc250k\") %>% \n  dplyr::select(HUC_CODE) %>% \n  #--- reproject to the CRS of IA ---#\n  st_transform(st_crs(IA_corn)) %>% \n  #--- select HUC units that overlaps with IA ---#\n  .[IA_corn, ]\ntm_shape(HUC_IA) +\n  tm_polygons() +\n  tm_layout(frame = FALSE, legend.outside = TRUE)\ntm_shape(IA_corn) +\n  tm_polygons(col = \"acres\") +\ntm_shape(HUC_IA) +\n  tm_polygons(alpha = 0) +\n  tm_layout(frame = FALSE, legend.outside = TRUE)\n(\nHUC_joined <- st_join(HUC_IA, IA_corn)\n)Simple feature collection with 349 features and 4 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 154941.7 ymin: 4346327 xmax: 773299.8 ymax: 4907735\nProjected CRS: NAD83 / UTM zone 15N\nFirst 10 features:\n      HUC_CODE county_code year  acres                       geometry\n608   10170203         149 2018 226500 POLYGON ((235551.4 4907513,...\n608.1 10170203         167 2018 249000 POLYGON ((235551.4 4907513,...\n608.2 10170203         193 2018 201000 POLYGON ((235551.4 4907513,...\n608.3 10170203         119 2018 184500 POLYGON ((235551.4 4907513,...\n621   07020009         063 2018 110500 POLYGON ((408580.7 4880798,...\n621.1 07020009         109 2018 304000 POLYGON ((408580.7 4880798,...\n621.2 07020009         189 2018 120000 POLYGON ((408580.7 4880798,...\n627   10170204         141 2018 167000 POLYGON ((248115.2 4891652,...\n627.1 10170204         143 2018 116000 POLYGON ((248115.2 4891652,...\n627.2 10170204         167 2018 249000 POLYGON ((248115.2 4891652,...\n#--- get the HUC unit with `HUC_CODE ==10170203`  ---#\n(\ntemp_HUC_county <- filter(HUC_joined, HUC_CODE == 10170203)\n)Simple feature collection with 4 features and 4 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 154941.7 ymin: 4709628 xmax: 248115.2 ymax: 4907735\nProjected CRS: NAD83 / UTM zone 15N\n      HUC_CODE county_code year  acres                       geometry\n608   10170203         149 2018 226500 POLYGON ((235551.4 4907513,...\n608.1 10170203         167 2018 249000 POLYGON ((235551.4 4907513,...\n608.2 10170203         193 2018 201000 POLYGON ((235551.4 4907513,...\n608.3 10170203         119 2018 184500 POLYGON ((235551.4 4907513,...\ntm_shape(temp_HUC_county) +\n  tm_polygons() +\n  tm_layout(frame = FALSE)"},{"path":"int-vv.html","id":"spatial-intersection-cropping-join","chapter":"3 Spatial Interactions of Vector Data: Subsetting and Joining","heading":"3.4 Spatial Intersection (cropping join)","text":"Sometimes face need crop spatial objects polygon boundaries. example, found total length railroads inside county Demonstration 4 Chapter 1.4 cutting parts railroads extend beyond boundary counties. Also, just saw area-weighted averages found using st_join() provide information much area HUC unit intersecting intersecting counties. can get geometry intersecting part HUC unit county, can calculate area, turn allows us find area-weighted averages joined attributes. purposes, can use sf::st_intersection(). , first illustrate st_intersection() works lines-polygons polygons-polygons intersections (Note use data generated Chapter 3.1). Intersections involve points using st_intersection() using st_join() points length-less area-less (nothing cut). Thus, discussed .","code":""},{"path":"int-vv.html","id":"st-intersection","chapter":"3 Spatial Interactions of Vector Data: Subsetting and Joining","heading":"3.4.1 st_intersection()","text":"st_intersects() returns indices intersecting objects, st_intersection() returns intersecting spatial objects non-intersecting parts sf objects cut . Moreover, attribute values source sf merged intersecting sfg target sf. see works lines-polygons polygons-polygons cases using toy examples used explain st_intersects() work. figure lines polygons (Figure 3.22):\nFigure 3.22: Visualization points, lines, polygons\nlines polygonsThe following code gets intersection lines polygons.can see output, instance intersections lines polygons become observation (line 1-polygon 1, line 2-polygon 1, line 2-polygon 2). part lines intersect polygon cut remain returned sf. see , see Figure 3.23 :\nFigure 3.23: outcome intersections lines polygons\nallows us calculate length part lines completely contained polygons, just like Chapter 1.4. Note also attribute (polygon_name) source sf (polygons) merged intersecting lines. Therefore, st_intersection() transforming original geometries joining attributes (call cropping join).polygons polygonsThe following code gets intersection polygon 1 polygon 3 polygon 2.can see Figure 3.24, instance intersections polygons 1 3 polygon 2 becomes observation (polygon 1-polygon 2 polygon 3-polygon 2). Just like lines-polygons case, non-intersecting part polygons 1 3 cut remain returned sf. see later st_intersection() can used find area-weighted values intersecting polygons help st_area().\nFigure 3.24: outcome intersections polygon 2 polygons 1 3\n","code":"\nggplot() +\n  geom_sf(data = polygons, aes(fill = polygon_name), alpha = 0.3) +\n  scale_fill_discrete(name = \"Polygons\") +\n  geom_sf(data = lines, aes(color = line_name)) +\n  scale_color_discrete(name = \"Lines\") \n(\nintersections_lp <- st_intersection(lines, polygons) %>% \n  mutate(int_name = paste0(line_name, \"-\", polygon_name))\n)Simple feature collection with 3 features and 3 fields\nGeometry type: LINESTRING\nDimension:     XY\nBounding box:  xmin: 0 ymin: 0 xmax: 2.5 ymax: 2\nCRS:           NA\n    line_name polygon_name                              x         int_name\n1      line 1    polygon 1        LINESTRING (0 0, 2 0.4) line 1-polygon 1\n2      line 2    polygon 1   LINESTRING (1.5 0.5, 2 1.25) line 2-polygon 1\n2.1    line 2    polygon 2 LINESTRING (2.166667 1.5, 2... line 2-polygon 2\nggplot() +\n  #--- here are all the original polygons  ---#\n  geom_sf(data = polygons, aes(fill = polygon_name), alpha = 0.3) +\n  #--- here is what is returned after st_intersection ---#\n  geom_sf(data = intersections_lp, aes(color = int_name), size = 1.5)\n(\nintersections_pp <- st_intersection(polygons[c(1,3), ], polygons[2, ]) %>% \n  mutate(int_name = paste0(polygon_name, \"-\", polygon_name.1))\n)Simple feature collection with 2 features and 3 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 0.5 ymin: 1.5 xmax: 2.3 ymax: 3.2\nCRS:           NA\n  polygon_name polygon_name.1                              x\n1    polygon 1      polygon 2 POLYGON ((2 2, 2 1.5, 0.5 1...\n3    polygon 3      polygon 2 POLYGON ((0.5 3.2, 2.3 3.2,...\n             int_name\n1 polygon 1-polygon 2\n3 polygon 3-polygon 2\nggplot() +\n  #--- here are all the original polygons  ---#\n  geom_sf(data = polygons, aes(fill = polygon_name), alpha = 0.3) +\n  #--- here is what is returned after st_intersection ---#\n  geom_sf(data = intersections_pp, aes(fill = int_name))"},{"path":"int-vv.html","id":"area-weighted-average","chapter":"3 Spatial Interactions of Vector Data: Subsetting and Joining","heading":"3.4.2 Area-weighted average","text":"Let’s now get back example HUC units county-level corn acres data saw Chapter 3.3. like find area-weighted average corn acres instead simple average corn acres.Using st_intersection(), HUC polygons, find intersecting counties, divide parts based boundary intersecting polygons.key difference st_join() example observation returned data unique HUC-county intersection. Figure 3.25 map intersections HUC unit HUC_CODE ==10170203 four intersecting counties.\nFigure 3.25: Intersections HUC unit Iowa counties\nNote also attributes county data joined can see acres output . said earlier, st_intersection() spatial kind spatial join resulting observations intersections target source sf objects.order find area-weighted average corn acres, can use st_area() first calculate area intersections, find area-weighted average follows:","code":"\n(\nHUC_intersections <- st_intersection(HUC_IA, IA_corn) %>% \n  mutate(huc_county = paste0(HUC_CODE, \"-\", county_code))\n)Simple feature collection with 349 features and 5 fields\nGeometry type: GEOMETRY\nDimension:     XY\nBounding box:  xmin: 203228.6 ymin: 4470941 xmax: 736832.9 ymax: 4822687\nProjected CRS: NAD83 / UTM zone 15N\nFirst 10 features:\n      HUC_CODE county_code year  acres                       geometry\n732   07080207         083 2018 183500 POLYGON ((482923.9 4711643,...\n803   07080205         083 2018 183500 POLYGON ((499725.6 4696873,...\n826   07080105         083 2018 183500 POLYGON ((461853.2 4682925,...\n627   10170204         141 2018 167000 POLYGON ((269364.9 4793311,...\n687   10230003         141 2018 167000 POLYGON ((271504.3 4754685,...\n731   10230002         141 2018 167000 POLYGON ((267684.6 4790972,...\n683   07100003         081 2018 184500 POLYGON ((435951.2 4789348,...\n686   07080203         081 2018 184500 MULTIPOLYGON (((459303.2 47...\n732.1 07080207         081 2018 184500 POLYGON ((429573.1 4779788,...\n747   07100005         081 2018 184500 POLYGON ((421044.8 4772268,...\n        huc_county\n732   07080207-083\n803   07080205-083\n826   07080105-083\n627   10170204-141\n687   10230003-141\n731   10230002-141\n683   07100003-081\n686   07080203-081\n732.1 07080207-081\n747   07100005-081\ntm_shape(filter(HUC_intersections, HUC_CODE == \"10170203\")) + \n  tm_polygons(col = \"huc_county\") +\n  tm_layout(frame = FALSE)\n(\nHUC_aw_acres <- HUC_intersections %>% \n  #--- get area ---#\n  mutate(area = as.numeric(st_area(.))) %>% \n  #--- get area-weight by HUC unit ---#\n  group_by(HUC_CODE) %>% \n  mutate(weight = area / sum(area)) %>% \n  #--- calculate area-weighted corn acreage by HUC unit ---#\n  summarize(aw_acres = sum(weight * acres))\n)Simple feature collection with 55 features and 2 fields\nGeometry type: GEOMETRY\nDimension:     XY\nBounding box:  xmin: 203228.6 ymin: 4470941 xmax: 736832.9 ymax: 4822687\nProjected CRS: NAD83 / UTM zone 15N\n# A tibble: 55 × 3\n   HUC_CODE aw_acres                                                    geometry\n   <chr>       <dbl>                                              <GEOMETRY [m]>\n 1 07020009  251185. POLYGON ((421060.3 4797550, 420940.3 4797420, 420860.3 479…\n 2 07040008  165000  POLYGON ((602922.4 4817166, 602862.4 4816996, 602772.4 481…\n 3 07060001  105234. MULTIPOLYGON (((649333.8 4761761, 648933.7 4761621, 648793…\n 4 07060002  140201. MULTIPOLYGON (((593780.7 4816946, 594000.7 4816865, 594380…\n 5 07060003  149000  MULTIPOLYGON (((692011.6 4712966, 691981.6 4712956, 691881…\n 6 07060004  162123. POLYGON ((652046.5 4718813, 651916.4 4718783, 651786.4 471…\n 7 07060005  142428. POLYGON ((734140.4 4642126, 733620.4 4641926, 733520.3 464…\n 8 07060006  159635. POLYGON ((720485.9 4656371, 720355.8 4656291, 720275.8 465…\n 9 07080101  115572. POLYGON ((666760.8 4558401, 666630.9 4558362, 666510.8 455…\n10 07080102  160008. POLYGON ((635896.1 4675834, 636036.2 4675844, 636316.3 467…\n# … with 45 more rows"},{"path":"raster-basics.html","id":"raster-basics","chapter":"4 Raster Data Handling","heading":"4 Raster Data Handling","text":"","code":""},{"path":"raster-basics.html","id":"before-you-start-3","chapter":"4 Raster Data Handling","heading":"Before you start","text":"chapter, learn use raster terra package handle raster data. raster package (must say still ) popular commonly used package raster data handling. However, period transitioning raster package terra package. terra package active development replace raster package (see --date version package ). terra written C++ thus faster raster package many raster data operations. raster terra packages share function name many raster operations. Key differences discussed become clear later.economists, raster data extraction vector data far common use case raster data also time-consuming part whole raster data handling experience. Therefore, introduce essential knowledge raster data operation required effectively implement task extracting values, covered extensively Chapter 5. example, cover raster arithmetic, focal operations, aggregation. interested fuller treatment raster terra package referred Spatial Data Science R “terra” Chapters 3, 4, 5 Geocomputation R, respectively.Even though terra package replacement raster package CRAN year, still learn raster object classes defined raster package switch raster terra object classes. useful packages us economists written work raster object classes still adapted support terra object classes moment.Finally, might benefit learning stars package raster data operations (covered Chapter 7), particularly often work raster data temporal dimension (e.g., PRISM, Daymet). provides data model makes working raster data temporal dimensions easier. also allows apply dplyr verbs data wrangling.","code":""},{"path":"raster-basics.html","id":"direction-for-replication-3","chapter":"4 Raster Data Handling","heading":"Direction for replication","text":"DatasetsAll datasets need import available . chapter, path files set relative working directory (hidden). run codes without mess paths files, follow steps:set folder (folder) working directory using setwd()create folder called “Data” inside folder designated working directory (created “Data” folder previously, skip step)download pertinent datasets hereplace files downloaded folder “Data” folderPackagesRun following code install load (already installed) pacman package, install load (already installed) listed package inside pacman::p_load() function.","code":"\nif (!require(\"pacman\")) install.packages(\"pacman\")\npacman::p_load(\n  terra, # handle raster data\n  raster, # handle raster data\n  cdlTools, # download CDL data\n  mapview, # create interactive maps\n  dplyr, # data wrangling\n  sf # vector data handling\n)"},{"path":"raster-basics.html","id":"raster-data-object-classes","chapter":"4 Raster Data Handling","heading":"4.1 Raster data object classes","text":"","code":""},{"path":"raster-basics.html","id":"raster-package-rasterlayer-rasterstack-and-rasterbrick","chapter":"4 Raster Data Handling","heading":"4.1.1 raster package: RasterLayer, RasterStack, and RasterBrick","text":"Let’s start taking look raster data. use CDL data Iowa 2015.Evaluating imported raster object provides information raster data, dimensions (number cells, number columns, number cells), spatial resolution (30 meter 30 meter raster data), extent, CRS minimum maximum values recorded raster layer. class downloaded data RasterLayer, raster data class defined raster package. RasterLayer consists one layer, meaning single variable associated cells (land use category code integer).can stack multiple raster layers spatial resolution extent create RasterStack using raster::stack() RasterBrick using raster::brick(). Often times, processing multi-layer object computational advantages processing multiple single-layer one one67.create RasterStack RasterBrick, let’s load CDL data IA 2016 stack 2015 data.IA_cdl_stack class RasterStack, two layers variables: CDL 2015 2016. can make RasterBrick using raster::brick():probably noticed took time create RasterBrick object68. spatial operations RasterBrick supposedly faster RasterStack, time create RasterBrick object often long enough kill speed advantage entirely69. Often, three raster object types collectively referred Raster\\(^*\\) objects shorthand documentation raster related packages.","code":"class      : RasterLayer \ndimensions : 11671, 17795, 207685445  (nrow, ncol, ncell)\nresolution : 30, 30  (x, y)\nextent     : -52095, 481755, 1938165, 2288295  (xmin, xmax, ymin, ymax)\ncrs        : +proj=aea +lat_0=23 +lon_0=-96 +lat_1=29.5 +lat_2=45.5 +x_0=0 +y_0=0 +ellps=GRS80 +units=m +no_defs \nsource     : IA_cdl_2015.tif \nnames      : IA_cdl_2015 \nvalues     : 0, 229  (min, max)class      : RasterStack \ndimensions : 11671, 17795, 207685445, 2  (nrow, ncol, ncell, nlayers)\nresolution : 30, 30  (x, y)\nextent     : -52095, 481755, 1938165, 2288295  (xmin, xmax, ymin, ymax)\ncrs        : +proj=aea +lat_0=23 +lon_0=-96 +lat_1=29.5 +lat_2=45.5 +x_0=0 +y_0=0 +ellps=GRS80 +units=m +no_defs \nnames      : IA_cdl_2015, IA_cdl_2016 \nmin values :           0,           0 \nmax values :         229,         241 \n#--- stack the two ---#\nIA_cdl_brick <- brick(IA_cdl_stack)\n\n#--- or this works as well ---#\n# IA_cdl_brick <- brick(IA_cdl_2015, IA_cdl_2016)\n\n#--- take a look ---#\nIA_cdl_brickclass      : RasterBrick \ndimensions : 11671, 17795, 207685445, 2  (nrow, ncol, ncell, nlayers)\nresolution : 30, 30  (x, y)\nextent     : -52095, 481755, 1938165, 2288295  (xmin, xmax, ymin, ymax)\ncrs        : +proj=aea +lat_0=23 +lon_0=-96 +lat_1=29.5 +lat_2=45.5 +x_0=0 +y_0=0 +ellps=GRS80 +units=m +no_defs \nsource     : r_tmp_2022-04-12_215247_67656_89224.grd \nnames      : IA_cdl_2015, IA_cdl_2016 \nmin values :           0,           0 \nmax values :         229,         241 "},{"path":"raster-basics.html","id":"terra-package-spatraster","chapter":"4 Raster Data Handling","heading":"4.1.2 terra package: SpatRaster","text":"terra package one object class raster data, SpatRaster distinctions one-layer multi-layer rasters necessary. Let’s first convert RasterLayer SpatRaster using terra::rast() function.can see number layers (nlyr dimensions) \\(1\\) original object RasterLayer, definition one layer. Now, let’s convert RasterStack SpatRaster using terra::rast()., SpatRaster, now see number layers 2. just confirmed terra one class raster data whether single-layer multiple-layer ones.order make multi-layer SpatRaster multiple single-layer SpatRaster can just use c() like :","code":"\n#--- convert to a SpatRaster ---#\nIA_cdl_2015_sr <- rast(IA_cdl_2015)\n\n#--- take a look ---#\nIA_cdl_2015_srclass       : SpatRaster \ndimensions  : 11671, 17795, 1  (nrow, ncol, nlyr)\nresolution  : 30, 30  (x, y)\nextent      : -52095, 481755, 1938165, 2288295  (xmin, xmax, ymin, ymax)\ncoord. ref. : +proj=aea +lat_0=23 +lon_0=-96 +lat_1=29.5 +lat_2=45.5 +x_0=0 +y_0=0 +ellps=GRS80 +units=m +no_defs \nsource      : IA_cdl_2015.tif \nname        : IA_cdl_2015 \nmin value   :           0 \nmax value   :         229 \n#--- convert to a SpatRaster ---#\nIA_cdl_stack_sr <- rast(IA_cdl_stack)\n\n#--- take a look ---#\nIA_cdl_stack_srclass       : SpatRaster \ndimensions  : 11671, 17795, 2  (nrow, ncol, nlyr)\nresolution  : 30, 30  (x, y)\nextent      : -52095, 481755, 1938165, 2288295  (xmin, xmax, ymin, ymax)\ncoord. ref. : +proj=aea +lat_0=23 +lon_0=-96 +lat_1=29.5 +lat_2=45.5 +x_0=0 +y_0=0 +ellps=GRS80 +units=m +no_defs \nsources     : IA_cdl_2015.tif  \n              IA_cdl_2016.tif  \nnames       : IA_cdl_2015, IA_cdl_2016 \nmin values  :           0,           0 \nmax values  :         229,         241 \n#--- create a single-layer SpatRaster ---#\nIA_cdl_2016_sr <- rast(IA_cdl_2016)\n\n#--- concatenate ---#\n(\n  IA_cdl_ml_sr <- c(IA_cdl_2015_sr, IA_cdl_2016_sr)\n)\n(\n  IA_cdl_ml_sr <- c(IA_cdl_2015_sr, IA_cdl_2016_sr)\n)class       : SpatRaster \ndimensions  : 11671, 17795, 2  (nrow, ncol, nlyr)\nresolution  : 30, 30  (x, y)\nextent      : -52095, 481755, 1938165, 2288295  (xmin, xmax, ymin, ymax)\ncoord. ref. : +proj=aea +lat_0=23 +lon_0=-96 +lat_1=29.5 +lat_2=45.5 +x_0=0 +y_0=0 +ellps=GRS80 +towgs84=0,0,0,0,0,0,0 +units=m +no_defs \nsources     : IA_cdl_2015.tif  \n              IA_cdl_2016.tif  \nnames       : Layer_1, Layer_1 \nmin values  :       0,       0 \nmax values  :     229,     241 "},{"path":"raster-basics.html","id":"converting-a-spatraster-object-to-a-raster-object.","chapter":"4 Raster Data Handling","heading":"4.1.3 Converting a SpatRaster object to a Raster\\(^*\\) object.","text":"can convert SpatRaster object Raster\\(^*\\) object using raster(), stack(), brick(). Keep mind use rater() even though SpatRaster multiple layers, resulting RasterLayer object first multiple layers., can just use (SpatRast, \"Raster\") like :works Raster\\(^*\\) object pick right function like .","code":"\n#--- RasterLayer (only 1st layer) ---#\nIA_cdl_stack_sr %>% raster()class      : RasterLayer \ndimensions : 11671, 17795, 207685445  (nrow, ncol, ncell)\nresolution : 30, 30  (x, y)\nextent     : -52095, 481755, 1938165, 2288295  (xmin, xmax, ymin, ymax)\ncrs        : +proj=aea +lat_0=23 +lon_0=-96 +lat_1=29.5 +lat_2=45.5 +x_0=0 +y_0=0 +ellps=GRS80 +units=m +no_defs \nsource     : IA_cdl_2015.tif \nnames      : IA_cdl_2015 \nvalues     : 0, 229  (min, max)\n#--- RasterLayer ---#\nIA_cdl_stack_sr %>% stack()class      : RasterStack \ndimensions : 11671, 17795, 207685445, 2  (nrow, ncol, ncell, nlayers)\nresolution : 30, 30  (x, y)\nextent     : -52095, 481755, 1938165, 2288295  (xmin, xmax, ymin, ymax)\ncrs        : +proj=aea +lat_0=23 +lon_0=-96 +lat_1=29.5 +lat_2=45.5 +x_0=0 +y_0=0 +ellps=GRS80 +units=m +no_defs \nnames      : IA_cdl_2015, IA_cdl_2016 \nmin values :           0,           0 \nmax values :         229,         241 \n#--- RasterLayer (this takes some time) ---#\nIA_cdl_stack_sr %>% brick()class      : RasterStack \ndimensions : 11671, 17795, 207685445, 2  (nrow, ncol, ncell, nlayers)\nresolution : 30, 30  (x, y)\nextent     : -52095, 481755, 1938165, 2288295  (xmin, xmax, ymin, ymax)\ncrs        : +proj=aea +lat_0=23 +lon_0=-96 +lat_1=29.5 +lat_2=45.5 +x_0=0 +y_0=0 +ellps=GRS80 +units=m +no_defs \nnames      : IA_cdl_2015, IA_cdl_2016 \nmin values :           0,           0 \nmax values :         229,         241 \nas(IA_cdl_stack_sr, \"Raster\")class      : RasterStack \ndimensions : 11671, 17795, 207685445, 2  (nrow, ncol, ncell, nlayers)\nresolution : 30, 30  (x, y)\nextent     : -52095, 481755, 1938165, 2288295  (xmin, xmax, ymin, ymax)\ncrs        : +proj=aea +lat_0=23 +lon_0=-96 +lat_1=29.5 +lat_2=45.5 +x_0=0 +y_0=0 +ellps=GRS80 +units=m +no_defs \nnames      : IA_cdl_2015, IA_cdl_2016 \nmin values :           0,           0 \nmax values :         229,         241 "},{"path":"raster-basics.html","id":"vector-data-in-the-terra-package","chapter":"4 Raster Data Handling","heading":"4.1.4 Vector data in the terra package","text":"terra package class vector data, called SpatVector. use vector data functionality provided terra package, learn convert sf object SpatVector terra functions support sf now (likely resolved soon). see use cases conversion next chapter learn raster value extractions vector data using terra::extract().example, let’s use Illinois county border data.can convert sf object SpatVector object using terra::vect().","code":"Simple feature collection with 102 features and 17 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -91.51308 ymin: 36.9703 xmax: -87.01993 ymax: 42.50848\nGeodetic CRS:  NAD83\nFirst 10 features:\n    STATEFP COUNTYFP COUNTYNS GEOID    NAME       NAMELSAD LSAD CLASSFP MTFCC\n86       17      067 00424235 17067 Hancock Hancock County   06      H1 G4020\n93       17      025 00424214 17025    Clay    Clay County   06      H1 G4020\n132      17      185 00424293 17185  Wabash  Wabash County   06      H1 G4020\n149      17      113 01784833 17113  McLean  McLean County   06      H1 G4020\n159      17      005 00424204 17005    Bond    Bond County   06      H1 G4020\n160      17      009 00424206 17009   Brown   Brown County   06      H1 G4020\n214      17      083 00424243 17083  Jersey  Jersey County   06      H1 G4020\n255      17      147 00424275 17147   Piatt   Piatt County   06      H1 G4020\n267      17      151 00424277 17151    Pope    Pope County   06      H1 G4020\n304      17      011 00424207 17011  Bureau  Bureau County   06      H1 G4020\n    CSAFP CBSAFP METDIVFP FUNCSTAT      ALAND   AWATER    INTPTLAT     INTPTLON\n86    161  22800     <NA>        A 2055798688 53563362 +40.4013180 -091.1688008\n93   <NA>   <NA>     <NA>        A 1213022841  3064720 +38.7468187 -088.4823254\n132  <NA>   <NA>     <NA>        A  578403995 10973569 +38.4458209 -087.8391674\n149   145  14010     <NA>        A 3064597287  7804856 +40.4945594 -088.8445391\n159   476  41180     <NA>        A  985065096  6462629 +38.8859240 -089.4365916\n160  <NA>   <NA>     <NA>        A  791828626  4144346 +39.9620694 -090.7503095\n214   476  41180     <NA>        A  957415216 20333974 +39.0801945 -090.3613850\n255  <NA>  16580     <NA>        A 1137492089   754122 +40.0090327 -088.5923546\n267  <NA>   <NA>     <NA>        A  955362037 14662240 +37.4171687 -088.5423737\n304   176  36837     <NA>        A 2250884551 11523914 +41.4013043 -089.5283772\n                          geometry\n86  MULTIPOLYGON (((-91.37421 4...\n93  MULTIPOLYGON (((-88.69517 3...\n132 MULTIPOLYGON (((-87.9446 38...\n149 MULTIPOLYGON (((-89.2665 40...\n159 MULTIPOLYGON (((-89.36179 3...\n160 MULTIPOLYGON (((-90.91469 4...\n214 MULTIPOLYGON (((-90.59216 3...\n255 MULTIPOLYGON (((-88.74516 4...\n267 MULTIPOLYGON (((-88.70963 3...\n304 MULTIPOLYGON (((-89.85691 4...\n(\n  IL_county_sv <- vect(IL_county)\n) class       : SpatVector \n geometry    : polygons \n dimensions  : 102, 17  (geometries, attributes)\n extent      : -91.51308, -87.01993, 36.9703, 42.50848  (xmin, xmax, ymin, ymax)\n coord. ref. : lon/lat NAD83 (EPSG:4269) \n names       : STATEFP COUNTYFP COUNTYNS GEOID    NAME       NAMELSAD  LSAD\n type        :   <chr>    <chr>    <chr> <chr>   <chr>          <chr> <chr>\n values      :      17      067 00424235 17067 Hancock Hancock County    06\n                    17      025 00424214 17025    Clay    Clay County    06\n                    17      185 00424293 17185  Wabash  Wabash County    06\n CLASSFP MTFCC CSAFP (and 7 more)\n   <chr> <chr> <chr>             \n      H1 G4020   161             \n      H1 G4020    NA             \n      H1 G4020    NA             "},{"path":"raster-basics.html","id":"read-and-write-a-raster-data-file","chapter":"4 Raster Data Handling","heading":"4.2 Read and write a raster data file","text":"Sometimes can download raster data interest saw Section 3.1. , time need read raster data stored file. Raster data files can come numerous different formats. example, PRPISM comes Band Interleaved Line (BIL) format, Daymet data comes netCDF format. popular formats include GeoTiff, SAGA, ENVI, many others.","code":""},{"path":"raster-basics.html","id":"read-raster-files","chapter":"4 Raster Data Handling","heading":"4.2.1 Read raster file(s)","text":"use terra::rast() read raster data many common formats, almost always case raster data got can read using function. , read GeoTiff file (file .tif extension):One important thing note cell values raster data actually memory “read” raster data file.\nbasically just established connection file. helps reduce memory footprint raster data handling. can check raster::inMemory() function Raster\\(^*\\) objects, function implemented terra yet.can read multiple single-layer raster datasets spatial extent resolution time multi-layer SpatRaster object. , import two single-layer raster datasets (IA_cdl_2015.tif IA_cdl_2016.tif) create two-layer SpatRaster object.course, works two datasets identical spatial extent resolution. , however, restrictions variable raster layers represent. example, can combine PRISM temperature precipitation raster layers.","code":"\n(\n  IA_cdl_2015_sr <- rast(\"Data/IA_cdl_2015.tif\")\n)class       : SpatRaster \ndimensions  : 11671, 17795, 1  (nrow, ncol, nlyr)\nresolution  : 30, 30  (x, y)\nextent      : -52095, 481755, 1938165, 2288295  (xmin, xmax, ymin, ymax)\ncoord. ref. : +proj=aea +lat_0=23 +lon_0=-96 +lat_1=29.5 +lat_2=45.5 +x_0=0 +y_0=0 +ellps=GRS80 +towgs84=0,0,0,0,0,0,0 +units=m +no_defs \nsource      : IA_cdl_2015.tif \nname        : Layer_1 \nmin value   :       0 \nmax value   :     229 \n#--- the list of path to the files ---#\nfiles_list <- c(\"Data/IA_cdl_2015.tif\", \"Data/IA_cdl_2016.tif\")\n\n#--- read the two at the same time ---#\n(\n  multi_layer_sr <- rast(files_list)\n)class       : SpatRaster \ndimensions  : 11671, 17795, 2  (nrow, ncol, nlyr)\nresolution  : 30, 30  (x, y)\nextent      : -52095, 481755, 1938165, 2288295  (xmin, xmax, ymin, ymax)\ncoord. ref. : +proj=aea +lat_0=23 +lon_0=-96 +lat_1=29.5 +lat_2=45.5 +x_0=0 +y_0=0 +ellps=GRS80 +towgs84=0,0,0,0,0,0,0 +units=m +no_defs \nsources     : IA_cdl_2015.tif  \n              IA_cdl_2016.tif  \nnames       : Layer_1, Layer_1 \nmin values  :       0,       0 \nmax values  :     229,     241 "},{"path":"raster-basics.html","id":"write-raster-files","chapter":"4 Raster Data Handling","heading":"4.2.2 Write raster files","text":"can write SpatRaster object using terra::writeRaster().code saves IA_cdl_2015_sr (SpatRaster object) GeoTiff file.70 filetype option can dropped writeRaster() infers filetype extension file name. overwrite = TRUE option necessary file name already exists overwriting . one many areas terra better raster. raster::writeRaster() can frustratingly slow large Raster\\(^*\\) object. terra::writeRaster() much faster.can also save multi-layer SpatRaster object just like save single-layer SpatRaster object.saved file multi-band raster datasets. , many raster files spatial extent resolution, can “stack” R export single multi-band raster datasets, cleans data folder.","code":"\nterra::writeRaster(IA_cdl_2015_sr, \"Data/IA_cdl_stack.tif\", filetype = \"GTiff\", overwrite = TRUE)\nterra::writeRaster(IA_cdl_stack_sr, \"Data/IA_cdl_stack.tif\", filetype = \"GTiff\", overwrite = TRUE)"},{"path":"raster-basics.html","id":"extract-information-from-raster-data-object","chapter":"4 Raster Data Handling","heading":"4.3 Extract information from raster data object","text":"","code":""},{"path":"raster-basics.html","id":"get-crs","chapter":"4 Raster Data Handling","heading":"4.3.1 Get CRS","text":"often need extract CRS raster object interact vector data (e.g., extracting values raster layer vector data, cropping raster layer spatial extent vector data), can done using terra::crs():","code":"\nterra::crs(IA_cdl_2015_sr)[1] \"BOUNDCRS[\\n    SOURCECRS[\\n        PROJCRS[\\\"unnamed\\\",\\n            BASEGEOGCRS[\\\"GRS 1980(IUGG, 1980)\\\",\\n                DATUM[\\\"unknown\\\",\\n                    ELLIPSOID[\\\"GRS80\\\",6378137,298.257222101,\\n                        LENGTHUNIT[\\\"metre\\\",1,\\n                            ID[\\\"EPSG\\\",9001]]]],\\n                PRIMEM[\\\"Greenwich\\\",0,\\n                    ANGLEUNIT[\\\"degree\\\",0.0174532925199433,\\n                        ID[\\\"EPSG\\\",9122]]]],\\n            CONVERSION[\\\"Albers Equal Area\\\",\\n                METHOD[\\\"Albers Equal Area\\\",\\n                    ID[\\\"EPSG\\\",9822]],\\n                PARAMETER[\\\"Latitude of false origin\\\",23,\\n                    ANGLEUNIT[\\\"degree\\\",0.0174532925199433],\\n                    ID[\\\"EPSG\\\",8821]],\\n                PARAMETER[\\\"Longitude of false origin\\\",-96,\\n                    ANGLEUNIT[\\\"degree\\\",0.0174532925199433],\\n                    ID[\\\"EPSG\\\",8822]],\\n                PARAMETER[\\\"Latitude of 1st standard parallel\\\",29.5,\\n                    ANGLEUNIT[\\\"degree\\\",0.0174532925199433],\\n                    ID[\\\"EPSG\\\",8823]],\\n                PARAMETER[\\\"Latitude of 2nd standard parallel\\\",45.5,\\n                    ANGLEUNIT[\\\"degree\\\",0.0174532925199433],\\n                    ID[\\\"EPSG\\\",8824]],\\n                PARAMETER[\\\"Easting at false origin\\\",0,\\n                    LENGTHUNIT[\\\"metre\\\",1],\\n                    ID[\\\"EPSG\\\",8826]],\\n                PARAMETER[\\\"Northing at false origin\\\",0,\\n                    LENGTHUNIT[\\\"metre\\\",1],\\n                    ID[\\\"EPSG\\\",8827]]],\\n            CS[Cartesian,2],\\n                AXIS[\\\"easting\\\",east,\\n                    ORDER[1],\\n                    LENGTHUNIT[\\\"metre\\\",1,\\n                        ID[\\\"EPSG\\\",9001]]],\\n                AXIS[\\\"northing\\\",north,\\n                    ORDER[2],\\n                    LENGTHUNIT[\\\"metre\\\",1,\\n                        ID[\\\"EPSG\\\",9001]]]]],\\n    TARGETCRS[\\n        GEOGCRS[\\\"WGS 84\\\",\\n            DATUM[\\\"World Geodetic System 1984\\\",\\n                ELLIPSOID[\\\"WGS 84\\\",6378137,298.257223563,\\n                    LENGTHUNIT[\\\"metre\\\",1]]],\\n            PRIMEM[\\\"Greenwich\\\",0,\\n                ANGLEUNIT[\\\"degree\\\",0.0174532925199433]],\\n            CS[ellipsoidal,2],\\n                AXIS[\\\"geodetic latitude (Lat)\\\",north,\\n                    ORDER[1],\\n                    ANGLEUNIT[\\\"degree\\\",0.0174532925199433]],\\n                AXIS[\\\"geodetic longitude (Lon)\\\",east,\\n                    ORDER[2],\\n                    ANGLEUNIT[\\\"degree\\\",0.0174532925199433]],\\n            USAGE[\\n                SCOPE[\\\"Horizontal component of 3D system.\\\"],\\n                AREA[\\\"World.\\\"],\\n                BBOX[-90,-180,90,180]],\\n            ID[\\\"EPSG\\\",4326]]],\\n    ABRIDGEDTRANSFORMATION[\\\"Transformation to WGS84\\\",\\n        METHOD[\\\"Position Vector transformation (geog2D domain)\\\",\\n            ID[\\\"EPSG\\\",9606]],\\n        PARAMETER[\\\"X-axis translation\\\",0,\\n            ID[\\\"EPSG\\\",8605]],\\n        PARAMETER[\\\"Y-axis translation\\\",0,\\n            ID[\\\"EPSG\\\",8606]],\\n        PARAMETER[\\\"Z-axis translation\\\",0,\\n            ID[\\\"EPSG\\\",8607]],\\n        PARAMETER[\\\"X-axis rotation\\\",0,\\n            ID[\\\"EPSG\\\",8608]],\\n        PARAMETER[\\\"Y-axis rotation\\\",0,\\n            ID[\\\"EPSG\\\",8609]],\\n        PARAMETER[\\\"Z-axis rotation\\\",0,\\n            ID[\\\"EPSG\\\",8610]],\\n        PARAMETER[\\\"Scale difference\\\",1,\\n            ID[\\\"EPSG\\\",8611]]]]\""},{"path":"raster-basics.html","id":"subset","chapter":"4 Raster Data Handling","heading":"4.3.2 Subset","text":"can access specific layers multi-layer raster object indexing:","code":"\n#--- index ---#\nIA_cdl_stack_sr[[2]] # (originally IA_cdl_2016.tif)class       : SpatRaster \ndimensions  : 11671, 17795, 1  (nrow, ncol, nlyr)\nresolution  : 30, 30  (x, y)\nextent      : -52095, 481755, 1938165, 2288295  (xmin, xmax, ymin, ymax)\ncoord. ref. : +proj=aea +lat_0=23 +lon_0=-96 +lat_1=29.5 +lat_2=45.5 +x_0=0 +y_0=0 +ellps=GRS80 +units=m +no_defs \nsource      : IA_cdl_2016.tif \nname        : IA_cdl_2016 \nmin value   :           0 \nmax value   :         241 "},{"path":"raster-basics.html","id":"get-cell-values","chapter":"4 Raster Data Handling","heading":"4.3.3 Get cell values","text":"can access values stored SpatRaster object using terra::values() function:returned values come matrix form two columns getting values two-layer SpatRaster object (one column layer). general, terra::values() returns \\(X\\) \\(n\\) matrix, \\(X\\) number cells \\(n\\) number layers.","code":"\n#--- terra::values ---#\nvalues_from_rs <- terra::values(IA_cdl_stack_sr)\n\n#--- take a look ---#\nhead(values_from_rs)     Layer_1 Layer_1\n[1,]       0       0\n[2,]       0       0\n[3,]       0       0\n[4,]       0       0\n[5,]       0       0\n[6,]       0       0"},{"path":"raster-basics.html","id":"turning-a-raster-object-into-a-data.frame","chapter":"4 Raster Data Handling","heading":"4.4 Turning a raster object into a data.frame","text":"creating maps using ggplot2() SpatRaster Raster\\(^*\\) object, necessary first convert data.frame (see Chapter 8.2). can use .data.frame() function xy = TRUE option construct data.frame row represents single cell cell values layer coordinates (center cell). Note: seen cases raster object converted data.frame ‘sf’ purpose interacting polygons using st_join() extract assign cell values polygons (see Chapter 3 type operation). , however, generally recommended two reasons. First , much slower using functions work raster object polygons directly extract values polygons, terra::extract() exactextractr::exact_extract(), introduced Chapter 5. primarily converting raster object data.frame() slow. Second, raster data object converted point sf data, one can longer weight cell values according degree overlaps target polygons. case found conversion beneficial (simply necessary) create map using ggplot() raster object (tmap package can work directly raster objects.). desire work data.frame understandable comfortable working data.frame, likely conversion data.frame unnecessary, also inefficient71.","code":"\n#--- converting to a data.frame ---#\nIA_cdl_df <- as.data.frame(IA_cdl_stack_sr, xy = TRUE) # this works with Raster* objects as well\n\n#--- take a look ---#\nhead(IA_cdl_df)       x       y Layer_1 Layer_1.1\n1 -52080 2288280       0         0\n2 -52050 2288280       0         0\n3 -52020 2288280       0         0\n4 -51990 2288280       0         0\n5 -51960 2288280       0         0\n6 -51930 2288280       0         0"},{"path":"raster-basics.html","id":"quick-visualization","chapter":"4 Raster Data Handling","heading":"4.5 Quick visualization","text":"quick visualization data values SpatRaster objects, can simply use plot():elaborate map using raster data, see Chapter 8.2.","code":"\nplot(IA_cdl_2015_sr)"},{"path":"raster-basics.html","id":"work-with-netcdf","chapter":"4 Raster Data Handling","heading":"4.6 Working with netCDFs","text":"worth talking read netCDFs format – multidimensional file format resembles raster stack/brick. netCDF file contains data specific structure: two-dimensional spatial grid (e.g., longitude latitude) third dimension usually date time. structure convenient weather data measured consistent grid time. One dataset called gridMET maintains gridded dataset weather variables 4km resolution. Let’s download daily precipitation data 2018 using downloader::download()72. set destination file name (call file want ), mode wb binary download.code stored data pr_2018.nc Data folder. can read netCDF file using terra::rast().can see 365 layers: one layer per day 2018. Let’s now look layer names:Since 365 layers number end layer names increase 1, think nth layer represents nth day 2018. case, correct. However, always good practice confirm layer represents without assuming anything. Now, let’s use ncdf4 package, built specifically handle netCDF4 objects.can see output, tons information see read data using rast(), includes explanation third dimension (day) raster object. turned numerical values end layer names SpatRaster object days since 1900-01-01. , first layer (named precipitation_amount_day=43099) represents:Actually, use raster::brick(), instead terra::rast(), can see naming convention layers:SpatRaster RasterBrick objects easier work many useful functions accept inputs, ncdf4 object. Personally, first scrutinize netCDFs file using nc_open() import SpatRaster RasterBrick object73. Recovering dates layers particularly important often wrangle resulting data based date (e.g., subset data April September). example date recovery can seen Chapter 9.5.detailed description work ncdf4 object provided . However, say economists unlikely benefit much . stated several times already, (economists) rarely need manipulate raster data . observation units almost always points (e.g., farms, houses) polygons (e.g., county boundary) just need associate units values held raster data (e.g., precipitation). can simply use raster objects supply functions take care business raster value extraction, covered next chapter (Chapter 5).","code":"\n#--- download gridMET precipitation 2018 ---#\ndownloader::download(\n  url = str_c(\"http://www.northwestknowledge.net/metdata/data/pr_2018.nc\"),\n  destfile = \"Data/pr_2018.nc\",\n  mode = \"wb\"\n)\n(\n  pr_2018_gm <- rast(\"Data/pr_2018.nc\")\n)class       : SpatRaster \ndimensions  : 585, 1386, 365  (nrow, ncol, nlyr)\nresolution  : 0.04166667, 0.04166667  (x, y)\nextent      : -124.7875, -67.0375, 25.04583, 49.42083  (xmin, xmax, ymin, ymax)\ncoord. ref. : lon/lat WGS 84 (EPSG:4326) \nsource      : pr_2018.nc \nvarname     : precipitation_amount (pr) \nnames       : preci~43099, preci~43100, preci~43101, preci~43102, preci~43103, preci~43104, ... \nunit        :          mm,          mm,          mm,          mm,          mm,          mm, ... \nhead(names(pr_2018_gm))[1] \"precipitation_amount_day=43099\" \"precipitation_amount_day=43100\"\n[3] \"precipitation_amount_day=43101\" \"precipitation_amount_day=43102\"\n[5] \"precipitation_amount_day=43103\" \"precipitation_amount_day=43104\"\n(\n  pr_2018_nc <- ncdf4::nc_open(\"Data/pr_2018.nc\")\n)File Data/pr_2018.nc (NC_FORMAT_NETCDF4):\n\n     1 variables (excluding dimension variables):\n        unsigned short precipitation_amount[lon,lat,day]   (Chunking: [231,98,61])  (Compression: level 9)\n            _FillValue: 32767\n            units: mm\n            description: Daily Accumulated Precipitation\n            long_name: pr\n            standard_name: pr\n            missing_value: 32767\n            dimensions: lon lat time\n            grid_mapping: crs\n            coordinate_system: WGS84,EPSG:4326\n            scale_factor: 0.1\n            add_offset: 0\n            coordinates: lon lat\n            _Unsigned: true\n\n     4 dimensions:\n        lon  Size:1386 \n            units: degrees_east\n            description: longitude\n            long_name: longitude\n            standard_name: longitude\n            axis: X\n        lat  Size:585 \n            units: degrees_north\n            description: latitude\n            long_name: latitude\n            standard_name: latitude\n            axis: Y\n        day  Size:365 \n            description: days since 1900-01-01\n            units: days since 1900-01-01 00:00:00\n            long_name: time\n            standard_name: time\n            calendar: gregorian\n        crs  Size:1 \n            grid_mapping_name: latitude_longitude\n            longitude_of_prime_meridian: 0\n            semi_major_axis: 6378137\n            long_name: WGS 84\n            inverse_flattening: 298.257223563\n            GeoTransform: -124.7666666333333 0.041666666666666 0  49.400000000000000 -0.041666666666666\n            spatial_ref: GEOGCS[\"WGS 84\",DATUM[\"WGS_1984\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]],AUTHORITY[\"EPSG\",\"6326\"]],PRIMEM[\"Greenwich\",0,AUTHORITY[\"EPSG\",\"8901\"]],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]],AUTHORITY[\"EPSG\",\"4326\"]]\n\n    19 global attributes:\n        geospatial_bounds_crs: EPSG:4326\n        Conventions: CF-1.6\n        geospatial_bounds: POLYGON((-124.7666666333333 49.400000000000000, -124.7666666333333 25.066666666666666, -67.058333300000015 25.066666666666666, -67.058333300000015 49.400000000000000, -124.7666666333333 49.400000000000000))\n        geospatial_lat_min: 25.066666666666666\n        geospatial_lat_max: 49.40000000000000\n        geospatial_lon_min: -124.7666666333333\n        geospatial_lon_max: -67.058333300000015\n        geospatial_lon_resolution: 0.041666666666666\n        geospatial_lat_resolution: 0.041666666666666\n        geospatial_lat_units: decimal_degrees north\n        geospatial_lon_units: decimal_degrees east\n        coordinate_system: EPSG:4326\n        author: John Abatzoglou - University of Idaho, jabatzoglou@uidaho.edu\n        date: 03 May 2021\n        note1: The projection information for this file is: GCS WGS 1984.\n        note2: Citation: Abatzoglou, J.T., 2013, Development of gridded surface meteorological data for ecological applications and modeling, International Journal of Climatology, DOI: 10.1002/joc.3413\n        note3: Data in slices after last_permanent_slice (1-based) are considered provisional and subject to change with subsequent updates\n        note4: Data in slices after last_provisional_slice (1-based) are considered early and subject to change with subsequent updates\n        note5: Days correspond approximately to calendar days ending at midnight, Mountain Standard Time (7 UTC the next calendar day)\nymd(\"1900-01-01\") + 43099[1] \"2018-01-01\"\n(\n  pr_2018_b <- brick(\"Data/pr_2018.nc\")\n)class      : RasterBrick \ndimensions : 585, 1386, 810810, 365  (nrow, ncol, ncell, nlayers)\nresolution : 0.04166667, 0.04166667  (x, y)\nextent     : -124.7875, -67.0375, 25.04583, 49.42083  (xmin, xmax, ymin, ymax)\ncrs        : +proj=longlat +datum=WGS84 +no_defs \nsource     : pr_2018.nc \nnames      : X43099, X43100, X43101, X43102, X43103, X43104, X43105, X43106, X43107, X43108, X43109, X43110, X43111, X43112, X43113, ... \nday (days since 1900-01-01 00:00:00): 43099, 43463 (min, max)\nvarname    : precipitation_amount "},{"path":"int-RV.html","id":"int-RV","chapter":"5 Spatial Interactions of Vector and Raster Data","heading":"5 Spatial Interactions of Vector and Raster Data","text":"","code":""},{"path":"int-RV.html","id":"before-you-start-4","chapter":"5 Spatial Interactions of Vector and Raster Data","heading":"Before you start","text":"chapter learn spatial interactions vector raster dataset. first look crop (spatially subset) raster dataset based geographic extent vector dataset. cover extract values raster data points polygons. precise, mean raster data extraction points polygons data:Points: points, find raster cell located within, assign value cell point.Points: points, find raster cell located within, assign value cell point.Polygons: polygons, identify raster cells intersect polygon, assign vector cell values polygonPolygons: polygons, identify raster cells intersect polygon, assign vector cell values polygonThis probably important operation economists run raster datasets.show can use terra::extract() cases. , also see polygons, exact_extract() exactextractr package often considerably faster terra::extract().Finally, see conversions Raster\\(^*\\) (raster package) objects SpatRaster object (terra package) incompatibility object classes across key packages. believe hassles go away soon start supporting .","code":""},{"path":"int-RV.html","id":"direction-for-replication-4","chapter":"5 Spatial Interactions of Vector and Raster Data","heading":"Direction for replication","text":"DatasetsAll datasets need import available . chapter, path files set relative working directory (hidden). run codes without mess paths files, follow steps:set folder (folder) working directory using setwd()create folder called “Data” inside folder designated working directory (created “Data” folder previously, skip step)download pertinent datasets hereplace files downloaded folder “Data” folderPackagesRun following code install load (already installed) pacman package, install load (already installed) listed package inside pacman::p_load() function.","code":"\nif (!require(\"pacman\")) install.packages(\"pacman\")\npacman::p_load(\n  terra, # handle raster data\n  raster, # handle raster data\n  exactextractr, # fast extractions\n  sf, # vector data operations\n  dplyr, # data wrangling\n  tidyr, # data wrangling\n  data.table, # data wrangling\n  prism, # download PRISM data\n  tictoc, # timing codes\n  tigris, # to get county sf\n  tmap # for mapping\n)"},{"path":"int-RV.html","id":"raster-crop","chapter":"5 Spatial Interactions of Vector and Raster Data","heading":"5.1 Cropping to the Area of Interest","text":"use PRISM maximum temperature (tmax) data raster dataset Kansas county boundaries vector dataset.Let’s download tmax data July 1, 2018 (Figure 5.1).\nFigure 5.1: Map PRISM tmax data July 1, 2018\nnow get Kansas county border data tigris package (Figure 5.2) sf.\nFigure 5.2: Kansas county boundaries\nSometimes, convenient crop raster layer specific area interest carry around unnecessary parts raster layer. Moreover, takes less time extract values raster layer size raster layer smaller. can crop raster layer using terra::crop(). works like :, case, job.figure (Figure 5.3) shows PRISM tmax raster data cropped geographic extent Kansas. Notice cropped raster layer extends beyond outer boundary Kansas state boundary (bit hard see, look upper right corner).\nFigure 5.3: PRISM tmax raster data cropped geographic extent Kansas\n","code":"\n#--- set the path to the folder to which you save the downloaded PRISM data ---#\n# This code sets the current working directory as the designated folder\noptions(prism.path = \"Data\")\n\n#--- download PRISM precipitation data ---#\nget_prism_dailys(\n  type = \"tmax\",\n  date = \"2018-07-01\",\n  keepZip = FALSE\n)\n\n#--- the file name of the PRISM data just downloaded ---#\nprism_file <- \"Data/PRISM_tmax_stable_4kmD2_20180701_bil/PRISM_tmax_stable_4kmD2_20180701_bil.bil\"\n\n#--- read in the prism data ---#\nprism_tmax_0701_sr <- rast(prism_file)\n#--- Kansas boundary (sf) ---#\nKS_county_sf <-\n  #--- get Kansas county boundary ---\n  tigris::counties(state = \"Kansas\", cb = TRUE) %>%\n  #--- sp to sf ---#\n  st_as_sf() %>%\n  #--- transform using the CRS of the PRISM tmax data  ---#\n  st_transform(terra::crs(prism_tmax_0701_sr))\n#--- syntax (this does not run) ---#\nterra::crop(SpatRaster, sf)\n#--- crop the entire PRISM to its KS portion---#\nprism_tmax_0701_KS_sr <- terra::crop(prism_tmax_0701_sr, KS_county_sf)"},{"path":"int-RV.html","id":"extracting-values-from-raster-layers-for-vector-data","chapter":"5 Spatial Interactions of Vector and Raster Data","heading":"5.2 Extracting Values from Raster Layers for Vector Data","text":"section, learn extract information raster layers spatial units represented vector data (points polygons). demonstrations section, use following datasets:Raster: PRISM tmax data cropped Kansas state border 07/01/2018 (obtained 5.1) 07/02/2018 (downloaded )Polygons: Kansas county boundaries (obtained 5.1)Points: Irrigation wells Kansas (imported )","code":""},{"path":"int-RV.html","id":"simple-visual-illustrations-of-raster-data-extraction","chapter":"5 Spatial Interactions of Vector and Raster Data","heading":"5.2.1 Simple visual illustrations of raster data extraction","text":"Extracting PointsFigure 5.4 shows visually mean “extract raster values points.”\nFigure 5.4: Visual illustration raster data extraction points data\nfigure, grids (cells) grid holds value (presented center). three points. find grid points fall inside get associated values assign points. example, Points 1, 2, 3 50, 4, 54, respectively,Extracting PolygonsFigure 5.5 shows visually mean “extract raster values polygons.”\nFigure 5.5: Visual illustration raster data extraction polygons data\npolygon overlaid top cells along centroids represented black dots. Extracting raster values polygon means finding raster cells intersect polygons get value cells assigns polygon. can see cells completely inside polygon, others partially overlapping polygon. Depending function use options, regard different cells spatially related polygons. example, default, terra::extract() extract cells whose centroid inside polygon. , can add option include cells partially intersected polygon. case, can also get fraction cell overlapped polygon, enables us find area-weighted values later. discuss details .PRISM tmax data 07/02/2018Irrigation wells Kansas:wells spatially distributed PRISM grids Kansas county borders (Figure 5.6):\nFigure 5.6: Map Kansas county borders, irrigation wells, PRISM tmax\n","code":"\n#--- download PRISM precipitation data ---#\nget_prism_dailys(\n  type = \"tmax\", \n  date = \"2018-07-02\", \n  keepZip = FALSE \n)\n\n#--- the file name of the PRISM data just downloaded ---#\nprism_file <- \"Data/PRISM_tmax_stable_4kmD2_20180702_bil/PRISM_tmax_stable_4kmD2_20180702_bil.bil\"\n\n#--- read in the prism data and crop it to Kansas state border ---#\nprism_tmax_0702_KS_sr <- rast(prism_file) %>% \n  terra::crop(KS_county_sf)\n#--- read in the KS points data ---#\n(\nKS_wells <- readRDS(\"Data/Chap_5_wells_KS.rds\") \n)Simple feature collection with 37647 features and 1 field\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: -102.0495 ymin: 36.99552 xmax: -94.62089 ymax: 40.00199\nGeodetic CRS:  NAD83\nFirst 10 features:\n   well_id                   geometry\n1        1 POINT (-100.4423 37.52046)\n2        3 POINT (-100.7118 39.91526)\n3        5 POINT (-99.15168 38.48849)\n4        7 POINT (-101.8995 38.78077)\n5        8  POINT (-100.7122 38.0731)\n6        9 POINT (-97.70265 39.04055)\n7       11 POINT (-101.7114 39.55035)\n8       12 POINT (-95.97031 39.16121)\n9       15 POINT (-98.30759 38.26787)\n10      17 POINT (-100.2785 37.71539)"},{"path":"int-RV.html","id":"extracting-to-points","chapter":"5 Spatial Interactions of Vector and Raster Data","heading":"5.2.2 Extracting to Points","text":"can extract values raster layers points using terra::extract(). terra::extract() finds raster cell points located within assigns value cell point.Let’s extract tmax values PRISM tmax layer (prism_tmax_0701_KS_sr) irrigation wells:Oh, okay. , problem points data (KS_wells) sf object, terra::extract() like (thought fixed now …). Let’s turn KS_wells SpatVect object apply terra::extract():resulting object data.frame, ID variable represents order observations points data second column represents values extracted points raster cells. , can assign extracted values new variable points data follows:Extracting values multi-layer SpatRaster works way. , combine prism_tmax_0701_KS_sr prism_tmax_0702_KS_sr create multi-layer SpatRaster extract values .now two columns hold values extracted raster cells: 2nd column 1st raster layer 3rd column 2nd raster layer prism_tmax_stack.Side note:Interestingly, terra::extract() can work sf long raster object type Raster\\(^*\\). code , prism_tmax_stack converted RasterStack terra::extract() job works just fine.can see, resulting object matrix instead data.frame. Now, let’s see can use SpatVect instead sf., work. turns terra::extract() works combinations SpatRaster-SpatVect Raster\\(^*\\)-sf/sp.","code":"\n#--- syntax (this does not run) ---#\nterra::extract(raster, points) \ntmax_from_prism <- terra::extract(prism_tmax_0701_KS_sr, KS_wells)Error in (function (classes, fdef, mtable) : unable to find an inherited method for function 'extract' for signature '\"SpatRaster\", \"sf\"'\ntmax_from_prism <- terra::extract(prism_tmax_0701_KS_sr, vect(KS_wells))\n\n#--- take a look ---#\nhead(tmax_from_prism)  ID PRISM_tmax_stable_4kmD2_20180701_bil\n1  1                               34.241\n2  2                               29.288\n3  3                               32.585\n4  4                               30.104\n5  5                               34.232\n6  6                               35.168\nKS_wells$tmax_07_01 <- tmax_from_prism[, -1]\n#--- create a multi-layer SpatRaster ---#\nprism_tmax_stack <- c(prism_tmax_0701_KS_sr, prism_tmax_0702_KS_sr)\n\n#--- extract tmax values ---#\ntmax_from_prism_stack <- terra::extract(prism_tmax_stack, vect(KS_wells))\n\n#--- take a look ---#\nhead(tmax_from_prism_stack)  ID PRISM_tmax_stable_4kmD2_20180701_bil PRISM_tmax_stable_4kmD2_20180702_bil\n1  1                               34.241                               30.544\n2  2                               29.288                               29.569\n3  3                               32.585                               29.866\n4  4                               30.104                               29.819\n5  5                               34.232                               30.481\n6  6                               35.168                               30.640\ntmax_from_prism <- terra::extract(as(prism_tmax_stack, \"Raster\"), KS_wells)\n#--- take a look ---#\nhead(tmax_from_prism)  ID PRISM_tmax_stable_4kmD2_20180701_bil\n1  1                               34.241\n2  2                               29.288\n3  3                               32.585\n4  4                               30.104\n5  5                               34.232\n6  6                               35.168\n#--- check the class ---#\nclass(tmax_from_prism)[1] \"data.frame\"\ntmax_from_prism <- \n  terra::extract(\n    as(prism_tmax_stack, \"Raster\"), \n    vect(KS_wells)\n  )Error in (function (classes, fdef, mtable) : unable to find an inherited method for function 'extract' for signature '\"RasterBrick\", \"SpatVector\"'"},{"path":"int-RV.html","id":"extracting-to-polygons-terra-way","chapter":"5 Spatial Interactions of Vector and Raster Data","heading":"5.2.3 Extracting to Polygons (terra way)","text":"can use terra::extract() extracting raster cell values polygons well. polygons, identify raster cells whose center lies inside polygon assign vector values cells polygon. Let’s first convert KS_county_sf (sf object) SpatVector.Now, let’s extract tmax values KS counties., terra::extract() returns data.frame, ID values represent corresponding row number polygons data. example, observations ID == n nth polygon. Using information, can easily merge extraction results polygons data. Suppose interested mean tmax values intersecting cells polygons, can following:Instead finding mean applying terra::extract() done , can within terra::extract() using fun option.can apply summary functions like min(), max(), sum().Extracting values multi-layer raster data works exactly way except data processing value extraction slightly complicated.Similar single-layer case, resulting object data.frame two columns corresponding layers two-layer raster object.Sometimes, like much raster cells intersecting intersecting polygon find area-weighted summary later. case, can add exact = TRUE option terra::extract().can see, now fraction column resulting data.frame can find area-weighted summary extracted values like :","code":"\n#--- Kansas boundary (SpatVector) ---#\nKS_county_sv <- vect(KS_county_sf)\n#--- extract values from the raster for each county ---#\ntmax_by_county <- terra::extract(prism_tmax_0701_KS_sr, KS_county_sv)  \n#--- check the class ---#\nclass(tmax_by_county)[1] \"data.frame\"\n#--- take a look ---#\nhead(tmax_by_county)  ID PRISM_tmax_stable_4kmD2_20180701_bil\n1  1                               34.228\n2  1                               34.222\n3  1                               34.256\n4  1                               34.268\n5  1                               34.262\n6  1                               34.477\n#--- take a look ---#\ntail(tmax_by_county)       ID PRISM_tmax_stable_4kmD2_20180701_bil\n12840 105                               34.185\n12841 105                               34.180\n12842 105                               34.241\n12843 105                               34.381\n12844 105                               34.295\n12845 105                               34.267\n#--- get mean tmax ---#\nmean_tmax <- \n  tmax_by_county %>% \n  group_by(ID) %>% \n  summarize(tmax = mean(PRISM_tmax_stable_4kmD2_20180701_bil))\n\n(\nKS_county_sf <- \n  #--- back to sf ---#\n  st_as_sf(KS_county_sv) %>% \n  #--- define ID ---#\n  mutate(ID := seq_len(nrow(.))) %>% \n  #--- merge by ID ---#\n  left_join(., mean_tmax, by = \"ID\")\n)Simple feature collection with 105 features and 14 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: -102.0517 ymin: 36.99302 xmax: -94.58841 ymax: 40.00316\nGeodetic CRS:  NAD83\nFirst 10 features:\n   STATEFP COUNTYFP COUNTYNS       AFFGEOID GEOID         NAME\n1       20      075 00485327 0500000US20075 20075     Hamilton\n2       20      149 00485038 0500000US20149 20149 Pottawatomie\n3       20      003 00484971 0500000US20003 20003     Anderson\n4       20      033 00484986 0500000US20033 20033     Comanche\n5       20      189 00485056 0500000US20189 20189      Stevens\n6       20      161 00485044 0500000US20161 20161        Riley\n7       20      025 00484982 0500000US20025 20025        Clark\n8       20      163 00485045 0500000US20163 20163        Rooks\n9       20      091 00485010 0500000US20091 20091      Johnson\n10      20      187 00485055 0500000US20187 20187      Stanton\n              NAMELSAD STUSPS STATE_NAME LSAD      ALAND   AWATER ID     tmax\n1      Hamilton County     KS     Kansas   06 2580958328  2893322  1 34.21421\n2  Pottawatomie County     KS     Kansas   06 2177507162 54149295  2 32.58354\n3      Anderson County     KS     Kansas   06 1501263686 10599981  3 34.63195\n4      Comanche County     KS     Kansas   06 2041681089  3604155  4 33.40829\n5       Stevens County     KS     Kansas   06 1883593926   464936  5 34.01768\n6         Riley County     KS     Kansas   06 1579116499 32002514  6 35.54426\n7         Clark County     KS     Kansas   06 2524300310  6603384  7 34.47948\n8         Rooks County     KS     Kansas   06 2306454194 11962259  8 33.55797\n9       Johnson County     KS     Kansas   06 1226694710 16303985  9 34.77049\n10      Stanton County     KS     Kansas   06 1762103387   178555 10 33.56413\n                         geometry\n1  POLYGON ((-102.0446 38.0480...\n2  POLYGON ((-96.72833 39.4271...\n3  POLYGON ((-95.51879 38.0671...\n4  POLYGON ((-99.54467 37.3047...\n5  POLYGON ((-101.5566 37.3884...\n6  POLYGON ((-96.96095 39.2867...\n7  POLYGON ((-100.1072 37.4748...\n8  POLYGON ((-99.60527 39.2638...\n9  POLYGON ((-95.05647 38.9660...\n10 POLYGON ((-102.0419 37.5411...\n#--- extract values from the raster for each county ---#\ntmax_by_county <- \n  terra::extract(\n    prism_tmax_0701_KS_sr, \n    KS_county_sv,\n    fun = mean\n  ) \n#--- take a look ---#\nhead(tmax_by_county)  ID PRISM_tmax_stable_4kmD2_20180701_bil\n1  1                             34.21421\n2  2                             32.58354\n3  3                             34.63195\n4  4                             33.40829\n5  5                             34.01768\n6  6                             35.54426\n#--- extract from a multi-layer raster object ---#\ntmax_by_county_from_stack <- \n  terra::extract(\n    prism_tmax_stack, \n    KS_county_sv\n  ) \n\n#--- take a look ---#\nhead(tmax_by_county_from_stack)  ID PRISM_tmax_stable_4kmD2_20180701_bil PRISM_tmax_stable_4kmD2_20180702_bil\n1  1                               31.457                               29.864\n2  1                               31.503                               29.927\n3  1                               31.604                               29.966\n4  1                               31.718                               30.009\n5  1                               31.741                               30.079\n6  1                               31.881                               30.128\n#--- extract from a multi-layer raster object ---#\ntmax_by_county_from_stack <- \n  terra::extract(\n    prism_tmax_stack, \n    KS_county_sv,\n    exact = TRUE\n  ) \n\n#--- take a look ---#  \nhead(tmax_by_county_from_stack)  ID PRISM_tmax_stable_4kmD2_20180701_bil PRISM_tmax_stable_4kmD2_20180702_bil\n1  1                               31.457                               29.864\n2  1                               31.503                               29.927\n3  1                               31.604                               29.966\n4  1                               31.718                               30.009\n5  1                               31.741                               30.079\n6  1                               31.881                               30.128\n   fraction\n1 0.4514711\n2 0.7977501\n3 0.7975878\n4 0.7974255\n5 0.7972632\n6 0.7971788\ntmax_by_county_from_stack %>% \n  group_by(ID) %>% \n  summarize(\n    tmax_0701 = sum(fraction * PRISM_tmax_stable_4kmD2_20180701_bil) / sum(fraction),\n    tmax_0702 = sum(fraction * PRISM_tmax_stable_4kmD2_20180702_bil) / sum(fraction)\n  )# A tibble: 105 × 3\n      ID tmax_0701 tmax_0702\n   <dbl>     <dbl>     <dbl>\n 1     1      32.3      30.1\n 2     2      35.7      28.8\n 3     3      34.9      29.4\n 4     4      33.8      29.9\n 5     5      34.1      30.5\n 6     6      35.6      29.8\n 7     7      34.1      31.0\n 8     8      33.0      28.9\n 9     9      35.1      29.7\n10    10      33.3      30.2\n# … with 95 more rows"},{"path":"int-RV.html","id":"extracting-to-polygons-exactextractr-way","chapter":"5 Spatial Interactions of Vector and Raster Data","heading":"5.2.4 Extracting to Polygons (exactextractr way)","text":"exact_extract() function exactextractr package faster alternative terra::extract() large raster data confirm later (exact_extract() work points data moment).74 exact_extract() also provides coverage fraction value cell-polygon intersections. syntax exact_extract() much similar terra::extract().exact_extract() can accept SpatRaster Raster\\(^*\\) objects raster object. However, accepts sf polygons object, accept SpatVect.Let’s get tmax values PRISM raster layer Kansas county polygons, following job:resulting object list data.frames ith element list ith polygon polygons sf object.element list, see value coverage_fraction. value tmax value intersecting raster cells, coverage_fraction fraction intersecting area relative full raster grid, can help find coverage-weighted summary extracted values (like fraction variable use terra::extract() exact = TRUE).can take advantage dplyr::bind_rows() combine list datasets single data.frame. , can use .id option create new identifier column links row original data.data.table users can use rbindlist() idcol option like :Note id value represents ith element list, turn corresponds ith polygon polygons sf data. Let’s summarize data id merge back polygons sf data. , calculate coverage-weighted mean tmax.Extracting values RasterStack works exactly manner RasterLayer.can see , exact_extract() appends additional columns additional layers.order find coverage-weighted tmax date, can first pivot long format using dplyr::pivot_longer().find coverage-weighted tmax date:data.table users, :","code":"\n#--- syntax (this does not run) ---#\nexact_extract(raster, polygons) \nlibrary(\"exactextractr\")\n\n#--- extract values from the raster for each county ---#\ntmax_by_county <- \n  exact_extract(\n    prism_tmax_0701_KS_sr, \n    KS_county_sf,\n    #--- this is for not displaying progress bar ---#\n    progress = FALSE\n  )  \n#--- take a look at the first 6 rows of the first two list elements ---#\ntmax_by_county[1:2] %>% lapply(function(x) head(x))[[1]]\n   value coverage_fraction\n1 31.457         0.4514454\n2 31.503         0.7977046\n3 31.604         0.7975423\n4 31.718         0.7973800\n5 31.741         0.7972176\n6 31.881         0.7971521\n\n[[2]]\n   value coverage_fraction\n1 35.492        0.04248790\n2 35.552        0.08910853\n3 35.638        0.08570296\n4 35.668        0.08580886\n5 35.655        0.08667702\n6 35.543        0.08706726\n(\n#--- combine ---#\ntmax_combined <- bind_rows(tmax_by_county, .id = \"id\") %>% \n  as_tibble()\n)# A tibble: 15,149 × 3\n   id    value coverage_fraction\n   <chr> <dbl>             <dbl>\n 1 1      31.5             0.451\n 2 1      31.5             0.798\n 3 1      31.6             0.798\n 4 1      31.7             0.797\n 5 1      31.7             0.797\n 6 1      31.9             0.797\n 7 1      31.9             0.799\n 8 1      32.0             0.801\n 9 1      32.0             0.804\n10 1      32.0             0.807\n# … with 15,139 more rows\nrbindlist(tmax_by_county, idcol = \"id\")          id  value coverage_fraction\n    1:   1 31.457         0.4514454\n    2:   1 31.503         0.7977046\n    3:   1 31.604         0.7975423\n    4:   1 31.718         0.7973800\n    5:   1 31.741         0.7972176\n   ---                             \n15145: 105 33.653         0.2876675\n15146: 105 33.670         0.2943960\n15147: 105 33.737         0.3016187\n15148: 105 33.789         0.3093757\n15149: 105 33.817         0.1121131\n#--- weighted mean ---#\n(\ntmax_by_id <- tmax_combined %>% \n  #--- convert from character to numeric  ---#\n  mutate(id = as.numeric(id)) %>% \n  #--- group summary ---#\n  group_by(id) %>% \n  summarise(tmax_aw = sum(value * coverage_fraction) / sum(coverage_fraction))\n)# A tibble: 105 × 2\n      id tmax_aw\n   <dbl>   <dbl>\n 1     1    32.3\n 2     2    35.7\n 3     3    34.9\n 4     4    33.8\n 5     5    34.1\n 6     6    35.6\n 7     7    34.1\n 8     8    33.0\n 9     9    35.1\n10    10    33.3\n# … with 95 more rows\n#--- merge ---#\nKS_county_sf %>% \n  mutate(id := seq_len(nrow(.))) %>% \n  left_join(., tmax_by_id, by = \"id\") %>% \n  dplyr::select(id, tmax_aw)Simple feature collection with 105 features and 2 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: -102.0517 ymin: 36.99302 xmax: -94.58841 ymax: 40.00316\nGeodetic CRS:  NAD83\nFirst 10 features:\n   id  tmax_aw                       geometry\n1   1 32.31343 POLYGON ((-102.0446 38.0480...\n2   2 35.67419 POLYGON ((-96.72833 39.4271...\n3   3 34.89472 POLYGON ((-95.51879 38.0671...\n4   4 33.81025 POLYGON ((-99.54467 37.3047...\n5   5 34.08601 POLYGON ((-101.5566 37.3884...\n6   6 35.57546 POLYGON ((-96.96095 39.2867...\n7   7 34.05798 POLYGON ((-100.1072 37.4748...\n8   8 33.02438 POLYGON ((-99.60527 39.2638...\n9   9 35.11876 POLYGON ((-95.05647 38.9660...\n10 10 33.32522 POLYGON ((-102.0419 37.5411...\ntmax_by_county_stack <- \n  exact_extract(\n    prism_tmax_stack, \n    KS_county_sf, \n    progress = F\n  )\n\n#--- take a look at the first 6 lines of the first element---#\ntmax_by_county_stack[[1]] %>% head()  PRISM_tmax_stable_4kmD2_20180701_bil PRISM_tmax_stable_4kmD2_20180702_bil\n1                               34.235                               29.632\n2                               34.228                               29.562\n3                               34.222                               29.557\n4                               34.256                               29.662\n5                               34.268                               29.704\n6                               34.262                               29.750\n  coverage_fraction\n1      0.0007206142\n2      0.7114530802\n3      0.7162888646\n4      0.7163895965\n5      0.7166024446\n6      0.7175790071\n#--- combine them ---#\ntmax_all_combined <- bind_rows(tmax_by_county_stack, .id = \"id\") \n\n#--- take a look ---#\nhead(tmax_all_combined)  id PRISM_tmax_stable_4kmD2_20180701_bil PRISM_tmax_stable_4kmD2_20180702_bil\n1  1                               34.235                               29.632\n2  1                               34.228                               29.562\n3  1                               34.222                               29.557\n4  1                               34.256                               29.662\n5  1                               34.268                               29.704\n6  1                               34.262                               29.750\n  coverage_fraction\n1      0.0007206142\n2      0.7114530802\n3      0.7162888646\n4      0.7163895965\n5      0.7166024446\n6      0.7175790071\n#--- pivot to a longer format ---#\n(\ntmax_long <- pivot_longer(\n  tmax_all_combined, \n  -c(id, coverage_fraction), \n  names_to = \"date\",\n  values_to = \"tmax\"\n  )  \n)# A tibble: 30,298 × 4\n   id    coverage_fraction date                                  tmax\n   <chr>             <dbl> <chr>                                <dbl>\n 1 1              0.000721 PRISM_tmax_stable_4kmD2_20180701_bil  34.2\n 2 1              0.000721 PRISM_tmax_stable_4kmD2_20180702_bil  29.6\n 3 1              0.711    PRISM_tmax_stable_4kmD2_20180701_bil  34.2\n 4 1              0.711    PRISM_tmax_stable_4kmD2_20180702_bil  29.6\n 5 1              0.716    PRISM_tmax_stable_4kmD2_20180701_bil  34.2\n 6 1              0.716    PRISM_tmax_stable_4kmD2_20180702_bil  29.6\n 7 1              0.716    PRISM_tmax_stable_4kmD2_20180701_bil  34.3\n 8 1              0.716    PRISM_tmax_stable_4kmD2_20180702_bil  29.7\n 9 1              0.717    PRISM_tmax_stable_4kmD2_20180701_bil  34.3\n10 1              0.717    PRISM_tmax_stable_4kmD2_20180702_bil  29.7\n# … with 30,288 more rows\n(\ntmax_long %>% \n  group_by(id, date) %>% \n  summarize(tmax = sum(tmax * coverage_fraction) / sum(coverage_fraction))\n)# A tibble: 210 × 3\n# Groups:   id [105]\n   id    date                                  tmax\n   <chr> <chr>                                <dbl>\n 1 1     PRISM_tmax_stable_4kmD2_20180701_bil  34.2\n 2 1     PRISM_tmax_stable_4kmD2_20180702_bil  29.7\n 3 10    PRISM_tmax_stable_4kmD2_20180701_bil  33.6\n 4 10    PRISM_tmax_stable_4kmD2_20180702_bil  31.2\n 5 100   PRISM_tmax_stable_4kmD2_20180701_bil  35.2\n 6 100   PRISM_tmax_stable_4kmD2_20180702_bil  29.8\n 7 101   PRISM_tmax_stable_4kmD2_20180701_bil  29.9\n 8 101   PRISM_tmax_stable_4kmD2_20180702_bil  30.0\n 9 102   PRISM_tmax_stable_4kmD2_20180701_bil  33.3\n10 102   PRISM_tmax_stable_4kmD2_20180702_bil  30.2\n# … with 200 more rows\n(\ntmax_all_combined %>% \n  data.table() %>% \n  melt(id.var = c(\"id\", \"coverage_fraction\")) %>% \n  .[, .(tmax = sum(value * coverage_fraction) / sum(coverage_fraction)), by = .(id, variable)]\n)      id                             variable     tmax\n  1:   1 PRISM_tmax_stable_4kmD2_20180701_bil 34.21148\n  2:   2 PRISM_tmax_stable_4kmD2_20180701_bil 32.59907\n  3:   3 PRISM_tmax_stable_4kmD2_20180701_bil 34.62680\n  4:   4 PRISM_tmax_stable_4kmD2_20180701_bil 33.41405\n  5:   5 PRISM_tmax_stable_4kmD2_20180701_bil 34.02482\n ---                                                  \n206: 101 PRISM_tmax_stable_4kmD2_20180702_bil 30.01027\n207: 102 PRISM_tmax_stable_4kmD2_20180702_bil 30.23891\n208: 103 PRISM_tmax_stable_4kmD2_20180702_bil 30.16537\n209: 104 PRISM_tmax_stable_4kmD2_20180702_bil 30.54567\n210: 105 PRISM_tmax_stable_4kmD2_20180702_bil 30.85847"},{"path":"int-RV.html","id":"extract-speed","chapter":"5 Spatial Interactions of Vector and Raster Data","heading":"5.3 Extraction speed comparison","text":"compare extraction speed raster::extract(), terra::extract(), exact_extract().","code":""},{"path":"int-RV.html","id":"points-terraextract-and-rasterextract","chapter":"5 Spatial Interactions of Vector and Raster Data","heading":"5.3.1 Points: terra::extract() and raster::extract()","text":"exact_extract() uses C++ backend. Therefore, considerably faster raster::extract().can see, terra::extract() much faster. time differential two packages can substantial raster data becomes larger.","code":"\n#--- terra ---#\nKS_wells_sv <- vect(KS_wells)\ntic()\ntemp <- terra::extract(prism_tmax_0701_KS_sr, KS_wells_sv)\ntoc()0.057 sec elapsed\n#--- raster ---#\nprism_tmax_0701_KS_lr <- as(prism_tmax_0701_KS_sr, \"Raster\")\ntic()\ntemp <- raster::extract(prism_tmax_0701_KS_lr, KS_wells)\ntoc()0.406 sec elapsed"},{"path":"int-RV.html","id":"polygons-exact_extract-terraextract-and-rasterextract","chapter":"5 Spatial Interactions of Vector and Raster Data","heading":"5.3.2 Polygons: exact_extract(), terra::extract(), and raster::extract()","text":"terra::extract() faster exact_extract() relatively small raster data. Let’s time see difference.can see, raster::extract() far slowest. terra::extract() faster exact_extract(). However, raster data becomes larger (spatially finer), exact_extact() starts shine.Let’s disaggregate prism data factor 10 create much larger raster data.75The disaggregated PRISM data now 10 times rows columns.Now, let’s compare terra::extrct() exact_extrct() using disaggregated data.can see, exact_extract() considerably faster. difference time becomes even pronounced size raster data becomes larger number polygons greater. time difference several seconds seem nothing, imagine processing PRISM files entire US 20 years, appreciate speed exact_extract().","code":"\n#--- terra::extract ---#\ntic()\nterra_extract_temp <- \n  terra::extract(\n    prism_tmax_0701_KS_sr, \n    KS_county_sv\n  )  \ntoc()0.072 sec elapsed\n#--- exact_extract ---#\ntic()\nexact_extract_temp <- \n  exact_extract(\n    prism_tmax_0701_KS_sr, \n    KS_county_sf, \n    progress = FALSE\n  )  \ntoc()0.255 sec elapsed\n#--- raster::extract ---#\ntic()\nraster_extract_temp <- \n  raster::extract(\n    prism_tmax_0701_KS_lr, \n    KS_county_sf\n  )  \ntoc()2.61 sec elapsed\n#--- disaggregate ---#\n(\nprism_tmax_0701_KS_sr_10 <- terra::disagg(prism_tmax_0701_KS_sr, fact = 10)\n)class       : SpatRaster \ndimensions  : 730, 1790, 1  (nrow, ncol, nlyr)\nresolution  : 0.004166667, 0.004166667  (x, y)\nextent      : -102.0625, -94.60417, 36.97917, 40.02083  (xmin, xmax, ymin, ymax)\ncoord. ref. : lon/lat NAD83 \nsource      : memory \nname        : PRISM_tmax_stable_4kmD2_20180701_bil \nmin value   :                               27.711 \nmax value   :                               37.656 \n#--- original ---#\ndim(prism_tmax_0701_KS_sr)  [1]  73 179   1\n#--- disaggregated ---#\ndim(prism_tmax_0701_KS_sr_10)  [1]  730 1790    1\n#--- terra extract ---#\ntic()\nterra_extract_temp <- \n  terra::extract(\n    prism_tmax_0701_KS_sr_10, \n    KS_county_sv\n  )  \ntoc()4.604 sec elapsed\n#--- exact extract ---#\ntic()\nexact_extract_temp <- \n  exact_extract(\n    prism_tmax_0701_KS_sr_10, \n    KS_county_sf, \n    progress = FALSE\n  )  \ntoc()0.288 sec elapsed"},{"path":"int-RV.html","id":"single-layer-vs-multi-layer","chapter":"5 Spatial Interactions of Vector and Raster Data","heading":"5.3.3 Single-layer vs multi-layer","text":"Pretend five dates PRISM tmax data (repeat file five times) like extract values . Extracting values multi-layer raster objects (RasterStack raster package) takes less time extracting values individual layers one time. can observed .terra::extract()exact_extract()reduction computation time methods makes sense. Since layers exactly geographic extent resolution, finding polygons-cells correspondence done can used repeatedly across layers multi-layer SparRaster RasterStack. clearly suggests processing many layers spatial resolution extent, first stack extract values time instead processing one one long memory allows .","code":"\n#--- extract from 5 layers one at a time ---#\ntic()\ntemp <- terra::extract(prism_tmax_0701_KS_sr_10, KS_county_sv)\ntemp <- terra::extract(prism_tmax_0701_KS_sr_10, KS_county_sv)\ntemp <- terra::extract(prism_tmax_0701_KS_sr_10, KS_county_sv)\ntemp <- terra::extract(prism_tmax_0701_KS_sr_10, KS_county_sv)\ntemp <- terra::extract(prism_tmax_0701_KS_sr_10, KS_county_sv)\ntoc()24.279 sec elapsed\n#--- extract from a 5-layer SpatRaster ---#\ntic()\nprism_tmax_ml_5 <- \n  c(\n    prism_tmax_0701_KS_sr_10, \n    prism_tmax_0701_KS_sr_10, \n    prism_tmax_0701_KS_sr_10, \n    prism_tmax_0701_KS_sr_10, \n    prism_tmax_0701_KS_sr_10\n  )\ntemp <- terra::extract(prism_tmax_ml_5, KS_county_sv)\ntoc()5.776 sec elapsed\n#--- extract from 5 layers one at a time ---#\ntic()\ntemp <- exact_extract(prism_tmax_0701_KS_sr_10, KS_county_sf, progress = FALSE)\ntemp <- exact_extract(prism_tmax_0701_KS_sr_10, KS_county_sf, progress = FALSE)\ntemp <- exact_extract(prism_tmax_0701_KS_sr_10, KS_county_sf, progress = FALSE)\ntemp <- exact_extract(prism_tmax_0701_KS_sr_10, KS_county_sf, progress = FALSE)\ntemp <- exact_extract(prism_tmax_0701_KS_sr_10, KS_county_sf, progress = FALSE)\ntoc()1.571 sec elapsed\n#--- extract from from a 5-layer SpatRaster ---#\ntic()\nprism_tmax_stack_5 <- \n  c(\n    prism_tmax_0701_KS_sr_10, \n    prism_tmax_0701_KS_sr_10, \n    prism_tmax_0701_KS_sr_10, \n    prism_tmax_0701_KS_sr_10, \n    prism_tmax_0701_KS_sr_10\n  )\ntemp <- \n  exact_extract(\n    prism_tmax_stack_5, \n    KS_county_sf, \n    progress = FALSE\n  )\ntoc()0.433 sec elapsed"},{"path":"EE.html","id":"EE","chapter":"6 Extraction Speed Considerations","heading":"6 Extraction Speed Considerations","text":"","code":""},{"path":"EE.html","id":"before-you-start-5","chapter":"6 Extraction Speed Considerations","heading":"Before you start","text":"chapter, learn parallelize raster data extraction polygons data. cover parallelization raster data extraction points data fast. Thus, repeated raster data extractions points unlikely bottleneck work. first start parallelizing data extraction single-layer raster data. move multi-layer raster data case.different ways parallelizing extraction process. discuss several parallelization approaches terms speed memory footprint. learn parallelize matters. naive parallelization can actually increase time raster data extraction, clever parallelization approach can save hours even days (depending size extraction job, course).use future.apply parallel packages parallelization. Basic knowledge parallelization using packages assumed. familiar parallelized looping using lapply() parallelization using mclapply() (Mac Linux users ) future_lapply() (including Windows), see Chapter first.","code":""},{"path":"EE.html","id":"direction-for-replication-5","chapter":"6 Extraction Speed Considerations","heading":"Direction for replication","text":"DatasetsAll datasets need import available . chapter, path files set relative working directory (hidden). run codes without mess paths files, follow steps:set folder (folder) working directory using setwd()create folder called “Data” inside folder designated working directory (created “Data” folder previously, skip step)download pertinent datasets hereplace files downloaded folder “Data” folderWarning: folder includes series daily PRISM datasets stored month 10 years. amount \\(12.75\\) GB data.PackagesRun following code install load (already installed) pacman package, install load (already installed) listed package inside pacman::p_load() function.","code":"\nif (!require(\"pacman\")) install.packages(\"pacman\")\npacman::p_load(\n  parallel, # for parallelization\n  future.apply, # for parallelization\n  terra, # handle raster data\n  raster, # handle raster data\n  exactextractr, # fast extractions\n  sf, # vector data operations\n  dplyr, # data wrangling\n  data.table, # data wrangling\n  prism # download PRISM data\n)  "},{"path":"EE.html","id":"single-raster-layer","chapter":"6 Extraction Speed Considerations","heading":"6.1 Single raster layer","text":"Let’s prepare parallel processing rest section.","code":"\nlibrary(parallel)\n\n#--- get the number of logical cores to use ---#\n(\nnum_cores <- detectCores() - 1\n)[1] 15"},{"path":"EE.html","id":"datasets","chapter":"6 Extraction Speed Considerations","heading":"6.1.1 Datasets","text":"use following datasets:raster: Iowa Cropland Data Layer (CDL) data 2015polygons: Regular polygon grids IowaIowa CDL data 2015Values recorded raster data integers representing land use type.Regularly-sized grids IowaHere look (Figure 6.1):\nFigure 6.1: Regularly-sized grids land use type Iowa 2105\n","code":"\n#--- Iowa CDL in 2015 ---#\n(\nIA_cdl_15 <- raster(\"Data/IA_cdl_2015.tif\")\n)class      : RasterLayer \ndimensions : 11671, 17795, 207685445  (nrow, ncol, ncell)\nresolution : 30, 30  (x, y)\nextent     : -52095, 481755, 1938165, 2288295  (xmin, xmax, ymin, ymax)\ncrs        : +proj=aea +lat_0=23 +lon_0=-96 +lat_1=29.5 +lat_2=45.5 +x_0=0 +y_0=0 +ellps=GRS80 +units=m +no_defs \nsource     : IA_cdl_2015.tif \nnames      : IA_cdl_2015 \nvalues     : 0, 229  (min, max)\n#--- regular grids over Iowa ---#\n(\nIA_grids <- st_as_sf(map(\"state\", \"iowa\", plot = FALSE, fill = TRUE)) %>% \n  #--- create regularly-sized grids ---#\n  st_make_grid(n = c(50, 50)) %>% \n  #--- project to the CRS of the CDL data ---#\n  st_transform(projection(IA_cdl_15)) %>% \n  #--- convert to sf from sfc  ---#\n  st_as_sf()\n)Simple feature collection with 2500 features and 0 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: -51047.02 ymin: 1929141 xmax: 493167.9 ymax: 2294247\nCRS:           +proj=aea +lat_0=23 +lon_0=-96 +lat_1=29.5 +lat_2=45.5 +x_0=0 +y_0=0 +ellps=GRS80 +units=m +no_defs\nFirst 10 features:\n                                x\n1  POLYGON ((-51047.02 1929303...\n2  POLYGON ((-40156.64 1929241...\n3  POLYGON ((-29266.18 1929194...\n4  POLYGON ((-18375.66 1929162...\n5  POLYGON ((-7485.114 1929144...\n6  POLYGON ((3405.449 1929141,...\n7  POLYGON ((14296 1929153, 25...\n8  POLYGON ((25186.53 1929180,...\n9  POLYGON ((36077.02 1929222,...\n10 POLYGON ((46967.43 1929278,...\ntm_shape(IA_cdl_15) +\n  tm_raster(title = \"Land Use \") +\ntm_shape(IA_grids) +\n  tm_polygons(alpha = 0) +\ntm_layout(legend.outside = TRUE)"},{"path":"EE.html","id":"parallelization","chapter":"6 Extraction Speed Considerations","heading":"6.1.2 Parallelization","text":"long takes extract raster data values polygon grids using exact_extract().One way parallelize process let core work one polygon time. Let’s first define function extract values one polygon run polygons parallelized.can see, terrible way parallelize computation process. see , let’s look computation time extracting one polygon, two polygons, five polygons.\nFigure 6.2: Comparison computation time raster data extractions\ncan see Figure 6.2, significant overhead (0.23 seconds) irrespective number polygons extract data . process initiated ready start extracting values polygons, spend much time processing additional units polygon. , typical example parallelize. Since core processes \\(136\\) polygons, simple math suggests spend least 31.28 (0.23 \\(\\times\\) 136) seconds just preparing extraction jobs.can minimize overhead much possible core use exact_extract() multiple polygons processed single call. Specifically, split collection polygons 15 groups core extract one group.Great, much better.76Now, can reduce processing time reducing size object returned core collated one. code , core returns list data.frames grid group multiple values intersecting raster cells.total, 3GB data collated one list 15 cores. turns , process costly. see , take look following example exact_extrct() processes run, yet nothing returned core.Approximately 5.212 seconds used just collect 3GB worth data cores one.cases, carry around individual cell values landuse types subsequent analysis. example, Demonstration 3 (Chapter 1.3) just need summary (count) unique landuse type polygon. , let’s get summary computer collect objects returned core follows:course slower one returns nothing, much faster one reduce size outcome collation.can see, computation time fastest approach now much less, still gained 81.21. much time spend writing code parallelized group processing? Three minutes. Obviously, matters total time (coding time plus processing time) spend get desired outcome. Indeed, time save clever coding 89.72 seconds. Writing kind code attempt make code faster takes time . , don’t even try make code faster processing time quite short first place. start parallelizing things, go need go terms coding head, judge ’s worth .Imagine processing CDL data states 2009 2020. , whole process take roughly 15.25 (\\(51 \\times 12 \\times 89.721/60/60\\)) hours. , super rough calculation tells us whole process done 1.45 hours parallelized way best approach saw . 15.25 still terrible (execute program go bed, results available afternoon next day.), worth parallelizing process even taking account time need spend code parallelization process.","code":"\ntic()\ntemp <- exact_extract(IA_cdl_15, IA_grids) \ntoc()elapsed \n 26.879 \n#--- function to extract raster values for a single polygon ---#\nget_values_i <- function(i){\n\n  temp <- exact_extract(IA_cdl_15, IA_grids[i, ])\n\n  return(temp)\n}\n\n#--- parallelized ---#\ntic()\ntemp <- mclapply(1:nrow(IA_grids), get_values_i, mc.cores = num_cores)\ntoc()elapsed \n 89.721 \nlibrary(microbenchmark)\nmb <- microbenchmark(\n  \"p_1\" = {\n    temp <- exact_extract(IA_cdl_15, IA_grids[1, ])\n  },\n  \"p_2\" = {\n    temp <- exact_extract(IA_cdl_15, IA_grids[1:2, ])\n  },\n  \"p_3\" = {\n    temp <- exact_extract(IA_cdl_15, IA_grids[1:3, ])\n  },\n  \"p_4\" = {\n    temp <- exact_extract(IA_cdl_15, IA_grids[1:4, ])\n  },\n  \"p_5\" = {\n    temp <- exact_extract(IA_cdl_15, IA_grids[1:5, ])\n  },\n  times = 100\n)\nmb %>% data.table() %>% \n  .[, expr := gsub(\"p_\", \"\", expr)] %>% \n  ggplot(.) +\n    geom_boxplot(aes(y = time/1e9, x = expr)) +\n    ylim(0, NA) +\n    ylab(\"seconds\") +\n    xlab(\"number of polygons to process\")\n#--- number of polygons in a group ---#\nnum_in_group <- floor(nrow(IA_grids)/num_cores)\n\n#--- assign group id to polygons ---#\nIA_grids <- IA_grids %>%  \n  mutate(\n    #--- create grid id ---#\n    grid_id = 1:nrow(.),\n    #--- assign group id  ---#\n    group_id = grid_id %/% num_in_group + 1\n  )\n\ntic()\n#--- parallelized processing by group ---#\ntemp <- mclapply(\n  1:num_cores, \n  function(i) exact_extract(IA_cdl_15, filter(IA_grids, group_id == i)),\n  mc.cores = num_cores\n) \ntoc()elapsed \n 11.901 \n#--- take a look at the the values extracted for the 1st polygon of the 1st group---#\nhead(temp[[1]][[1]])[1] \"Error in (function (cond)  : \\n  error in evaluating the argument 'y' in selecting a method for function 'exact_extract': Problem while computing `..1 = group_id == i`.\\nCaused by error:\\n! object 'group_id' not found\\n\"\n#--- the size of the list of data returned by the first core ---#\nobject.size(temp[[1]]) %>%  format(units = \"GB\")[1] \"0 Gb\"\n#--- define the function to extract values by block of polygons ---#\nextract_by_group <- function(i){\n  temp <- exact_extract(IA_cdl_15, filter(IA_grids, group_id == i))\n\n  #--- returns nothing! ---#\n  return(NULL)\n}\n\n#--- parallelized processing by group ---#\ntic()\ntemp <- mclapply(\n  1:num_cores, \n  function(i) extract_by_group(i),\n  mc.cores = num_cores\n)   \ntoc()elapsed \n  6.689 \nextract_by_group_reduced <- function(i){\n\n  temp_return <- exact_extract(\n    IA_cdl_15, \n    filter(IA_grids, group_id == i)\n    ) %>% \n    #--- combine the list of data.frames into one with polygon id ---#\n    rbindlist(idcol = \"id_within_group\") %>% \n    #--- find the count of land use type values by polygon ---#\n    .[, .(num_value = .N), by = .(value, id_within_group)]\n\n  return(temp_return)\n}\n\ntic()\n#--- parallelized processing by group ---#\ntemp <- mclapply(\n  1:num_cores, \n  function(i) extract_by_group_reduced(i),\n  mc.cores = num_cores\n)   \ntoc()elapsed \n  8.514 "},{"path":"EE.html","id":"summary-1","chapter":"6 Extraction Speed Considerations","heading":"6.1.3 Summary","text":"let core runs small tasks (extracting raster values one polygon time), suffer significant overhead.Blocking one way avoid problem .Reduce size outcome core much possible spend less time simply collating one.forget time spend coding parallelized processes.","code":""},{"path":"EE.html","id":"many-multi-layer","chapter":"6 Extraction Speed Considerations","heading":"6.2 Many multi-layer raster files","text":"discuss ways parallelize process extracting values many multi-layer raster files.","code":""},{"path":"EE.html","id":"datasets-1","chapter":"6 Extraction Speed Considerations","heading":"6.2.1 Datasets","text":"use following datasets:raster: daily PRISM data 2010 2019 stacked monthpolygons: US County polygonsdaily PRISM precipitation 2010 2019You can download prism files . interested learning generate series daily PRISM data files stored month, see section 9.3 code.US counties","code":"\n(\nUS_county <- st_as_sf(map(database = \"county\", plot = FALSE, fill = TRUE)) %>% \n  #--- get state name from ID  ---#\n  mutate(state = str_split(ID, \",\") %>% lapply(., `[[`, 1) %>% unlist) %>% \n  #--- project to the CRS of the CDL data ---#\n  st_transform(projection(brick(\"Data/PRISM/PRISM_ppt_y2017_m7.tif\"))) \n)Simple feature collection with 3076 features and 2 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -124.6813 ymin: 25.12993 xmax: -67.00742 ymax: 49.38323\nCRS:           +proj=longlat +datum=NAD83 +no_defs\nFirst 10 features:\n                 ID                           geom   state\n1   alabama,autauga MULTIPOLYGON (((-86.50517 3... alabama\n2   alabama,baldwin MULTIPOLYGON (((-87.93757 3... alabama\n3   alabama,barbour MULTIPOLYGON (((-85.42801 3... alabama\n4      alabama,bibb MULTIPOLYGON (((-87.02083 3... alabama\n5    alabama,blount MULTIPOLYGON (((-86.9578 33... alabama\n6   alabama,bullock MULTIPOLYGON (((-85.66866 3... alabama\n7    alabama,butler MULTIPOLYGON (((-86.8604 31... alabama\n8   alabama,calhoun MULTIPOLYGON (((-85.74313 3... alabama\n9  alabama,chambers MULTIPOLYGON (((-85.59416 3... alabama\n10 alabama,cherokee MULTIPOLYGON (((-85.46812 3... alabama"},{"path":"EE.html","id":"non-par-ext-multi","chapter":"6 Extraction Speed Considerations","heading":"6.2.2 Non-parallelized extraction","text":"already learned Chapter 5.3 extracting values stacked raster layers faster multiple single-layer raster datasets one time. , daily precipitation datasets stacked year-month saved multi-layer GeoTIFF files. example, PRISM_ppt_y2009_m1.tif stores daily precipitation data January, 2009. long takes extract values US counties month’s daily PRISM precipitation data.Now, process precipitation data 2009-2018, consider two approaches section :parallelize polygons (blocked) regular loop year-monthparallelize year-month","code":"\ntic()\ntemp <- exact_extract(stack(\"Data/PRISM/PRISM_ppt_y2009_m1.tif\"), US_county, progress = F) \ntoc()elapsed \n 26.571 "},{"path":"EE.html","id":"approach-1-parallelize-over-polygons-and-do-regular-loop-over-year-month","chapter":"6 Extraction Speed Considerations","heading":"6.2.3 Approach 1: parallelize over polygons and do regular loop over year-month","text":"approach, let’s measure time spent processing one year-month PRISM dataset guess long take process 120 year-month PRISM datasets.Okay, approach really help. process 10 years daily PRISM data, take roughly 167.39 minutes.","code":"\n#--- number of polygons in a group ---#\nnum_in_group <- floor(nrow(US_county)/num_cores)\n\n#--- define group id ---#\nUS_county <- US_county %>%  \n  mutate(\n    #--- create grid id ---#\n    poly_id = 1:nrow(.),\n    #--- assign group id  ---#\n    group_id = poly_id %/% num_in_group + 1\n  )\n\nextract_by_group <- function(i){\n  temp_return <- exact_extract(\n    stack(\"Data/PRISM/PRISM_ppt_y2009_m1.tif\"), \n    filter(US_county, group_id == i)\n    ) %>% \n    #--- combine the list of data.frames into one with polygon id ---#\n    rbindlist(idcol = \"id_within_group\") %>% \n    #--- find the count of land use type values by polygon ---#\n    melt(id.var = c(\"id_within_group\", \"coverage_fraction\")) %>% \n    .[, sum(value * coverage_fraction)/sum(coverage_fraction), by = .(id_within_group, variable)]\n\n  return(temp_return)\n}\n\ntic()\ntemp <- mclapply(1:num_cores, extract_by_group, mc.cores = num_cores)\ntoc()elapsed \n 83.694 "},{"path":"EE.html","id":"approach-2-parallelize-over-the-temporal-dimension-year-month","chapter":"6 Extraction Speed Considerations","heading":"6.2.4 Approach 2: parallelize over the temporal dimension (year-month)","text":"Instead parallelize polygons, let’s parallelize time (year-month). , first create data.frame year-month combinations work .following function extract data single year-month case:loop rows month_year_data parallel.took 7.52 minutes. , Approach 2 clear winner.","code":"\n(\nmonth_year_data <- expand.grid(month  = 1:12, year = 2009:2018) %>% \n  data.table()\n)\nget_prism_by_month <- function(i, vector){\n\n  temp_month <- month_year_data[i, month] # month to work on\n  temp_year <- month_year_data[i, year] # year to work on\n\n  #--- import raster data ---#\n  temp_raster <- stack(paste0(\"Data/PRISM/PRISM_ppt_y\", temp_year, \"_m\", temp_month, \".tif\")) \n\n  temp <- exact_extract(temp_raster, vector) %>% \n    #--- combine the extraction results into one data.frame ---#\n    rbindlist(idcol = \"row_id\") %>% \n    #--- wide to long ---#\n    melt(id.var = c(\"row_id\", \"coverage_fraction\")) %>% \n    #--- find coverage-weighted average ---#\n    .[, sum(value*coverage_fraction)/sum(coverage_fraction), by = .(row_id, variable)]\n\n  return(temp)\n\n  gc()\n}\ntic()\ntemp <- mclapply(1:nrow(month_year_data), function(x) get_prism_by_month(x, US_county), mc.cores = num_cores)\ntoc()elapsed \n451.477 "},{"path":"EE.html","id":"memory-consideration","chapter":"6 Extraction Speed Considerations","heading":"6.2.5 Memory consideration","text":"far, paid attention memory footprint parallelized processes. , crucial parallelizing many large datasets. Approaches 1 2 differ substantially memory footprints.Approach 1 divides polygons group polygons parallelizes groups extracting raster values. Approach 2 extracts holds raster values 15 whole U.S. polygons. , Approach 1 clearly lesser memory footprint. Approach 2 used 40 Gb computer’s memory, almost maxing 64 Gb RAM memory computer (’s just R C++ consuming RAM memory time). go limit, perfectly fine. Approach 2 definitely better option . However, 32 Gb RAM memory, Approach 2 suffered significant loss performance, Approach 1 . , raster data twice many cells spatial extent, Approach 2 suffered significant loss performance, Approach 1 .easy come case Approach 1 preferable. example, suppose multiple 10-Gb raster layers computer 16 Gb RAM memory. , Approach 2 clearly work, Approach 1 choice, better parallelizing .summary, letting core process larger amount data, need careful exceed RAM memory limit computer.","code":""},{"path":"stars-basics.html","id":"stars-basics","chapter":"7 Spatiotemporal Raster Data Handling with stars","heading":"7 Spatiotemporal Raster Data Handling with stars","text":"","code":""},{"path":"stars-basics.html","id":"before-you-start-6","chapter":"7 Spatiotemporal Raster Data Handling with stars","heading":"Before you start","text":"Chapter, introduce stars package (Pebesma 2020) raster data handling. can particularly useful use spatiotemporal raster data often (like daily PRISM Daymet data) brings framework provides consistent treatment raster data temporal dimensions. Specifically, stars objects can time dimension addition spatial 2D dimensions (longitude latitude), time dimension can take Date values.77 can handy several reasons see (e.g., filtering data date).Another advantage stars package compatibility sf objects lead developer two packages person. Therefore, unlike terra package approach, need tedious conversions sf SpatVector. stars package also allows dplyr-like data operations using functions like filter(), mutate() (see section 7.6).Chapters 4 5, used raster terra packages handle raster data interact raster data vector data. feel inconvenience approach, need read . Also, note stars package written replace either raster terra packages. good summary raster functions map stars functions. can see, many functions available raster packages implemented stars package. However, must say functionality stars package rather complete least economists, definitely possible use just stars package raster data work cases.78Finally, book cover use stars_proxy big data fit memory, may useful . provides introduction stars_proxy interested. book also cover irregular raster cells (e.g., curvelinear grids). Interested readers referred .","code":""},{"path":"stars-basics.html","id":"direction-for-replication-6","chapter":"7 Spatiotemporal Raster Data Handling with stars","heading":"Direction for replication","text":"DatasetsAll datasets need import available . chapter, path files set relative working directory (hidden). run codes without mess paths files, follow steps:set folder (folder) working directory using setwd()create folder called “Data” inside folder designated working directory (created “Data” folder previously, skip step)download pertinent datasets put “Data” folderPackagesRun following code install load (already installed) pacman package, install load (already installed) listed package inside pacman::p_load() function.Run following code define theme map:","code":"\nif (!require(\"pacman\")) install.packages(\"pacman\")\npacman::p_load(\n  stars, # spatiotemporal data handling\n  sf, # vector data handling\n  tidyverse, # data wrangling\n  cubelyr, # handle raster data\n  tmap, # make maps\n  mapview, # make maps\n  exactextractr, # fast raster data extraction\n  lubridate, # handle dates\n  prism # download PRISM data\n)\ntheme_set(theme_bw())\n\ntheme_for_map <- theme(\n  axis.ticks = element_blank(),\n  axis.text= element_blank(), \n  axis.line = element_blank(),\n  panel.border = element_blank(),\n  panel.grid.major = element_line(color='transparent'),\n  panel.grid.minor = element_line(color='transparent'),\n  panel.background = element_blank(),\n  plot.background = element_rect(fill = \"transparent\",color='transparent')\n)"},{"path":"stars-basics.html","id":"stars-structure","chapter":"7 Spatiotemporal Raster Data Handling with stars","heading":"7.1 Understanding the structure of a stars object","text":"Let’s import stars object daily PRISM precipitation tmax saved R dataset.stars object two attributes: ppt (precipitation) tmax (maximum temperature). like variables regular data.frame. hold information interested using analysis.stars object three dimensions: x (longitude), y (latitude), date. dimension , , offset, delta, refsys, point, values. definitions (except point79):: beginning indexto: ending indexoffset: starting valuedelta: step valuerefsys: GCS CRS x y (can Date date dimension)values: values dimension objects regularIn order understand dimensions stars objects, let’s first take look figure (Figure 7.1), visualizes tmax values 2D x-y surfaces stars objects 10th date value (spatial distribution tmax particular date).\nFigure 7.1: Map tmax values August 10, 2009: 20 20 matrix cells\ncan consider 2D x-y surface matrix, location cell defined row number column number. Since x 1 x 20, 20 columns. Similarly, since y 1 y 20, 20 rows. offset value x y dimensions longitude latitude upper-left corner point upper left cell 2D x-y surface, respectively (red circle Figure 7.1).80 refsys indicates, NAD83 GCS. longitude upper-left corner point cells \\(j\\)th column (left) 2D x-y surface -121.7291667 + \\((j-1)\\times\\) 0.0416667, -121.7291667 offset x 0.0416667 delta x. Similarly, latitude upper-left corner point cells \\(\\)th row (top) 2D x-y surface 46.6458333 +\\((-1)\\times\\) -0.0416667, 46.6458333 offset y -0.0416667 delta y.dimension characteristics x y shared layers across date dimension, particular combination x y indexes refers exactly location earth layers across dates (course). date dimension, 10 date values since date 1 date 10. refsys date dimension Date. Since offset 2009-08-11 delta 1, \\(k\\)th layer represents tmax values August \\(11+k-1\\), 2009.Putting information together, 20 20 x-y surfaces stacked date dimension (10 layers), thus making 20 20 10 three-dimensional array (cube) shown figure (Figure 7.2).\nFigure 7.2: Visual illustration stars data structure\nRemember prcp_tmax_PRISM_m8_y09 also another attribute (ppt) structured exactly way tmax. , prcp_tmax_PRISM_m8_y09 basically four dimensions: attribute, x, y, date.guaranteed dimensions regularly spaced timed. irregular dimension, dimension values stored values instead using indexes, offset, delta find dimension values. example, observe satellite data 6-day gaps sometimes 7-day gaps times, date dimension irregular. see made-example irregular time dimension Chapter 7.5.","code":"\n#--- read PRISM prcp and tmax data  ---#\n(\nprcp_tmax_PRISM_m8_y09 <- readRDS(\"Data/prcp_tmax_PRISM_m8_y09_small.rds\")\n)stars object with 3 dimensions and 2 attributes\nattribute(s):\n       Min.  1st Qu. Median      Mean  3rd Qu.   Max.\nppt   0.000  0.00000  0.000  1.292334  0.01100 30.851\ntmax  1.833 17.55575 21.483 22.035435 26.54275 39.707\ndimension(s):\n     from to     offset      delta refsys point values x/y\nx       1 20   -121.729  0.0416667  NAD83 FALSE   NULL [x]\ny       1 20    46.6458 -0.0416667  NAD83 FALSE   NULL [y]\ndate    1 10 2009-08-11     1 days   Date    NA   NULL    "},{"path":"stars-basics.html","id":"some-basic-operations-on-stars-objects","chapter":"7 Spatiotemporal Raster Data Handling with stars","heading":"7.2 Some basic operations on stars objects","text":"","code":""},{"path":"stars-basics.html","id":"subset-a-stars-object-by-index","chapter":"7 Spatiotemporal Raster Data Handling with stars","heading":"7.2.1 Subset a stars object by index","text":"order access value attribute (say ppt) particular location particular time stars object (prcp_tmax_PRISM_m8_y09), need tell R interested ppt attribute specify corresponding index x, y, date. , get ppt value (3, 4) cell date = 10.much similar accessing value matrix except dimensions. first argument selects attribute interest, 2nd x, 3rd y, 4th date.course, can subset stars object access value multiple cells like :","code":"\nprcp_tmax_PRISM_m8_y09[\"ppt\", 3, 4, 10]stars object with 3 dimensions and 1 attribute\nattribute(s):\n     Min. 1st Qu. Median Mean 3rd Qu. Max.\nppt     0       0      0    0       0    0\ndimension(s):\n     from to     offset      delta refsys point values x/y\nx       3  3   -121.729  0.0416667  NAD83 FALSE   NULL [x]\ny       4  4    46.6458 -0.0416667  NAD83 FALSE   NULL [y]\ndate   10 10 2009-08-11     1 days   Date    NA   NULL    \nprcp_tmax_PRISM_m8_y09[\"ppt\", 3:6, 3:4, 5:10]stars object with 3 dimensions and 1 attribute\nattribute(s):\n     Min. 1st Qu. Median     Mean 3rd Qu.  Max.\nppt     0       0      0 0.043125       0 0.546\ndimension(s):\n     from to     offset      delta refsys point values x/y\nx       3  6   -121.729  0.0416667  NAD83 FALSE   NULL [x]\ny       3  4    46.6458 -0.0416667  NAD83 FALSE   NULL [y]\ndate    5 10 2009-08-11     1 days   Date    NA   NULL    "},{"path":"stars-basics.html","id":"set-attribute-names","chapter":"7 Spatiotemporal Raster Data Handling with stars","heading":"7.2.2 Set attribute names","text":"read raster dataset GeoTIFF file, attribute name file name default. , often encounter cases want change attribute name. can set name attributes using setNames().Note attribute names prcp_tmax_PRISM_m8_y09 changed operation (just like mutate() dplyr functions ):want reflect changes variable names keeping object name, need assign output setNames() object follows:using function Chapter 7.7.2.","code":"\nprcp_tmax_PRISM_m8_y09_dif_names <- setNames(prcp_tmax_PRISM_m8_y09, c(\"precipitation\", \"maximum_temp\"))\nprcp_tmax_PRISM_m8_y09stars object with 3 dimensions and 2 attributes\nattribute(s):\n       Min.  1st Qu. Median      Mean  3rd Qu.   Max.\nppt   0.000  0.00000  0.000  1.292334  0.01100 30.851\ntmax  1.833 17.55575 21.483 22.035435 26.54275 39.707\ndimension(s):\n     from to     offset      delta refsys point values x/y\nx       1 20   -121.729  0.0416667  NAD83 FALSE   NULL [x]\ny       1 20    46.6458 -0.0416667  NAD83 FALSE   NULL [y]\ndate    1 10 2009-08-11     1 days   Date    NA   NULL    \n#--- NOT RUN ---#\n# the codes in the rest of the chapter use \"ppt\" and \"tmax\" as the variables names, not these ones\nprcp_tmax_PRISM_m8_y09 <- setNames(prcp_tmax_PRISM_m8_y09, c(\"precipitation\", \"maximum_temp\"))"},{"path":"stars-basics.html","id":"get-the-coordinate-reference-system","chapter":"7 Spatiotemporal Raster Data Handling with stars","heading":"7.2.3 Get the coordinate reference system","text":"can get CRS stars object using st_crs(), actually function name used extract CRS sf object.use change projection vector datasets interact :crop stars raster dataset spatial extent sf objectsextract values stars raster dataset sf objectsas see Chapter 5. Notice also used exactly function name (st_crs()) get CRS sf objects (see Chapter 2).","code":"\nst_crs(prcp_tmax_PRISM_m8_y09)Coordinate Reference System:\n  User input: GEOGCS[\"NAD83\",DATUM[\"North_American_Datum_1983\",SPHEROID[\"GRS 1980\",6378137,298.2572221010042,AUTHORITY[\"EPSG\",\"7019\"]],AUTHORITY[\"EPSG\",\"6269\"]],PRIMEM[\"Greenwich\",0],UNIT[\"degree\",0.0174532925199433],AUTHORITY[\"EPSG\",\"4269\"]] \n  wkt:\nGEOGCS[\"NAD83\",\n    DATUM[\"North_American_Datum_1983\",\n        SPHEROID[\"GRS 1980\",6378137,298.2572221010042,\n            AUTHORITY[\"EPSG\",\"7019\"]],\n        AUTHORITY[\"EPSG\",\"6269\"]],\n    PRIMEM[\"Greenwich\",0],\n    UNIT[\"degree\",0.0174532925199433],\n    AUTHORITY[\"EPSG\",\"4269\"]]"},{"path":"stars-basics.html","id":"get-the-dimension-characteristics-and-values","chapter":"7 Spatiotemporal Raster Data Handling with stars","heading":"7.2.4 Get the dimension characteristics and values","text":"can access dimension values using st_dimensions().following shows can get object.example, can get offset x follows:can extract dimension values using st_get_dimension_values(). example, get values date,can handy see Chapter 9.4.2. Later Chapter 7.5, learn set dimensions using st_set_dimensions().","code":"\n#--- get dimension characteristics ---#\n(\ndim_prcp_tmin <- st_dimensions(prcp_tmax_PRISM_m8_y09)\n)     from to     offset      delta refsys point values x/y\nx       1 20   -121.729  0.0416667  NAD83 FALSE   NULL [x]\ny       1 20    46.6458 -0.0416667  NAD83 FALSE   NULL [y]\ndate    1 10 2009-08-11     1 days   Date    NA   NULL    \nstr(dim_prcp_tmin)List of 3\n $ x   :List of 7\n  ..$ from  : num 1\n  ..$ to    : num 20\n  ..$ offset: num -122\n  ..$ delta : num 0.0417\n  ..$ refsys:List of 2\n  .. ..$ input: chr \"GEOGCS[\\\"NAD83\\\",DATUM[\\\"North_American_Datum_1983\\\",SPHEROID[\\\"GRS 1980\\\",6378137,298.2572221010042,AUTHORITY[\"| __truncated__\n  .. ..$ wkt  : chr \"GEOGCS[\\\"NAD83\\\",\\n    DATUM[\\\"North_American_Datum_1983\\\",\\n        SPHEROID[\\\"GRS 1980\\\",6378137,298.25722210\"| __truncated__\n  .. ..- attr(*, \"class\")= chr \"crs\"\n  ..$ point : logi FALSE\n  ..$ values: NULL\n  ..- attr(*, \"class\")= chr \"dimension\"\n $ y   :List of 7\n  ..$ from  : num 1\n  ..$ to    : num 20\n  ..$ offset: num 46.6\n  ..$ delta : num -0.0417\n  ..$ refsys:List of 2\n  .. ..$ input: chr \"GEOGCS[\\\"NAD83\\\",DATUM[\\\"North_American_Datum_1983\\\",SPHEROID[\\\"GRS 1980\\\",6378137,298.2572221010042,AUTHORITY[\"| __truncated__\n  .. ..$ wkt  : chr \"GEOGCS[\\\"NAD83\\\",\\n    DATUM[\\\"North_American_Datum_1983\\\",\\n        SPHEROID[\\\"GRS 1980\\\",6378137,298.25722210\"| __truncated__\n  .. ..- attr(*, \"class\")= chr \"crs\"\n  ..$ point : logi FALSE\n  ..$ values: NULL\n  ..- attr(*, \"class\")= chr \"dimension\"\n $ date:List of 7\n  ..$ from  : num 1\n  ..$ to    : int 10\n  ..$ offset: Date[1:1], format: \"2009-08-11\"\n  ..$ delta : 'difftime' num 1\n  .. ..- attr(*, \"units\")= chr \"days\"\n  ..$ refsys: chr \"Date\"\n  ..$ point : logi NA\n  ..$ values: NULL\n  ..- attr(*, \"class\")= chr \"dimension\"\n - attr(*, \"raster\")=List of 3\n  ..$ affine     : num [1:2] 0 0\n  ..$ dimensions : chr [1:2] \"x\" \"y\"\n  ..$ curvilinear: logi FALSE\n  ..- attr(*, \"class\")= chr \"stars_raster\"\n - attr(*, \"class\")= chr \"dimensions\"\ndim_prcp_tmin$x$offset[1] -121.7292\n#--- get date values ---#\nst_get_dimension_values(prcp_tmax_PRISM_m8_y09, \"date\") [1] \"2009-08-11\" \"2009-08-12\" \"2009-08-13\" \"2009-08-14\" \"2009-08-15\"\n [6] \"2009-08-16\" \"2009-08-17\" \"2009-08-18\" \"2009-08-19\" \"2009-08-20\""},{"path":"stars-basics.html","id":"attributes-to-dimensions-and-vice-versa","chapter":"7 Spatiotemporal Raster Data Handling with stars","heading":"7.2.5 Attributes to dimensions, and vice versa","text":"can make attributes dimensions using merge().can see, new stars object additional dimension called X, represents attributes two dimension values: first ppt second tmax. Now, want access ppt, can following:can merge kept attribute names dimension values can see values.can revert back original state using split(). Since want fourth dimension dissolve,","code":"\n(\nprcp_tmax_four <- merge(prcp_tmax_PRISM_m8_y09)\n)stars object with 4 dimensions and 1 attribute\nattribute(s):\n   Min. 1st Qu. Median     Mean 3rd Qu.   Max.\nX     0       0 11.799 11.66388  21.535 39.707\ndimension(s):\n           from to     offset      delta refsys point     values x/y\nx             1 20   -121.729  0.0416667  NAD83 FALSE       NULL [x]\ny             1 20    46.6458 -0.0416667  NAD83 FALSE       NULL [y]\ndate          1 10 2009-08-11     1 days   Date    NA       NULL    \nattributes    1  2         NA         NA     NA    NA ppt , tmax    \nprcp_tmax_four[, , , , \"ppt\"]stars object with 4 dimensions and 1 attribute\nattribute(s):\n   Min. 1st Qu. Median     Mean 3rd Qu.   Max.\nX     0       0      0 1.292334   0.011 30.851\ndimension(s):\n           from to     offset      delta refsys point values x/y\nx             1 20   -121.729  0.0416667  NAD83 FALSE   NULL [x]\ny             1 20    46.6458 -0.0416667  NAD83 FALSE   NULL [y]\ndate          1 10 2009-08-11     1 days   Date    NA   NULL    \nattributes    1  1         NA         NA     NA    NA    ppt    \nsplit(prcp_tmax_four, 4)stars object with 3 dimensions and 2 attributes\nattribute(s):\n       Min.  1st Qu. Median      Mean  3rd Qu.   Max.\nppt   0.000  0.00000  0.000  1.292334  0.01100 30.851\ntmax  1.833 17.55575 21.483 22.035435 26.54275 39.707\ndimension(s):\n     from to     offset      delta refsys point values x/y\nx       1 20   -121.729  0.0416667  NAD83 FALSE   NULL [x]\ny       1 20    46.6458 -0.0416667  NAD83 FALSE   NULL [y]\ndate    1 10 2009-08-11     1 days   Date    NA   NULL    "},{"path":"stars-basics.html","id":"quick-visualization-for-exploration","chapter":"7 Spatiotemporal Raster Data Handling with stars","heading":"7.3 Quick visualization for exploration","text":"can use plot() quick static map mapview() tmap package interactive views.","code":""},{"path":"stars-basics.html","id":"quick-static-map","chapter":"7 Spatiotemporal Raster Data Handling with stars","heading":"7.3.1 quick static map","text":"plots first attribute stars object multiple attributes:, identical :","code":"\nplot(prcp_tmax_PRISM_m8_y09[\"tmax\",,,])\nplot(prcp_tmax_PRISM_m8_y09)\nplot(prcp_tmax_PRISM_m8_y09[\"ppt\",,,])"},{"path":"stars-basics.html","id":"interactive-map","chapter":"7 Spatiotemporal Raster Data Handling with stars","heading":"7.3.2 interactive map","text":"can use tmap package. can apply tmap_leaflet() static tmap object make interactive map. tm_facets(.layers = TRUE) option stacks layers single map.","code":"\n#--- make it interactive ---#\ntmap_leaflet(\n  tm_shape(prcp_tmax_PRISM_m8_y09[\"tmax\",,,]) + \n  tm_raster() + \n  tm_facets(as.layers = TRUE)\n)"},{"path":"stars-basics.html","id":"read-write-stars","chapter":"7 Spatiotemporal Raster Data Handling with stars","heading":"7.4 Reading and writing raster data","text":"many formats raster data stored. common ones include GeoTIFF, netCDF, GRIB. available GDAL drivers reading writing raster data can found following code:81The output function put table .","code":"\nsf::st_drivers(what = \"raster\") "},{"path":"stars-basics.html","id":"reading-raster-data","chapter":"7 Spatiotemporal Raster Data Handling with stars","heading":"7.4.1 Reading raster data","text":"can use read_stars() read raster data file. unlikely raster file trying read one supported formats.example, can read GeoTIFF file follows:one imports raw PRISM data stored BIL file.can import multiple raster data files one stars object simply supplying vector file names:can see, file becomes attribute.82 convenient layers stacked along third dimension. can add along = 3 option follows:Note GeoTIFF format (many formats) can store multi-band (multi-layer) raster data allowing additional dimension beyond x y, store values dimension like date dimension saw prcp_tmax_PRISM_m8_y09. , read multi-layer raster data saved GeoTIFF, third dimension resulting stars object always called band without explicit time information. hand, netCDF files capable storing time dimension values. , read netCDF file valid time dimension, time dimension read.refsys time dimension POSIXct, one date classes.","code":"\n(\nppt_m1_y09_stars <- read_stars(\"Data/PRISM_ppt_y2009_m1.tif\") \n)stars object with 3 dimensions and 1 attribute\nattribute(s), summary of first 1e+05 cells:\n                        Min. 1st Qu. Median     Mean 3rd Qu.   Max.  NA's\nPRISM_ppt_y2009_m1.tif     0       0  0.436 3.543222  3.4925 56.208 60401\ndimension(s):\n     from   to   offset      delta refsys point\nx       1 1405 -125.021  0.0416667  NAD83 FALSE\ny       1  621  49.9375 -0.0416667  NAD83 FALSE\nband    1   31       NA         NA     NA    NA\n                                                                          values\nx                                                                           NULL\ny                                                                           NULL\nband PRISM_ppt_stable_4kmD2_20090101_bil,...,PRISM_ppt_stable_4kmD2_20090131_bil\n     x/y\nx    [x]\ny    [y]\nband    \n(\nprism_tmax_20180701 <- read_stars(\"Data/PRISM_tmax_stable_4kmD2_20180701_bil/PRISM_tmax_stable_4kmD2_20180701_bil.bil\")\n)stars object with 2 dimensions and 1 attribute\nattribute(s):\n                                    Min. 1st Qu. Median     Mean 3rd Qu.   Max.\nPRISM_tmax_stable_4kmD2_201807...  3.623  25.318 31.596 29.50749  33.735 48.008\n                                     NA's\nPRISM_tmax_stable_4kmD2_201807...  390874\ndimension(s):\n  from   to   offset      delta refsys point values x/y\nx    1 1405 -125.021  0.0416667  NAD83    NA   NULL [x]\ny    1  621  49.9375 -0.0416667  NAD83    NA   NULL [y]\nfiles <- c(\"Data/PRISM_tmax_stable_4kmD2_20180701_bil/PRISM_tmax_stable_4kmD2_20180701_bil.bil\", \"Data/PRISM_tmax_stable_4kmD2_20180702_bil/PRISM_tmax_stable_4kmD2_20180702_bil.bil\")\n\n(\nprism_tmax_201807_two <- read_stars(files)\n)stars object with 2 dimensions and 2 attributes\nattribute(s):\n                                    Min. 1st Qu. Median     Mean 3rd Qu.   Max.\nPRISM_tmax_stable_4kmD2_201807...  3.623  25.318 31.596 29.50749  33.735 48.008\nPRISM_tmax_stable_4kmD2_201807...  0.808  26.505 30.632 29.93235  33.632 47.999\n                                     NA's\nPRISM_tmax_stable_4kmD2_201807...  390874\nPRISM_tmax_stable_4kmD2_201807...  390874\ndimension(s):\n  from   to   offset      delta refsys point values x/y\nx    1 1405 -125.021  0.0416667  NAD83    NA   NULL [x]\ny    1  621  49.9375 -0.0416667  NAD83    NA   NULL [y]\n(\nprism_tmax_201807_two <- read_stars(files, along = 3)\n)stars object with 3 dimensions and 1 attribute\nattribute(s), summary of first 1e+05 cells:\n                                    Min. 1st Qu. Median     Mean 3rd Qu.   Max.\nPRISM_tmax_stable_4kmD2_201807...  4.693 19.5925 23.653 22.05224 24.5945 33.396\n                                    NA's\nPRISM_tmax_stable_4kmD2_201807...  60401\ndimension(s):\n        from   to   offset      delta refsys point values x/y\nx          1 1405 -125.021  0.0416667  NAD83    NA   NULL [x]\ny          1  621  49.9375 -0.0416667  NAD83    NA   NULL [y]\nnew_dim    1    2       NA         NA     NA    NA   NULL    \n(\nread_ncdf(system.file(\"nc/bcsd_obs_1999.nc\", package = \"stars\"))  \n)stars object with 3 dimensions and 2 attributes\nattribute(s):\n                Min.   1st Qu.   Median      Mean   3rd Qu.      Max. NA's\npr [mm/m]  0.5900000 56.139999 81.88000 101.26433 121.07250 848.54999 7116\ntas [C]   -0.4209678  8.898887 15.65763  15.48932  21.77979  29.38581 7116\ndimension(s):\n          from to offset delta  refsys point                    values x/y\nlongitude    1 81    -85 0.125  WGS 84    NA                      NULL [x]\nlatitude     1 33     33 0.125  WGS 84    NA                      NULL [y]\ntime         1 12     NA    NA POSIXct    NA 1999-01-31,...,1999-12-31    "},{"path":"stars-basics.html","id":"writing-a-stars-object-to-a-file","chapter":"7 Spatiotemporal Raster Data Handling with stars","heading":"7.4.2 Writing a stars object to a file","text":"can write stars object file using write_stars() using one GDAL drivers.Let’s save prcp_tmax_PRISM_m8_y09[\"tmax\",,,], date dimension whose refsys Date.Let’s read file just saved.Notice third dimension now called band date information lost. loss information happened saved prcp_tmax_PRISM_m8_y09[\"tmax\",,,] GeoTIFF file. One easy way avoid problem just save stars object R dataset.can see, date information retained. , one uses data team members use R, nice solution problem.83 moment, possible use write_stars() write netCDF file supports third dimension time. However, may case long time (See discussion ).","code":"\nwrite_stars(prcp_tmax_PRISM_m8_y09[\"tmax\",,,], \"Data/tmax_m8_y09_from_stars.tif\")\nread_stars(\"Data/tmax_m8_y09_from_stars.tif\")stars object with 3 dimensions and 1 attribute\nattribute(s):\n                             Min.  1st Qu. Median     Mean  3rd Qu.   Max.\ntmax_m8_y09_from_stars.tif  1.833 17.55575 21.483 22.03543 26.54275 39.707\ndimension(s):\n     from to   offset      delta refsys point values x/y\nx       1 20 -121.729  0.0416667  NAD83 FALSE   NULL [x]\ny       1 20  46.6458 -0.0416667  NAD83 FALSE   NULL [y]\nband    1 10       NA         NA     NA    NA   NULL    \n#--- save it as an rds file ---#\nsaveRDS(prcp_tmax_PRISM_m8_y09[\"tmax\",,,], \"Data/tmax_m8_y09_from_stars.rds\")\n#--- read it back ---#\nreadRDS(\"Data/tmax_m8_y09_from_stars.rds\") stars object with 3 dimensions and 1 attribute\nattribute(s):\n       Min.  1st Qu. Median     Mean  3rd Qu.   Max.\ntmax  1.833 17.55575 21.483 22.03543 26.54275 39.707\ndimension(s):\n     from to     offset      delta refsys point values x/y\nx       1 20   -121.729  0.0416667  NAD83 FALSE   NULL [x]\ny       1 20    46.6458 -0.0416667  NAD83 FALSE   NULL [y]\ndate    1 10 2009-08-11     1 days   Date    NA   NULL    "},{"path":"stars-basics.html","id":"stars-set-time","chapter":"7 Spatiotemporal Raster Data Handling with stars","heading":"7.5 Setting the time dimension manually","text":"section, use PRISM precipitation data U.S. January, 2009.can see, read GeoTIFF file, third dimension always called band GeoTIFF format support dimension values third dimension.84You can use st_set_dimension() set third dimension (called band) time dimension using Date object. can convenient like filter data date using filter() see later.ppt_m1_y09_stars, precipitation observed daily basis January 1, 2009 January 31, 2009, band value x corresponds January x, 2009. , can first create vector dates follows (familiar Dates lubridate pacakge, good resource learn .):can use st_set_dimensions() change third dimension dimension date.can see, third dimension become date. value offset date dimension become 2009-01-01 meaning starting date value now 2009-01-01. , value delta now 1 days, date dimension x corresponds 2009-01-01 + x - 1 = 2009-01-x.Note date dimension regularly spaced. example, may satellite images available area 5-day interval sometimes 6-day interval times. perfectly fine. illustration, create wrong sequence dates data 2-day gap middle assign date dimension see happens.Now assign date values ppt_m1_y09_stars:Since step date values longer \\(1\\) day entire sequence, value delta now NA. However, notice value date longer NULL. Since date regular, can represent date using three values (, , delta) , date values observation stored now.Finally, note just applying st_set_dimensions() stars object change dimension stars object (just like setNames() discussed ).can see, date dimension altered. need assign results st_set_dimensions() stars object see changes dimension reflected just like right date values.","code":"\n(\nppt_m1_y09_stars <- read_stars(\"Data/PRISM/PRISM_ppt_y2009_m1.tif\") \n) stars object with 3 dimensions and 1 attribute\nattribute(s), summary of first 1e+05 cells:\n                        Min. 1st Qu. Median     Mean 3rd Qu.   Max.  NA's\nPRISM_ppt_y2009_m1.tif     0       0  0.436 3.543222  3.4925 56.208 60401\ndimension(s):\n     from   to   offset      delta refsys point\nx       1 1405 -125.021  0.0416667  NAD83 FALSE\ny       1  621  49.9375 -0.0416667  NAD83 FALSE\nband    1   31       NA         NA     NA    NA\n                                                                          values\nx                                                                           NULL\ny                                                                           NULL\nband PRISM_ppt_stable_4kmD2_20090101_bil,...,PRISM_ppt_stable_4kmD2_20090131_bil\n     x/y\nx    [x]\ny    [y]\nband    \n#--- starting date ---#\nstart_date <- ymd(\"2009-01-01\")\n\n#--- ending date ---#\nend_date <- ymd(\"2009-01-31\")\n\n#--- sequence of dates  ---#\ndates_ls_m1 <- seq(start_date, end_date, \"days\")#--- syntax (NOT RUN) ---#  \nst_set_dimensions(stars object, dimension, values = dimension values, names = name of the dimension)\n(\nppt_m1_y09_stars <- st_set_dimensions(ppt_m1_y09_stars, 3, values = dates_ls_m1, names = \"date\")\n)stars object with 3 dimensions and 1 attribute\nattribute(s), summary of first 1e+05 cells:\n                        Min. 1st Qu. Median     Mean 3rd Qu.   Max.  NA's\nPRISM_ppt_y2009_m1.tif     0       0  0.436 3.543222  3.4925 56.208 60401\ndimension(s):\n     from   to     offset      delta refsys point values x/y\nx       1 1405   -125.021  0.0416667  NAD83 FALSE   NULL [x]\ny       1  621    49.9375 -0.0416667  NAD83 FALSE   NULL [y]\ndate    1   31 2009-01-01     1 days   Date    NA   NULL    \n#--- 2009-01-23 removed and 2009-02-01 added ---#\n(\ndates_ls_wrong <- c(seq(start_date, end_date, \"days\")[-23], ymd(\"2009-02-01\")) \n) [1] \"2009-01-01\" \"2009-01-02\" \"2009-01-03\" \"2009-01-04\" \"2009-01-05\"\n [6] \"2009-01-06\" \"2009-01-07\" \"2009-01-08\" \"2009-01-09\" \"2009-01-10\"\n[11] \"2009-01-11\" \"2009-01-12\" \"2009-01-13\" \"2009-01-14\" \"2009-01-15\"\n[16] \"2009-01-16\" \"2009-01-17\" \"2009-01-18\" \"2009-01-19\" \"2009-01-20\"\n[21] \"2009-01-21\" \"2009-01-22\" \"2009-01-24\" \"2009-01-25\" \"2009-01-26\"\n[26] \"2009-01-27\" \"2009-01-28\" \"2009-01-29\" \"2009-01-30\" \"2009-01-31\"\n[31] \"2009-02-01\"\n#--- set date values ---#\n(\nppt_m1_y09_stars_wrong <- st_set_dimensions(ppt_m1_y09_stars, 3, values = dates_ls_wrong, names = \"date\")\n)stars object with 3 dimensions and 1 attribute\nattribute(s), summary of first 1e+05 cells:\n                        Min. 1st Qu. Median     Mean 3rd Qu.   Max.  NA's\nPRISM_ppt_y2009_m1.tif     0       0  0.436 3.543222  3.4925 56.208 60401\ndimension(s):\n     from   to   offset      delta refsys point                    values x/y\nx       1 1405 -125.021  0.0416667  NAD83 FALSE                      NULL [x]\ny       1  621  49.9375 -0.0416667  NAD83 FALSE                      NULL [y]\ndate    1   31       NA         NA   Date    NA 2009-01-01,...,2009-02-01    \nppt_m1_y09_starsstars object with 3 dimensions and 1 attribute\nattribute(s), summary of first 1e+05 cells:\n                        Min. 1st Qu. Median     Mean 3rd Qu.   Max.  NA's\nPRISM_ppt_y2009_m1.tif     0       0  0.436 3.543222  3.4925 56.208 60401\ndimension(s):\n     from   to     offset      delta refsys point values x/y\nx       1 1405   -125.021  0.0416667  NAD83 FALSE   NULL [x]\ny       1  621    49.9375 -0.0416667  NAD83 FALSE   NULL [y]\ndate    1   31 2009-01-01     1 days   Date    NA   NULL    "},{"path":"stars-basics.html","id":"dplyr-op","chapter":"7 Spatiotemporal Raster Data Handling with stars","heading":"7.6 dplyr-like operations","text":"can use dplyr language basic data operations stars objects.","code":""},{"path":"stars-basics.html","id":"filter","chapter":"7 Spatiotemporal Raster Data Handling with stars","heading":"7.6.1 filter()","text":"filter() function allows subset data dimension values: x, y, band (date).spatial filteringtemporal filteringFinally, since date dimension Date, can use Date math filter data.85filter attribute?Just case wondering. filter attribute.","code":"\nlibrary(tidyverse)\n\n#--- longitude greater than -100 ---#\nfilter(ppt_m1_y09_stars, x > -100) %>% plot()\n#--- latitude less than 40 ---#\nfilter(ppt_m1_y09_stars, y < 40) %>% plot()\n#--- dates after 2009-01-15  ---#\nfilter(ppt_m1_y09_stars, date > ymd(\"2009-01-21\")) %>% plot()\nfilter(ppt_m1_y09_stars, ppt > 20) Error in `glubort()`:\n! ``~``, `ppt > 20` must refer to exactly one dimension, not ``"},{"path":"stars-basics.html","id":"select","chapter":"7 Spatiotemporal Raster Data Handling with stars","heading":"7.6.2 select()","text":"select() function lets pick certain attributes.","code":"\nselect(prcp_tmax_PRISM_m8_y09, ppt)stars object with 3 dimensions and 1 attribute\nattribute(s):\n     Min. 1st Qu. Median     Mean 3rd Qu.   Max.\nppt     0       0      0 1.292334   0.011 30.851\ndimension(s):\n     from to     offset      delta refsys point values x/y\nx       1 20   -121.729  0.0416667  NAD83 FALSE   NULL [x]\ny       1 20    46.6458 -0.0416667  NAD83 FALSE   NULL [y]\ndate    1 10 2009-08-11     1 days   Date    NA   NULL    "},{"path":"stars-basics.html","id":"mutate","chapter":"7 Spatiotemporal Raster Data Handling with stars","heading":"7.6.3 mutate()","text":"can mutate attributes using mutate() function. example, can useful calculate NDVI stars object Red NIR (spectral reflectance measurements red near-infrared regions) attributes. , just simply convert unit precipitation mm inches.","code":"\n#--- mm to inches ---#\nmutate(prcp_tmax_PRISM_m8_y09, ppt = ppt * 0.0393701)stars object with 3 dimensions and 2 attributes\nattribute(s):\n       Min.  1st Qu. Median       Mean      3rd Qu.      Max.\nppt   0.000  0.00000  0.000  0.0508793 4.330711e-04  1.214607\ntmax  1.833 17.55575 21.483 22.0354348 2.654275e+01 39.707001\ndimension(s):\n     from to     offset      delta refsys point values x/y\nx       1 20   -121.729  0.0416667  NAD83 FALSE   NULL [x]\ny       1 20    46.6458 -0.0416667  NAD83 FALSE   NULL [y]\ndate    1 10 2009-08-11     1 days   Date    NA   NULL    "},{"path":"stars-basics.html","id":"pull","chapter":"7 Spatiotemporal Raster Data Handling with stars","heading":"7.6.4 pull()","text":"can extract attribute values using pull().","code":"\n#--- tmax values of the 1st date layer ---#\npull(prcp_tmax_PRISM_m8_y09[\"tmax\",,,1], \"tmax\"), , 1\n\n        [,1]   [,2]   [,3]   [,4]   [,5]   [,6]   [,7]   [,8]   [,9]  [,10]\n [1,] 25.495 27.150 22.281 19.553 21.502 19.616 21.664 24.458 21.712 19.326\n [2,] 27.261 27.042 21.876 19.558 19.141 19.992 18.694 24.310 21.094 20.032\n [3,] 27.568 21.452 23.220 19.428 18.974 19.366 20.293 24.418 20.190 21.127\n [4,] 23.310 20.357 20.111 21.380 19.824 19.638 22.135 20.885 20.121 18.396\n [5,] 19.571 20.025 18.407 19.142 19.314 22.759 22.652 19.566 18.817 16.065\n [6,] 17.963 16.581 17.121 16.716 19.680 20.521 17.991 18.007 17.430 14.586\n [7,] 20.911 19.202 16.309 14.496 17.429 19.083 18.509 18.723 17.645 16.415\n [8,] 17.665 16.730 17.691 14.370 16.849 18.413 17.787 20.000 19.319 18.401\n [9,] 16.795 18.091 20.460 16.405 18.331 19.005 19.142 21.226 21.184 19.520\n[10,] 19.208 19.624 17.210 19.499 18.492 20.854 18.684 19.811 22.058 19.923\n[11,] 23.148 18.339 19.676 20.674 18.545 21.126 19.013 19.722 21.843 21.271\n[12,] 23.254 21.279 21.921 19.894 19.445 21.499 19.765 20.742 21.560 22.989\n[13,] 23.450 21.956 19.813 18.970 20.173 20.567 21.152 20.932 19.836 20.347\n[14,] 24.075 21.120 20.166 19.177 20.428 20.908 21.060 19.832 19.764 19.981\n[15,] 24.318 20.943 20.024 20.022 19.040 19.773 20.452 20.152 20.321 20.304\n[16,] 22.538 19.461 20.100 21.149 19.958 20.486 20.535 20.445 21.564 21.493\n[17,] 20.827 20.192 21.165 22.369 21.488 22.031 21.552 21.089 21.687 23.375\n[18,] 21.089 21.451 22.692 21.793 22.160 23.049 22.562 22.738 23.634 24.697\n[19,] 22.285 22.992 23.738 23.497 24.255 25.177 25.411 24.324 24.588 26.032\n[20,] 23.478 23.584 24.589 24.719 26.114 26.777 27.310 26.643 26.516 26.615\n       [,11]  [,12]  [,13]  [,14]  [,15]  [,16]  [,17]  [,18]  [,19]  [,20]\n [1,] 24.845 23.211 21.621 23.180 21.618 21.322 22.956 23.749 23.162 25.746\n [2,] 24.212 21.820 21.305 22.960 22.806 22.235 23.604 23.689 25.597 25.933\n [3,] 20.879 20.951 22.280 23.536 23.455 22.537 24.760 23.587 26.170 24.764\n [4,] 17.435 18.681 22.224 24.122 25.828 25.604 25.298 22.817 24.438 24.835\n [5,] 14.186 17.789 20.624 23.416 26.059 27.571 25.158 24.201 26.001 26.235\n [6,] 10.188 15.632 19.907 22.660 25.268 27.469 27.376 27.488 27.278 27.558\n [7,] 14.797 15.933 19.204 21.641 23.107 25.626 26.990 25.838 26.906 27.247\n [8,] 17.325 18.299 19.691 21.553 21.840 23.754 26.099 25.270 26.282 26.981\n [9,] 19.322 19.855 20.489 22.597 23.614 25.873 26.906 26.368 26.332 25.844\n[10,] 20.241 21.800 22.111 24.128 25.765 27.105 27.200 25.491 26.306 25.663\n[11,] 23.398 24.090 24.884 25.596 26.545 27.014 26.464 25.708 25.742 25.336\n[12,] 22.576 21.996 23.874 26.447 26.955 26.871 25.533 25.576 25.610 25.902\n[13,] 22.023 22.358 24.996 26.185 27.249 25.617 25.623 25.600 25.433 26.681\n[14,] 20.974 23.533 25.388 25.975 27.316 26.199 26.090 25.920 25.767 27.956\n[15,] 20.982 23.632 24.703 25.539 26.515 27.133 27.407 27.518 27.149 28.506\n[16,] 22.500 24.012 25.282 25.751 25.212 25.290 26.058 28.258 28.290 29.842\n[17,] 23.024 24.381 25.157 25.259 24.829 24.183 25.632 26.947 28.601 29.589\n[18,] 24.016 24.425 24.965 24.930 24.482 23.274 25.412 26.733 28.494 29.656\n[19,] 24.065 24.105 24.145 24.318 23.912 22.782 25.039 26.554 28.184 29.012\n[20,] 23.979 24.321 23.477 22.135 22.395 22.189 24.944 26.542 27.923 28.849"},{"path":"stars-basics.html","id":"merging-stars-objects-using-c-and-st_mosaic","chapter":"7 Spatiotemporal Raster Data Handling with stars","heading":"7.7 Merging stars objects using c() and st_mosaic()","text":"","code":""},{"path":"stars-basics.html","id":"merging-stars-objects-along-the-third-dimension-band","chapter":"7 Spatiotemporal Raster Data Handling with stars","heading":"7.7.1 Merging stars objects along the third dimension (band)","text":"learn merge multiple stars objects havethe attributesthe spatial extent resolutiondifferent bands (dates )example, consider merging PRISM precipitation data January February. exactly spatial extent resolutions represent attribute (precipitation). However, differ third dimension (date). , trying stack data attributes along third dimension (date) making sure spatial correspondence maintained. merge kind like rbind() stacks multiple data.frames vertically making sure variables aligned correctly.Let’s import PRISM precipitation data February, 2009.Note third dimension ppt_m2_y09_stars changed date.Now, let’s try merge two:noticed, second object (ppt_m2_y09_stars) assumed date characteristics first one: data February observed daily (delta 1 day). causes problem instance February data indeed observed daily starting 2009-02-01. However, careful appending data start 1 day (generally delta time dimension) first data data follow observation interval.reason, advisable first set date values set. Pretend February data actually starts 2009-02-02 2009-03-01 see happens regular interval (delta) kept merging.merge two,date dimension delta , correctly one-day gap end date first stars object (“2009-01-31”) start date second stars object (“2009-02-02”). , date values now stored values.","code":"\n#--- read the February ppt data ---#        \n(\nppt_m2_y09_stars <- read_stars(\"Data/PRISM_ppt_y2009_m2.tif\") \n)stars object with 3 dimensions and 1 attribute\nattribute(s), summary of first 1e+05 cells:\n                        Min. 1st Qu. Median      Mean 3rd Qu.   Max.  NA's\nPRISM_ppt_y2009_m2.tif     0       0      0 0.1855858       0 11.634 60401\ndimension(s):\n     from   to   offset      delta refsys point\nx       1 1405 -125.021  0.0416667  NAD83 FALSE\ny       1  621  49.9375 -0.0416667  NAD83 FALSE\nband    1   28       NA         NA     NA    NA\n                                                                          values\nx                                                                           NULL\ny                                                                           NULL\nband PRISM_ppt_stable_4kmD2_20090201_bil,...,PRISM_ppt_stable_4kmD2_20090228_bil\n     x/y\nx    [x]\ny    [y]\nband    \n#--- combine the two ---#\n(\nppt_m1_to_m2_y09_stars <- c(ppt_m1_y09_stars, ppt_m2_y09_stars, along = 3)\n)stars object with 3 dimensions and 1 attribute\nattribute(s), summary of first 1e+05 cells:\n                        Min. 1st Qu. Median     Mean 3rd Qu.   Max.  NA's\nPRISM_ppt_y2009_m1.tif     0       0  0.436 3.543222  3.4925 56.208 60401\ndimension(s):\n     from   to     offset      delta refsys point values x/y\nx       1 1405   -125.021  0.0416667  NAD83 FALSE   NULL [x]\ny       1  621    49.9375 -0.0416667  NAD83 FALSE   NULL [y]\ndate    1   59 2009-01-01     1 days   Date    NA   NULL    \n#--- starting date ---#\nstart_date <- ymd(\"2009-02-01\")\n\n#--- ending date ---#\nend_date <- ymd(\"2009-02-28\")\n\n#--- sequence of dates  ---#\ndates_ls <- seq(start_date, end_date, \"days\")       \n\n#--- pretend the data actually starts from `2009-02-02` to `2009-03-01` ---#\n(\nppt_m2_y09_stars <- st_set_dimensions(ppt_m2_y09_stars, 3, values = c(dates_ls[-1], ymd(\"2009-03-01\")), name = \"date\")\n)stars object with 3 dimensions and 1 attribute\nattribute(s), summary of first 1e+05 cells:\n                        Min. 1st Qu. Median      Mean 3rd Qu.   Max.  NA's\nPRISM_ppt_y2009_m2.tif     0       0      0 0.1855858       0 11.634 60401\ndimension(s):\n     from   to     offset      delta refsys point values x/y\nx       1 1405   -125.021  0.0416667  NAD83 FALSE   NULL [x]\ny       1  621    49.9375 -0.0416667  NAD83 FALSE   NULL [y]\ndate    1   28 2009-02-02     1 days   Date    NA   NULL    \nc(ppt_m1_y09_stars, ppt_m2_y09_stars, along = 3)stars object with 3 dimensions and 1 attribute\nattribute(s), summary of first 1e+05 cells:\n                        Min. 1st Qu. Median     Mean 3rd Qu.   Max.  NA's\nPRISM_ppt_y2009_m1.tif     0       0  0.436 3.543222  3.4925 56.208 60401\ndimension(s):\n     from   to   offset      delta refsys point                    values x/y\nx       1 1405 -125.021  0.0416667  NAD83 FALSE                      NULL [x]\ny       1  621  49.9375 -0.0416667  NAD83 FALSE                      NULL [y]\ndate    1   59       NA         NA   Date    NA 2009-01-01,...,2009-03-01    "},{"path":"stars-basics.html","id":"merge-two","chapter":"7 Spatiotemporal Raster Data Handling with stars","heading":"7.7.2 Merging stars objects of different attributes","text":"learn merge multiple stars objects havedifferent attributesthe spatial extent resolutionthe bands (dates )example, consider merging PRISM precipitation tmax data January. exactly spatial extent resolutions date characteristics (starting ending dates time interval). However, differ represent: precipitation tmax. merge kind like cbind() combines multiple data.frames different variables making sure observation correspondence correct.Let’s read daily tmax data January, 2009:Now, let’s merge PRISM ppt tmax data January, 2009.Oops. Well, problem third dimension two objects . Even though know xth element third dimension represent thing, look different R’s eyes. , first need change third dimension tmax_m1_y09_stars consistent third dimension ppt_m1_y09_stars (dates_ls_m1 defined Chapter 7.5).Now, can merge two.can see, now another attribute called tmax.","code":"\n(\ntmax_m1_y09_stars <- read_stars(\"Data/PRISM_tmax_y2009_m1.tif\") %>% \n    #--- change the attribute name ---#\n    setNames(\"tmax\")\n)   stars object with 3 dimensions and 1 attribute\nattribute(s), summary of first 1e+05 cells:\n         Min. 1st Qu. Median      Mean 3rd Qu.  Max.  NA's\ntmax  -17.898  -7.761 -2.248 -3.062721   1.718 7.994 60401\ndimension(s):\n     from   to   offset      delta refsys point\nx       1 1405 -125.021  0.0416667  NAD83 FALSE\ny       1  621  49.9375 -0.0416667  NAD83 FALSE\nband    1   31       NA         NA     NA    NA\n                                                                            values\nx                                                                             NULL\ny                                                                             NULL\nband PRISM_tmax_stable_4kmD2_20090101_bil,...,PRISM_tmax_stable_4kmD2_20090131_bil\n     x/y\nx    [x]\ny    [y]\nband    \nc(ppt_m1_y09_stars, tmax_m1_y09_stars)Error in c.stars(ppt_m1_y09_stars, tmax_m1_y09_stars): don't know how to merge arrays: please specify parameter along\n(\ntmax_m1_y09_stars <- st_set_dimensions(tmax_m1_y09_stars, 3, values = dates_ls_m1, names = \"date\")\n)   stars object with 3 dimensions and 1 attribute\nattribute(s), summary of first 1e+05 cells:\n         Min. 1st Qu. Median      Mean 3rd Qu.  Max.  NA's\ntmax  -17.898  -7.761 -2.248 -3.062721   1.718 7.994 60401\ndimension(s):\n     from   to     offset      delta refsys point values x/y\nx       1 1405   -125.021  0.0416667  NAD83 FALSE   NULL [x]\ny       1  621    49.9375 -0.0416667  NAD83 FALSE   NULL [y]\ndate    1   31 2009-01-01     1 days   Date    NA   NULL    \n(\nppt_tmax_m1_y09_stars <- c(ppt_m1_y09_stars, tmax_m1_y09_stars)\n)stars object with 3 dimensions and 2 attributes\nattribute(s), summary of first 1e+05 cells:\n                           Min. 1st Qu. Median      Mean 3rd Qu.   Max.  NA's\nPRISM_ppt_y2009_m1.tif    0.000   0.000  0.436  3.543222  3.4925 56.208 60401\ntmax                    -17.898  -7.761 -2.248 -3.062721  1.7180  7.994 60401\ndimension(s):\n     from   to     offset      delta refsys point values x/y\nx       1 1405   -125.021  0.0416667  NAD83 FALSE   NULL [x]\ny       1  621    49.9375 -0.0416667  NAD83 FALSE   NULL [y]\ndate    1   31 2009-01-01     1 days   Date    NA   NULL    "},{"path":"stars-basics.html","id":"merging-stars-objects-of-different-spatial-extents","chapter":"7 Spatiotemporal Raster Data Handling with stars","heading":"7.7.3 Merging stars objects of different spatial extents","text":"learn merge multiple stars objects havethe attributesdifferent spatial extent resolution86the bands (dates )times multiple separate raster datasets different spatial coverages like combine one. can using st_mosaic().Let’s split tmax_m1_y09_stars two parts:(Figure 7.3) look like (Jan 1, 1980):\nFigure 7.3: Two spatially non-overlapping stars objects\nLet’s combine two using st_mosaic():combined object looks like (Figure 7.4)\nFigure 7.4: Map stars objects combined one\nokay two stars objects combined spatial overlap. following split creates two stars objects spatial overlap.(Figure 7.5) look like (Jan 1, 1980):\nFigure 7.5: Two spatially overlapping stars objects\ncan see , st_mosaic() reconciles spatial overlap two stars objects.plot (Figure 7.6):\nFigure 7.6: Map spatially-overlapping stars objects combined one\n","code":"\ntmax_1 <- filter(tmax_m1_y09_stars, x <= -100) \ntmax_2 <- filter(tmax_m1_y09_stars, x > -100)\ng_1 <- ggplot() +\n  geom_stars(data = tmax_1[,,,1]) +\n  theme_for_map +\n  theme(\n    legend.position = \"bottom\"\n  )\n\ng_2 <- ggplot() +\n  geom_stars(data = tmax_2[,,,1]) +\n  theme_for_map +\n  theme(\n    legend.position = \"bottom\"\n  )\n\nlibrary(patchwork)\ng_1 + g_2\ntmax_combined <- st_mosaic(tmax_1, tmax_2) \nggplot() +\n  geom_stars(data = tmax_combined[,,,1]) +\n  theme_for_map \ntmax_1 <- filter(tmax_m1_y09_stars, x <= -100) \ntmax_2 <- filter(tmax_m1_y09_stars, x > -110) \ng_1 <- ggplot() +\n  geom_stars(data = tmax_m1_y09_stars[,,,1], fill = NA) +\n  geom_stars(data = tmax_1[,,,1]) +\n  theme_for_map\n\ng_2 <- ggplot() +\n  geom_stars(data = tmax_m1_y09_stars[,,,1], fill = NA) +\n  geom_stars(data = tmax_2[,,,1]) +\n  theme_for_map\n\nlibrary(patchwork)\ng_1 / g_2\n(\ntmax_combined <- st_mosaic(tmax_1, tmax_2) \n)stars object with 3 dimensions and 1 attribute\nattribute(s), summary of first 1e+05 cells:\n         Min. 1st Qu. Median      Mean 3rd Qu.  Max.  NA's\ntmax  -17.898  -7.761 -2.248 -3.062721   1.718 7.994 60401\ndimension(s):\n     from   to   offset      delta refsys point values x/y\nx       1 1405 -125.021  0.0416667  NAD83    NA   NULL [x]\ny       1  621  49.9375 -0.0416667  NAD83    NA   NULL [y]\nband    1   31       NA         NA     NA    NA   NULL    \nggplot() +\n  geom_stars(data = tmax_combined[,,,1]) +\n  theme_for_map"},{"path":"stars-basics.html","id":"convert-to-rb","chapter":"7 Spatiotemporal Raster Data Handling with stars","heading":"7.8 Convert from and to Raster\\(^*\\) or SpatRaster objects","text":"need convert stars object Raster\\(^*\\) SpatRaster object, can use () function follows:can see, date values prcp_tmax_PRISM_m8_y09 appear time dimension prcp_tmax_PRISM_m8_y09_rb (RasterBrick). However, prcp_tmax_PRISM_m8_y09_sr (SpatRaster), date values appear names dimension date added prefix original date values.Note also conversion done ppt attribute. simply raster terrapackage accommodate multiple attributes 3-dimensional array. , want RasterBrick SpatRaster tmax data, need following:can convert Raster\\(^*\\) object stars object using st_as_stars() (see use case Chapter 9.4.2).Notice original date values (stored date dimension) recovered, dimension now called just band.can SpatRaster object.Note unlike conversion RasterBrick object, band dimension inherited values name dimension tmax_PRISM_m8_y09_sr.","code":"\n(\n# to Raster* object\nprcp_tmax_PRISM_m8_y09_rb <- as(prcp_tmax_PRISM_m8_y09, \"Raster\")\n)class      : RasterBrick \ndimensions : 20, 20, 400, 10  (nrow, ncol, ncell, nlayers)\nresolution : 0.04166667, 0.04166667  (x, y)\nextent     : -121.7292, -120.8958, 45.8125, 46.64583  (xmin, xmax, ymin, ymax)\ncrs        : +proj=longlat +datum=NAD83 +no_defs \nsource     : memory\nnames      : layer.1, layer.2, layer.3, layer.4, layer.5, layer.6, layer.7, layer.8, layer.9, layer.10 \nmin values :       0,       0,       0,       0,       0,       0,       0,       0,       0,        0 \nmax values :   0.019,  18.888,  30.851,   4.287,   0.797,   0.003,   3.698,   1.801,   0.000,    0.000 \ntime       : 2009-08-11, 2009-08-12, 2009-08-13, 2009-08-14, 2009-08-15, 2009-08-16, 2009-08-17, 2009-08-18, 2009-08-19, 2009-08-20 \n(\n# to SpatRaster\nprcp_tmax_PRISM_m8_y09_sr <- as(prcp_tmax_PRISM_m8_y09, \"SpatRaster\")\n)class       : SpatRaster \ndimensions  : 20, 20, 10  (nrow, ncol, nlyr)\nresolution  : 0.04166667, 0.04166667  (x, y)\nextent      : -121.7292, -120.8958, 45.8125, 46.64583  (xmin, xmax, ymin, ymax)\ncoord. ref. : lon/lat NAD83 (EPSG:4269) \nsource      : memory \nnames       : date2~08-11, date2~08-12, date2~08-13, date2~08-14, date2~08-15, date2~08-16, ... \nmin values  :           0,           0,           0,           0,           0,           0, ... \nmax values  :       0.019,      18.888,      30.851,       4.287,       0.797,       0.003, ... \n(\n# to RasterBrick\ntmax_PRISM_m8_y09_rb <- as(prcp_tmax_PRISM_m8_y09[\"tmax\",,,], \"Raster\")\n)class      : RasterBrick \ndimensions : 20, 20, 400, 10  (nrow, ncol, ncell, nlayers)\nresolution : 0.04166667, 0.04166667  (x, y)\nextent     : -121.7292, -120.8958, 45.8125, 46.64583  (xmin, xmax, ymin, ymax)\ncrs        : +proj=longlat +datum=NAD83 +no_defs \nsource     : memory\nnames      : layer.1, layer.2, layer.3, layer.4, layer.5, layer.6, layer.7, layer.8, layer.9, layer.10 \nmin values :  10.188,   8.811,   6.811,   3.640,   1.833,   6.780,  10.318,  14.270,  18.281,   19.818 \nmax values :  29.842,  28.892,  27.123,  23.171,  21.858,  24.690,  27.740,  32.906,  35.568,   39.707 \ntime       : 2009-08-11, 2009-08-12, 2009-08-13, 2009-08-14, 2009-08-15, 2009-08-16, 2009-08-17, 2009-08-18, 2009-08-19, 2009-08-20 \n(\n# to SpatRaster\ntmax_PRISM_m8_y09_sr <- as(prcp_tmax_PRISM_m8_y09[\"tmax\",,,], \"SpatRaster\")\n) class       : SpatRaster \ndimensions  : 20, 20, 10  (nrow, ncol, nlyr)\nresolution  : 0.04166667, 0.04166667  (x, y)\nextent      : -121.7292, -120.8958, 45.8125, 46.64583  (xmin, xmax, ymin, ymax)\ncoord. ref. : lon/lat NAD83 (EPSG:4269) \nsource      : memory \nnames       : date2~08-11, date2~08-12, date2~08-13, date2~08-14, date2~08-15, date2~08-16, ... \nmin values  :      10.188,       8.811,       6.811,       3.640,       1.833,       6.780, ... \nmax values  :      29.842,      28.892,      27.123,      23.171,      21.858,      24.690, ... \n(\ntmax_PRISM_m8_y09_back_to_stars <- st_as_stars(tmax_PRISM_m8_y09_rb) \n)stars object with 3 dimensions and 1 attribute\nattribute(s):\n          Min.  1st Qu. Median     Mean  3rd Qu.   Max.\nlayer.1  1.833 17.55575 21.483 22.03543 26.54275 39.707\ndimension(s):\n     from to     offset      delta refsys point values x/y\nx       1 20   -121.729  0.0416667  NAD83    NA   NULL [x]\ny       1 20    46.6458 -0.0416667  NAD83    NA   NULL [y]\nband    1 10 2009-08-11     1 days   Date    NA   NULL    \n(\ntmax_PRISM_m8_y09_back_to_stars <- st_as_stars(tmax_PRISM_m8_y09_sr) \n)stars object with 3 dimensions and 1 attribute\nattribute(s):\n         Min.  1st Qu. Median     Mean  3rd Qu.   Max.\nvalues  1.833 17.55575 21.483 22.03543 26.54275 39.707\ndimension(s):\n     from to   offset      delta refsys point                            values\nx       1 20 -121.729  0.0416667  NAD83    NA                              NULL\ny       1 20  46.6458 -0.0416667  NAD83    NA                              NULL\nband    1 10       NA         NA     NA    NA date2009-08-11,...,date2009-08-20\n     x/y\nx    [x]\ny    [y]\nband    "},{"path":"create-maps.html","id":"create-maps","chapter":"8 Creating Maps using ggplot2","heading":"8 Creating Maps using ggplot2","text":"","code":""},{"path":"create-maps.html","id":"before-you-start-7","chapter":"8 Creating Maps using ggplot2","heading":"Before you start","text":"previous chapters, shown create simple maps quickly vector raster data. section focuses using ggplot2 package create high-quality maps publishable journal articles, conference presentations, kind professional reports. requires fine-tuning aesthetics maps beyond default maps, using appropriate color scheme, removing unnecessary information, formatting legends, etc. chapter focuses creating static maps, cover interactive maps often see web.Creating maps differs creating non-spatial figures ways. However, underlying principle syntax ggplot2 create maps non-spatial figures similar. Indeed, find map making intuitive rather easy already knowledge ggplot2 works even created maps using ggplot2 . Indeed, major difference choice geom_*() types. several geom_*() types disposal spatial data visualization.geom_sf() sf (vector) objectsgeom_raster() raster datageom_stars() stars (vector raster) objectThese geom_*()s allow visualizing vector raster data consistent simple ggplot2 syntax. provides direct supports sf stars objects, meaning transformation objects necessary prior creating maps. hand, (simple) data transformation necessary Raster\\(^*\\) objects raster package SpatRaster SpatVector terra package. look geoms individually understand basic usage sections 8.1 8.2. notice nothing spatial sections following sections. general applicable kind figures. Note : chapter assume much knowledge ggplot2, basic knowledge ggplot2 extremely helpful. know anything ggplot2 afraid knowledge ggplot2 insufficient, Appendix B provides minimal knowledge data visualization using ggplot2 package can least understand happening Chapter.Useful resourcesAs mentioned earlier, general knowledge ggplot2 works useful. , resources learning ggplot2 useful. :ggplot2: Elegant Graphics Data AnalysisThe following book provides numerous map making examples using ggplot2. good place improve map making skills completing chapter.Bob Rudis’ Code Examples","code":""},{"path":"create-maps.html","id":"direction-for-replication-7","chapter":"8 Creating Maps using ggplot2","heading":"Direction for replication","text":"DatasetsAll datasets need import available . chapter, path files set relative working directory (hidden). run codes without mess paths files, follow steps:set folder (folder) working directory using setwd()create folder called “Data” inside folder designated working directory (created “Data” folder previously, skip step)download pertinent datasets hereplace files downloaded folder “Data” folderPackagesRun following code install load (already installed) pacman package, install load (already installed) listed package inside pacman::p_load() function.","code":"\nif (!require(\"pacman\")) install.packages(\"pacman\")\npacman::p_load(\n  stars, # spatiotemporal data handling\n  raster, # raster data handling\n  terra, # raster data handling\n  sf, # vector data handling\n  dplyr, # data wrangling\n  stringr, # string manipulation\n  lubridate, # dates handling\n  data.table, # data wrangling\n  patchwork, # arranging figures\n  tigris, # county border\n  colorspace, # color scale\n  viridis, # arranging figures\n  tidyr, # reshape\n  ggspatial, # north arrow and scale bar\n  ggplot2 # make maps\n)"},{"path":"create-maps.html","id":"geom-sf","chapter":"8 Creating Maps using ggplot2","heading":"8.1 Creating maps from sf objects","text":"section explains create maps vector data stored sf object via geom_sf().","code":""},{"path":"create-maps.html","id":"datasets-2","chapter":"8 Creating Maps using ggplot2","heading":"8.1.1 Datasets","text":"following datasets used illustrations.Pointsaf_used: total annual groundwater pumping individual irrigation wellsPolygonsLines","code":"\n#--- read in the KS wells data ---#\n(\n  gw_KS_sf <- readRDS(\"Data/gw_KS_sf.rds\")\n)Simple feature collection with 56225 features and 3 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: -102.0495 ymin: 36.99561 xmax: -94.70746 ymax: 40.00191\nGeodetic CRS:  NAD83\nFirst 10 features:\n   well_id year   af_used                   geometry\n1        1 2010  67.00000 POINT (-100.4423 37.52046)\n2        1 2011 171.00000 POINT (-100.4423 37.52046)\n3        3 2010  30.93438 POINT (-100.7118 39.91526)\n4        3 2011  12.00000 POINT (-100.7118 39.91526)\n5        7 2010   0.00000 POINT (-101.8995 38.78077)\n6        7 2011   0.00000 POINT (-101.8995 38.78077)\n7       11 2010 154.00000 POINT (-101.7114 39.55035)\n8       11 2011 160.00000 POINT (-101.7114 39.55035)\n9       12 2010  28.17239 POINT (-95.97031 39.16121)\n10      12 2011  89.53479 POINT (-95.97031 39.16121)\n(\n  KS_county <- tigris::counties(state = \"Kansas\", cb = TRUE) %>%\n    st_as_sf() %>%\n    st_transform(st_crs(gw_KS_sf))\n)Simple feature collection with 105 features and 12 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -102.0517 ymin: 36.99302 xmax: -94.58841 ymax: 40.00316\nGeodetic CRS:  NAD83\nFirst 10 features:\n    STATEFP COUNTYFP COUNTYNS       AFFGEOID GEOID         NAME\n48       20      075 00485327 0500000US20075 20075     Hamilton\n210      20      149 00485038 0500000US20149 20149 Pottawatomie\n211      20      003 00484971 0500000US20003 20003     Anderson\n409      20      033 00484986 0500000US20033 20033     Comanche\n415      20      189 00485056 0500000US20189 20189      Stevens\n601      20      161 00485044 0500000US20161 20161        Riley\n602      20      025 00484982 0500000US20025 20025        Clark\n603      20      163 00485045 0500000US20163 20163        Rooks\n810      20      091 00485010 0500000US20091 20091      Johnson\n894      20      187 00485055 0500000US20187 20187      Stanton\n               NAMELSAD STUSPS STATE_NAME LSAD      ALAND   AWATER\n48      Hamilton County     KS     Kansas   06 2580958328  2893322\n210 Pottawatomie County     KS     Kansas   06 2177507162 54149295\n211     Anderson County     KS     Kansas   06 1501263686 10599981\n409     Comanche County     KS     Kansas   06 2041681089  3604155\n415      Stevens County     KS     Kansas   06 1883593926   464936\n601        Riley County     KS     Kansas   06 1579116499 32002514\n602        Clark County     KS     Kansas   06 2524300310  6603384\n603        Rooks County     KS     Kansas   06 2306454194 11962259\n810      Johnson County     KS     Kansas   06 1226694710 16303985\n894      Stanton County     KS     Kansas   06 1762103387   178555\n                          geometry\n48  MULTIPOLYGON (((-102.0446 3...\n210 MULTIPOLYGON (((-96.72833 3...\n211 MULTIPOLYGON (((-95.51879 3...\n409 MULTIPOLYGON (((-99.54467 3...\n415 MULTIPOLYGON (((-101.5566 3...\n601 MULTIPOLYGON (((-96.96095 3...\n602 MULTIPOLYGON (((-100.1072 3...\n603 MULTIPOLYGON (((-99.60527 3...\n810 MULTIPOLYGON (((-95.05647 3...\n894 MULTIPOLYGON (((-102.0419 3...\n(\n  KS_railroads <- st_read(dsn = \"Data/\", layer = \"tl_2015_us_rails\") %>%\n    st_crop(KS_county)\n)Simple feature collection with 5796 features and 3 fields\nGeometry type: GEOMETRY\nDimension:     XY\nBounding box:  xmin: -102.0517 ymin: 36.99769 xmax: -94.58841 ymax: 40.06304\nGeodetic CRS:  NAD83\nFirst 10 features:\n        LINEARID                         FULLNAME MTFCC\n2124 11051038759                          Bnsf RR R1011\n2136 11051038771                          Bnsf RR R1011\n2141 11051038776                          Bnsf RR R1011\n2186 11051047374                   Rock Island RR R1011\n2240 11051048071  Burlington Northern Santa Fe RR R1011\n2252 11051048083  Burlington Northern Santa Fe RR R1011\n2256 11051048088  Burlington Northern Santa Fe RR R1011\n2271 11051048170 Chicago Burlington and Quincy RR R1011\n2272 11051048171 Chicago Burlington and Quincy RR R1011\n2293 11051048193 Chicago Burlington and Quincy RR R1011\n                           geometry\n2124 LINESTRING (-94.58841 39.15...\n2136 LINESTRING (-94.59017 39.11...\n2141 LINESTRING (-94.58841 39.15...\n2186 LINESTRING (-94.58893 39.11...\n2240 LINESTRING (-94.58841 39.15...\n2252 LINESTRING (-94.59017 39.11...\n2256 LINESTRING (-94.58841 39.15...\n2271 LINESTRING (-94.58862 39.15...\n2272 LINESTRING (-94.58883 39.11...\n2293 LINESTRING (-94.58871 39.11..."},{"path":"create-maps.html","id":"basic-usage-of-geom_sf","chapter":"8 Creating Maps using ggplot2","heading":"8.1.2 Basic usage of geom_sf()","text":"geom_sf() allows visualizing sf objects. Conveniently, geom_sf() automatically detects geometry type spatial objects stored sf draw maps accordingly. example, following codes create maps Kansas wells (points), Kansas counties (polygons), railroads Kansas (lines):can see, different geometry types handled single geom type, geom_sf(). Notice also neither x-axis (longitude) y-axis (latitude) provided geom_sf() example codes . create map, longitude latitude always used x- y-axis. geom_sf() smart enough know geometry types draw spatial objects accordingly.","code":"\n(\n  g_wells <- ggplot(data = gw_KS_sf) +\n    geom_sf()\n)\n(\n  g_county <- ggplot(data = KS_county) +\n    geom_sf()\n)\n(\n  g_rail <- ggplot(data = KS_railroads) +\n    geom_sf()\n)"},{"path":"create-maps.html","id":"specifying-the-aesthetics","chapter":"8 Creating Maps using ggplot2","heading":"8.1.3 Specifying the aesthetics","text":"various aesthetics options can use. Available aesthetics vary type geometry. section shows basics specify aesthetics maps. Finer control aesthetics discussed later.","code":""},{"path":"create-maps.html","id":"points","chapter":"8 Creating Maps using ggplot2","heading":"8.1.3.1 Points","text":"color: color pointsfill: available shapes (likely useless)shape: shape pointssize: size points (rarely useful)illustration , let’s focus wells one county easy detect differences across various aesthetics configurations.example 1color: dependent af_used (amount groundwater extraction)size: constant across points (bigger default)example 2color: constant across points (blue)size: dependent af_usedshape: constant across points (square)example 3color: dependent whether located east west -101.3 longitudeshape: dependent whether located east west -101.3 longitude","code":"\n#--- wells in Stevens County ---#\ngw_Stevens <- KS_county %>%\n  filter(NAME == \"Stevens\") %>%\n  st_crop(gw_KS_sf, .)\n(\n  ggplot(data = gw_Stevens) +\n    geom_sf(aes(color = af_used), size = 2)\n)\n(\n  ggplot(data = gw_Stevens) +\n    geom_sf(aes(size = af_used), color = \"blue\", shape = 15)\n)\n(\n  gw_Stevens %>%\n    cbind(., st_coordinates(.)) %>%\n    mutate(east_west = ifelse(X < -101.3, \"west\", \"east\")) %>%\n    ggplot(data = .) +\n    geom_sf(aes(shape = east_west, color = east_west))\n)"},{"path":"create-maps.html","id":"polygons","chapter":"8 Creating Maps using ggplot2","heading":"8.1.3.2 Polygons","text":"color: color borders polygonsfill: color inside polygonsshape: availablesize: availableexample 1color: constant (red)fill: constant (dark green)example 2color: default (black)fill: dependent total amount pumping 2010","code":"\nggplot(data = KS_county) +\n  geom_sf(color = \"red\", fill = \"darkgreen\")\n# In knitting the book, dplyr operations like filter, mutate cause an error even though they would not produce any errors when you are running on an R session. The solution used here is to create the object to be used and save it, and then read it.\n\n# KS_county_with_pumping <-\n#   gw_KS_sf %>%\n#   #--- only year == 2010 ---#\n#   dplyr::filter(year == 2010) %>%\n#   #--- get total pumping by county ---#\n#   aggregate(., KS_county, sum, na.rm = TRUE)\n\n# saveRDS(KS_county_with_pumping, \"Data/KS_county_with_pumping.rds\")\n\nKS_county_with_pumping <- readRDS(\"Data/KS_county_with_pumping.rds\")\nKS_county_with_pumping <-\n  gw_KS_sf %>%\n  #--- only year == 2010 ---#\n  dplyr::filter(year == 2010) %>%\n  #--- get total pumping by county ---#\n  aggregate(., KS_county, sum, na.rm = TRUE)\n\nggplot(data = KS_county_with_pumping) +\n  geom_sf(aes(fill = af_used))"},{"path":"create-maps.html","id":"plotting-multiple-spatial-objects-in-one-figure","chapter":"8 Creating Maps using ggplot2","heading":"8.1.4 Plotting multiple spatial objects in one figure","text":"can combine layers created geom_sf() additively appear single map:Oops, see wells (points) figure. order geom_sf() matters. layer added later come top preceding layers. ’s wells hidden beneath Kansas counties. , let’s :Better.Note since using different datasets layer, need specify dataset use layer except first geom_sf() inherits data = KS_wells ggplot(data = KS_wells). course, create exactly map:rule need supply data ggplot().87Alternatively, add fill = NA geom_sf(data = KS_county) instead switching order.fine long intend color-code counties.","code":"\nggplot() +\n  #--- this one uses KS_wells ---#\n  geom_sf(data = gw_KS_sf, size = 0.4) +\n  #--- this one uses KS_county ---#\n  geom_sf(data = KS_county) +\n  #--- this one uses KS_railroads ---#\n  geom_sf(data = KS_railroads, color = \"red\")\nggplot(data = KS_county) +\n  #--- this one uses KS_county ---#\n  geom_sf() +\n  #--- this one uses KS_county ---#\n  geom_sf(data = gw_KS_sf, size = 0.4) +\n  #--- this one uses KS_railroads ---#\n  geom_sf(data = KS_railroads, color = \"red\")\n(\n  g_all <- ggplot() +\n    #--- this one uses KS_county ---#\n    geom_sf(data = KS_county) +\n    #--- this one uses KS_wells ---#\n    geom_sf(data = gw_KS_sf, size = 0.4) +\n    #--- this one uses KS_railroads ---#\n    geom_sf(data = KS_railroads, color = \"red\")\n)\nggplot() +\n  #--- this one uses KS_wells ---#\n  geom_sf(data = gw_KS_sf, size = 0.4) +\n  #--- this one uses KS_county ---#\n  geom_sf(data = KS_county, fill = NA) +\n  #--- this one uses KS_railroads ---#\n  geom_sf(data = KS_railroads, color = \"red\")"},{"path":"create-maps.html","id":"crs","chapter":"8 Creating Maps using ggplot2","heading":"8.1.5 CRS","text":"ggplot() uses CRS sf draw map. example, right now CRS KS_county :Let’s convert CRS WGS 84/ UTM zone 14N (EPSG code: 32614), make map, compare ones different CRS side side.Alternatively, use coord_sf() alter CRS map, CRS sf object .multiple layers used map creation, CRS first layer applied layers.coord_sf() applies layers.Finally, limit geographic scope map created adding xlim() ylim().","code":"\nst_crs(KS_county)Coordinate Reference System:\n  User input: EPSG:4269 \n  wkt:\nGEOGCS[\"NAD83\",\n    DATUM[\"North_American_Datum_1983\",\n        SPHEROID[\"GRS 1980\",6378137,298.257222101,\n            AUTHORITY[\"EPSG\",\"7019\"]],\n        TOWGS84[0,0,0,0,0,0,0],\n        AUTHORITY[\"EPSG\",\"6269\"]],\n    PRIMEM[\"Greenwich\",0,\n        AUTHORITY[\"EPSG\",\"8901\"]],\n    UNIT[\"degree\",0.0174532925199433,\n        AUTHORITY[\"EPSG\",\"9122\"]],\n    AUTHORITY[\"EPSG\",\"4269\"]]\ng_32614 <- st_transform(KS_county, 32614) %>%\n  ggplot(data = .) +\n  geom_sf()\ng_county / g_32614\nggplot() +\n  #--- epsg: 4269 ---#\n  geom_sf(data = KS_county) +\n  coord_sf(crs = 32614)\nggplot() +\n  #--- epsg: 32614 ---#\n  geom_sf(data = st_transform(KS_county, 32614)) +\n  #--- epsg: 4269 ---#\n  geom_sf(data = KS_railroads)\nggplot() +\n  #--- epsg: 32614 ---#\n  geom_sf(data = st_transform(KS_county, 32614)) +\n  #--- epsg: 4269 ---#\n  geom_sf(data = KS_railroads) +\n  #--- using 4269 ---#\n  coord_sf(crs = 4269)\nggplot() +\n  #--- epsg: 32614 ---#\n  geom_sf(data = st_transform(KS_county, 32614)) +\n  #--- epsg: 4269 ---#\n  geom_sf(data = KS_railroads) +\n  #--- using 4269 ---#\n  coord_sf(crs = 4269) +\n  #--- limit the geographic scope of the map ---#\n  xlim(-99, -97) +\n  ylim(37, 39)"},{"path":"create-maps.html","id":"faceting","chapter":"8 Creating Maps using ggplot2","heading":"8.1.6 Faceting","text":"Faceting splits data groups generates figure group, aesthetics figures consistent across groups. Faceting can done using facet_wrap() facet_grid(). Let’s try create map groundwater use wells year points color differentiated amount groundwater use (af_used).Note code creates single legend applies panels, allows compare values across panels (years ). , also note values faceting variable (year) displayed gray strips maps. can panels stacked vertically using ncol option (nrow also works) facet_wrap(. ~ year) follows:Two-way faceting possible supplying variable name (expression) place . facet_wrap(. ~ year). code uses expression (af_used > 200) place .. divides dataset whether water use greater 200 year.values expression (TRUE FALSE) appear gray strips, informative. discuss detail control texts strips section 8.5.feel like panels close , provide space using panel.spacing (vertically horizontally), panel.spacing.x (horizontally), panel.spacing.y (vertically) options theme(). Suppose like place space upper lower panels, use panel.spacing.y like :","code":"\nggplot() +\n  #--- KS county boundary ---#\n  geom_sf(data = st_transform(KS_county, 32614)) +\n  #--- wells ---#\n  geom_sf(data = gw_KS_sf, aes(color = af_used)) +\n  #--- facet by year (side by side) ---#\n  facet_wrap(. ~ year)\nggplot() +\n  #--- KS county boundary ---#\n  geom_sf(data = st_transform(KS_county, 32614)) +\n  #--- wells ---#\n  geom_sf(data = gw_KS_sf, aes(color = af_used)) +\n  #--- facet by year (side by side) ---#\n  facet_wrap(. ~ year, ncol = 1)\nggplot() +\n  #--- KS county boundary ---#\n  geom_sf(data = st_transform(KS_county, 32614)) +\n  #--- wells ---#\n  geom_sf(data = gw_KS_sf, aes(color = af_used)) +\n  #--- facet by year (side by side) ---#\n  facet_wrap((af_used > 200) ~ year)\nggplot() +\n  #--- KS county boundary ---#\n  geom_sf(data = st_transform(KS_county, 32614)) +\n  #--- wells ---#\n  geom_sf(data = gw_KS_sf, aes(color = af_used)) +\n  #--- facet by year (side by side) ---#\n  facet_wrap((af_used > 200) ~ year) +\n  #--- add more space between panels ---#\n  theme(panel.spacing.y = unit(2, \"lines\"))"},{"path":"create-maps.html","id":"adding-texts-labels-on-a-map","chapter":"8 Creating Maps using ggplot2","heading":"8.1.7 Adding texts (labels) on a map","text":"can add labels map using geom_sf_text() geom_sf_label() providing aes(label = x) inside x variable contains labels print map.like overlapping labels printed, can add check_overlap = TRUE.nudge_x nudge_y options let shift labels.like fine control objects, can always work separately.also use annotate() place texts map, can useful like place arbitrary texts part sf object.can see, need tell texts placed x y, provide texts want map label.","code":"\nggplot() +\n  #--- KS county boundary ---#\n  geom_sf(data = KS_county) +\n  geom_sf_text(\n    data = KS_county,\n    aes(label = NAME),\n    size = 3,\n    color = \"blue\"\n  )\nggplot() +\n  #--- KS county boundary ---#\n  geom_sf(data = KS_county) +\n  geom_sf_text(\n    data = KS_county,\n    aes(label = NAME),\n    check_overlap = TRUE,\n    size = 3,\n    color = \"blue\"\n  )\nggplot() +\n  #--- KS county boundary ---#\n  geom_sf(data = KS_county) +\n  geom_sf_text(\n    data = KS_county,\n    aes(label = NAME),\n    check_overlap = TRUE,\n    size = 3,\n    color = \"blue\",\n    nudge_x = -0.1,\n    nudge_y = 0.1\n  )\nCheyenne <- filter(KS_county, NAME == \"Cheyenne\")\nKS_less_Cheyenne <- filter(KS_county, NAME != \"Cheyenne\")\nggplot() +\n  #--- KS county boundary ---#\n  geom_sf(data = KS_county) +\n  geom_sf_text(\n    data = KS_less_Cheyenne,\n    aes(label = NAME),\n    check_overlap = TRUE,\n    size = 3,\n    color = \"blue\",\n    nudge_x = -0.1,\n    nudge_y = 0.1\n  ) +\n  geom_sf_text(\n    data = Cheyenne,\n    aes(label = NAME),\n    size = 2.5,\n    color = \"red\",\n    nudge_y = 0.2\n  )\nggplot() +\n  #--- KS county boundary ---#\n  geom_sf(data = KS_county) +\n  geom_sf_text(\n    data = KS_less_Cheyenne,\n    aes(label = NAME),\n    check_overlap = TRUE,\n    size = 3,\n    color = \"blue\",\n    nudge_x = -0.1,\n    nudge_y = 0.1\n  ) +\n  #--- use annotate to add texts on the map ---#\n  annotate(\n    geom = \"text\",\n    x = -102,\n    y = 39.8,\n    size = 3,\n    label = \"Cheyennes\",\n    color = \"red\"\n  )"},{"path":"create-maps.html","id":"geom-raster","chapter":"8 Creating Maps using ggplot2","heading":"8.2 Raster data visualization: geom_raster() and geom_stars()","text":"section shows use geom_raster() geom_stars() create maps raster datasets two object classes: Raster\\(^*\\) objects raster package stars objects stars package. geom_raster accept either Raster\\(^*\\) stars input. Instead, geom_raster() accepts data.frame coordinates create maps. , two-step procedureconvert raster dataset data.frame coordinatesuse geom_raster() make mapgeom_stars() stars package accepts stars object, data transformation necessary.use following objects information come different object classes illustration section.Raster starsRaster RasterStack","code":"\ntmax_Jan_09 <- readRDS(\"Data/tmax_Jan_09_stars.rds\")\ntmax_Jan_09_rs <- stack(\"Data/tmax_Jan_09.tif\")"},{"path":"create-maps.html","id":"visualize-raster-with-geom_raster","chapter":"8 Creating Maps using ggplot2","heading":"8.2.1 Visualize Raster\\(^*\\) with geom_raster()","text":"order create maps information stored Raster\\(^*\\) objects, first convert regular data.frame using .data.frame() follows:xy = TRUE option adds coordinates centroid raster cells, na.omit() removes cells outside Kansas border NA values. Notice band comprises column data.frame.conversion done, can use geom_raster() create map. Unlike geom_sf(), need supply variables names geographical coordinates (x longitude y latitude). , also need specify variable use fill color differentiation.can add coord_equal() one degree latitude longitude length map. course, one degree longitude one degree latitude length US. However, distortion smaller compared default least.visualized tmax data one day (layer.1) five days tmax records. present time can facet using facet_wrap(). However, need data long format like :Let’s now use facet_wrap() create series tmax maps.","code":"\n#--- convert to data.frame ---#\ntmax_Jan_09_df <-\n  as.data.frame(tmax_Jan_09_rs, xy = TRUE) %>%\n  #--- remove cells with NA for any of the layers ---#\n  na.omit() %>%\n  #--- change the variable names ---#\n  setnames(\n    paste0(\"tmax_Jan_09.\", 1:5),\n    seq(ymd(\"2009-01-01\"), ymd(\"2009-01-05\"), by = \"days\") %>%\n      as.character()\n  )\n\n#--- take a look ---#\nhead(tmax_Jan_09_df)              x        y 2009-01-01 2009-01-02 2009-01-03 2009-01-04 2009-01-05\n17578 -95.12500 49.41667    -12.863    -11.455    -17.516    -12.755    -26.446\n18982 -95.16667 49.37500    -12.698    -11.441    -17.436    -12.672    -25.889\n18983 -95.12500 49.37500    -12.921    -11.611    -17.589    -12.830    -26.544\n18984 -95.08333 49.37500    -13.074    -11.827    -17.684    -12.913    -26.535\n18985 -95.04167 49.37500    -13.591    -12.220    -17.920    -12.684    -25.780\n18986 -95.00000 49.37500    -13.688    -12.177    -17.911    -12.548    -25.424\n(\n  g_tmax_map <- ggplot(data = tmax_Jan_09_df) +\n    geom_raster(aes(x = x, y = y, fill = `2009-01-01`)) +\n    scale_fill_viridis_c() +\n    theme_void() +\n    theme(\n      legend.position = \"bottom\"\n    )\n)\ng_tmax_map + coord_equal()\ntmax_long_df <- \n  tmax_Jan_09_df %>%\n  pivot_longer(\n    c(-x, -y),\n    names_to = \"date\",\n    values_to = \"tmax\"\n  )\n\n#--- take a look ---#\nhead(tmax_long_df)# A tibble: 6 × 4\n      x     y date        tmax\n  <dbl> <dbl> <chr>      <dbl>\n1 -95.1  49.4 2009-01-01 -12.9\n2 -95.1  49.4 2009-01-02 -11.5\n3 -95.1  49.4 2009-01-03 -17.5\n4 -95.1  49.4 2009-01-04 -12.8\n5 -95.1  49.4 2009-01-05 -26.4\n6 -95.2  49.4 2009-01-01 -12.7\nggplot() +\n  geom_raster(data = tmax_long_df, aes(x = x, y = y, fill = tmax)) +\n  facet_wrap(date ~ .) +\n  coord_equal() +\n  scale_fill_viridis_c() +\n  theme_void() +\n  theme(\n    legend.position = \"bottom\"\n  )"},{"path":"create-maps.html","id":"visualize-stars-with-geom_raster","chapter":"8 Creating Maps using ggplot2","heading":"8.2.2 Visualize stars with geom_raster()","text":"Similarly Raster\\(^*\\) objects, first need convert stars regular data.frame using .data.frame() follows:One key difference conversion Raster\\(^*\\) objects data.frame already long format, means can immediately make faceted figures like :","code":"\n#--- converting to a data.frame ---#\ntmax_long_df <- as.data.frame(tmax_Jan_09, xy = TRUE) %>%\n  na.omit()\n\n#--- take a look ---#\nhead(tmax_Jan_09_df)              x        y 2009-01-01 2009-01-02 2009-01-03 2009-01-04 2009-01-05\n17578 -95.12500 49.41667    -12.863    -11.455    -17.516    -12.755    -26.446\n18982 -95.16667 49.37500    -12.698    -11.441    -17.436    -12.672    -25.889\n18983 -95.12500 49.37500    -12.921    -11.611    -17.589    -12.830    -26.544\n18984 -95.08333 49.37500    -13.074    -11.827    -17.684    -12.913    -26.535\n18985 -95.04167 49.37500    -13.591    -12.220    -17.920    -12.684    -25.780\n18986 -95.00000 49.37500    -13.688    -12.177    -17.911    -12.548    -25.424\nggplot() +\n  geom_raster(data = tmax_long_df, aes(x = x, y = y, fill = tmax)) +\n  facet_wrap(date ~ .) +\n  coord_equal() +\n  scale_fill_viridis_c() +\n  theme_void() +\n  theme(\n    legend.position = \"bottom\"\n  )"},{"path":"create-maps.html","id":"visualize-stars-with-geom_stars","chapter":"8 Creating Maps using ggplot2","heading":"8.2.3 Visualize stars with geom_stars()","text":"saw geom_raster() requires converting stars object data.frame first creating map. geom_stars() stars package lets use stars object directly easily create map ggplot2 framework. geom_stars() works just like geom_sf(). need supply stars object geom_stars() data.fill color raster cells automatically set attribute (tmax) aes(fill = tmax). good idea add coord_equal() issue saw geom_raster().default, geom_stars() plots first band. order present layers time, can add facet_wrap( ~ x) x name third dimension stars object (date ).","code":"\nggplot() +\n  geom_stars(data = tmax_Jan_09) +\n  theme_void()\nggplot() +\n  geom_stars(data = tmax_Jan_09) +\n  theme_void() +\n  coord_equal()\nggplot() +\n  geom_stars(data = tmax_Jan_09) +\n  facet_wrap(~date) +\n  coord_equal() +\n  theme_void()"},{"path":"create-maps.html","id":"adding-geom_sf-layers","chapter":"8 Creating Maps using ggplot2","heading":"8.2.4 adding geom_sf() layers","text":"can easily add geom_sf() layers map created geom_raster() geom_stars(). Let’s crop tmax data Kansas create map tmax values displayed top Kansas county borders.","code":"\n#--- crop to KS ---#\nKS_tmax_Jan_09_stars <- tmax_Jan_09 %>%\n  st_crop(., KS_county)\n\n#--- convert to a df ---#\nKS_tmax_Jan_09_df <- as.data.frame(KS_tmax_Jan_09_stars, xy = TRUE) %>%\n  na.omit()"},{"path":"create-maps.html","id":"adding-geom_sf-to-a-map-with-geom_raster","chapter":"8 Creating Maps using ggplot2","heading":"8.2.4.1 adding geom_sf() to a map with geom_raster()","text":"Notice coord_equal() necessary code. Indeed, try add coord_equal(), error., original stars Raster\\(^*\\) must CRS sf objects. following code transforms CRS KS_county 32614 try plot together.plotting multiple sf objects, CRS map set CRS sf objects used first geom_sf() layer, rest sf objects followed suit. case .","code":"\nggplot() +\n  geom_raster(data = KS_tmax_Jan_09_df, aes(x = x, y = y, fill = tmax)) +\n  geom_sf(data = KS_county, fill = NA) +\n  facet_wrap(date ~ .) +\n  scale_fill_viridis_c() +\n  theme_void() +\n  theme(\n    legend.position = \"bottom\"\n  )\nggplot() +\n  geom_raster(data = KS_tmax_Jan_09_df, aes(x = x, y = y, fill = tmax)) +\n  geom_sf(data = KS_county, fill = NA) +\n  facet_wrap(date ~ .) +\n  coord_equal() +\n  scale_fill_viridis_c() +\n  theme_void() +\n  theme(\n    legend.position = \"bottom\"\n  )Error in `f()`:\n! geom_sf() must be used with coord_sf()\nggplot() +\n  geom_raster(data = KS_tmax_Jan_09_df, aes(x = x, y = y, fill = tmax)) +\n  geom_sf(data = st_transform(KS_county, 32614), fill = NA) +\n  facet_wrap(date ~ .) +\n  scale_fill_viridis_c() +\n  theme_void() +\n  theme(\n    legend.position = \"bottom\"\n  )"},{"path":"create-maps.html","id":"adding-geom_sf-to-a-map-with-geom_stars","chapter":"8 Creating Maps using ggplot2","heading":"8.2.4.2 adding geom_sf() to a map with geom_stars()","text":"basically case geom_stars()Just like case geom_raster(), coord_equal() necessary stars sf objects must CRS.","code":"\nggplot() +\n  geom_stars(data = KS_tmax_Jan_09_stars) +\n  geom_sf(data = KS_county, fill = NA) +\n  facet_wrap(~date) +\n  theme_void()\nggplot() +\n  geom_stars(data = KS_tmax_Jan_09_stars) +\n  geom_sf(data = st_transform(KS_county, 32614), fill = NA) +\n  facet_wrap(~date) +\n  theme_void()"},{"path":"create-maps.html","id":"color-scale","chapter":"8 Creating Maps using ggplot2","heading":"8.3 Color scale","text":"Color scale refers way variable values mapped colors figure. example, example , color scale fill maps value af_used gradient colors: dark blue low values light blue high values af_used.Often times, aesthetically desirable change default color scale ggplot2. example, like color-differentiate temperature values, might want start blue low values red high values.can control color scale using scale_*() functions. scale_*() function use depends type aesthetics (fill color) whether aesthetic variable continuous discrete. Providing wrong kind scale_*() function results error.","code":"\ntmax_long_df <- readRDS(\"Data/tmax_Jan_09_stars.rds\") %>%\n  as.data.frame(xy = TRUE) %>%\n  na.omit()\ng_col_scale <- ggplot() +\n  geom_raster(data = tmax_long_df, aes(x = x, y = y, fill = tmax)) +\n  facet_wrap(date ~ .) +\n  coord_equal() +\n  theme_void() +\n  theme(\n    legend.position = \"bottom\"\n  )"},{"path":"create-maps.html","id":"viridis-color-maps","chapter":"8 Creating Maps using ggplot2","heading":"8.3.1 Viridis color maps","text":"ggplot2 packages offers scale_A_viridis_B() functions viridis color map, type aesthetics attribute (fill, color), B type variable. example, scale_fill_viridis_c() can used fill aesthetics applied continuous variable.five types palettes available viridis color map can selected using option = option. visualization five palettes.Let’s see PRISM tmax maps look like using Option D (default). Since aesthetics type fill tmax continuous, scale_fill_viridis_c() appropriate one .can reverse order color adding direction = -1.Let’s now work aesthetics mapping based discrete variable. code groups af_used five groups ranges.Since like control color aesthetics based discrete variable, using scale_color_viridis_d().","code":"\ndata(\"geyser\", package = \"MASS\")\n\nggplot(geyser, aes(x = duration, y = waiting)) +\n  xlim(0.5, 6) +\n  ylim(40, 110) +\n  stat_density2d(aes(fill = ..level..), geom = \"polygon\") +\n  theme_bw() +\n  theme(panel.grid = element_blank()) -> gg\n\n(gg + scale_fill_viridis_c(option = \"A\") + labs(x = \"magma\", y = NULL)) /\n  (gg + scale_fill_viridis_c(option = \"B\") + labs(x = \"inferno\", y = NULL)) /\n  (gg + scale_fill_viridis_c(option = \"C\") + labs(x = \"plasma\", y = NULL)) /\n  (gg + scale_fill_viridis_c(option = \"D\") + labs(x = \"viridis\", y = NULL)) /\n  (gg + scale_fill_viridis_c(option = \"E\") + labs(x = \"cividis\", y = NULL))\ng_col_scale + scale_fill_viridis_c(option = \"A\")\ng_col_scale + scale_fill_viridis_c(option = \"D\")\ng_col_scale + scale_fill_viridis_c(option = \"D\", direction = -1)\n#--- convert af_used to a discrete variable ---#\ngw_Stevens <- mutate(gw_Stevens, af_used_cat = cut_number(af_used, n = 5))\n\n#--- take a look ---#\nhead(gw_Stevens)Source: local data table [6 x 5]\nCall:   head(copy(`_DT1`)[, `:=`(af_used_cat = cut_number(af_used, n = 5))], \n    n = 6L)\n\n  well_id  year af_used             geometry af_used_cat   \n    <dbl> <dbl>   <dbl>          <POINT [°]> <fct>         \n1     234  2010    462.  (-101.4232 37.1165) (377,508]     \n2     234  2011    486.  (-101.4232 37.1165) (377,508]     \n3     290  2010    457. (-101.2301 37.29295) (377,508]     \n4     290  2011    580. (-101.2301 37.29295) (508,1.19e+03]\n5     317  2010    258  (-101.1111 37.04683) (157,264]     \n6     317  2011    255  (-101.1111 37.04683) (157,264]     \n\n# Use as.data.table()/as.data.frame()/as_tibble() to access results\nggplot(data = gw_Stevens) +\n  geom_sf(aes(color = af_used_cat), size = 2) +\n  scale_color_viridis_d(option = \"C\")"},{"path":"create-maps.html","id":"rcolorbrewer-scale__distiller-and-scale__brewer","chapter":"8 Creating Maps using ggplot2","heading":"8.3.2 RColorBrewer: scale_*_distiller() and scale_*_brewer()","text":"RColorBrewer package provides set color scales useful. list color scales can use.first set color palettes sequential palettes suitable variable ordinal meaning: temperature, precipitation, etc. second set palettes qualitative palettes suitable qualitative categorical data. Finally, third set palettes diverging palettes can suitable variables take negative positive values like changes groundwater level.Two types scale functions can used use palettes:scale_*_distiller() continuous variablescale_*_brewer() discrete variableTo use specific color palette, can simply add palette = \"palette name\" inside scale_fill_distiller(). codes applies “Spectral” example.can reverse color order adding trans = \"reverse\" option.specifying color aesthetics based continuous variable, use scale_color_distiller().Now, suppose variable interest comes categories ranges values. code groups af_used five ranges using ggplo2::cut_number().Since af_used_cat discrete variable, can use scale_color_brewer() instead.","code":"\n#--- load RColorBrewer ---#\nlibrary(RColorBrewer)\n\n#--- disply all the color schemes from the package ---#\ndisplay.brewer.all()\ng_col_scale + theme_void() +\n  scale_fill_distiller(palette = \"Spectral\")\ng_col_scale + theme_void() +\n  scale_fill_distiller(palette = \"Spectral\", trans = \"reverse\")\nggplot(data = gw_Stevens) +\n  geom_sf(aes(color = af_used), size = 2) +\n  scale_color_distiller(palette = \"Spectral\")\ngw_Stevens <- mutate(gw_Stevens, af_used_cat = cut_number(af_used, n = 5))\nggplot(data = gw_Stevens) +\n  geom_sf(aes(color = af_used_cat), size = 2) +\n  scale_color_brewer(palette = \"Spectral\")"},{"path":"create-maps.html","id":"colorspace-package","chapter":"8 Creating Maps using ggplot2","heading":"8.3.3 colorspace package","text":"satisfied viridis color map ColorBrewer palette options, might want try colorspace package.palettes colorspace package offers.packages offers scale_*() functions follows following naming convention:scale_aesthetic_datatype_colorscale whereaesthetic: fill colordatatype: continuous discretecolorscale: qualitative, sequential, diverging, divergingxFor example, add sequential color scale following map, use scale_fill_continuous_sequential() pick palette set sequential palettes shown . code uses Viridis palette reverse option:still find palette satisfies need (obsession point), can easily make . package offers hclwizard(), starts shiny-based web application let design color palette.running ,see web application pop looks like .find color scale like use, can go Exporttab, select R tab, copy code appear highlighted area.register color palette completing register = option copied code think use times. Otherwise, can delete option.use code follows:Note now using scale_*_gradientn() approach.discrete variable, can use scale_*_manual():","code":"\n#--- plot the palettes ---#\nhcl_palettes(plot = TRUE)\nggplot() +\n  geom_sf(data = gw_by_county, aes(fill = af_used)) +\n  facet_wrap(. ~ year) +\n  scale_fill_continuous_sequential(palette = \"Viridis\", trans = \"reverse\")\nhclwizard()\ncol_palette <- sequential_hcl(n = 7, h = c(36, 200), c = c(60, NA, 0), l = c(25, 95), power = c(0.7, 1.3))\ng_col_scale + theme_void() +\n  scale_fill_gradientn(colors = col_palette)\ncol_discrete <- sequential_hcl(n = 5, h = c(240, 130), c = c(30, NA, 33), l = c(25, 95), power = c(1, NA), rev = TRUE)\n\nggplot() +\n  geom_sf(data = gw_Stevens, aes(color = af_used_cat), size = 2) +\n  scale_color_manual(values = col_discrete)"},{"path":"create-maps.html","id":"arranging-maps","chapter":"8 Creating Maps using ggplot2","heading":"8.4 Arranging maps","text":"","code":""},{"path":"create-maps.html","id":"multiple-panels-of-figures-as-a-single-figure","chapter":"8 Creating Maps using ggplot2","heading":"8.4.1 Multiple panels of figures as a single figure","text":"Faceting using facet_wrap() facet_grid() allows dividing data groups creating map group. particularly suitable visualizing one variable different facets. good example collection maps tmax observed different dates (see figure ). Faceting provides single consistent color scale shared across facets.However, faceting suitable creating maps different variables. see let’s plot tmax precipitation Jan 1, 2009 together. first read precipitation data Jan 1, 2009.Let’s extract tmax precipitation Jan 1, 2009 respective datasets, combine .map faceted tmax precipitation:can see, single color scale created precipitation recorded mm temperature observed Celsius. particular day, precipitation 150 mm observed part west coast. Consequently, see almost color differentiation tmax map ranges roughly -20 30. simply good idea facet two variables observed different scales.Instead, independent color scale variables just combine . Now, might ask really need combine two. Can’t just two figures arrange manner like pdf WORD document? still convinced need two panels figures one figure, can use patchwork package.patchwork combines ggplot objects (maps) using simple operators: +, /, |. Let’s first create maps tmax precipitation separately.best just look examples get sense patchwork works. fuller treatment patchwork found (https://patchwork.data-imaginist.com/index.html).Example 1Example 2Example 3Example 4Example 5Sometimes figures placed close . case, can pad figure time generating individual figures adding plot.margin option theme(). example, following code creates space bottom g_tmax (5 cm), vertically stack g_tmax g_ppt.plot.margin = unit(c(, b, c, d), \"cm\"), margin , b, c, d refers .: topb: rightc: bottomd: left","code":"\nggplot() +\n  geom_raster(data = tmax_long_df, aes(x = x, y = y, fill = tmax)) +\n  facet_wrap(date ~ .) +\n  coord_equal() +\n  scale_fill_viridis_c() +\n  theme_void() +\n  theme(\n    legend.position = \"bottom\"\n  )\nppt_long_df <- readRDS(\"Data/ppt_long_df.rds\")\n#--- get tmax on Jan 1, 2009 ---#\ntmax_df <- filter(tmax_long_df, date == ymd(\"2009-01-01\")) %>%\n  setnames(\"tmax\", \"value\") %>%\n  mutate(type = \"tmax\")\n\n#--- get precipitation on Jan 1, 2009 ---#\nppt_df <- filter(ppt_long_df, date == ymd(\"2009-01-01\")) %>%\n  mutate(type = \"ppt\")\n\n#--- combine them ---#\ncombined_df <- rbind(tmax_df, ppt_df)\nggplot() +\n  geom_raster(data = combined_df, aes(x = x, y = y, fill = value)) +\n  facet_grid(type ~ .) +\n  scale_fill_viridis() +\n  theme_void()\nlibrary(patchwork)\n#--- tmax ---#\n(\n  g_tmax <- ggplot() +\n    geom_raster(data = tmax_df, aes(x = x, y = y, fill = value)) +\n    scale_fill_viridis() +\n    theme_void() +\n    coord_equal() +\n    theme(legend.position = \"bottom\")\n)\n#--- ppt ---#\n(\n  g_ppt <- ggplot() +\n    geom_raster(data = ppt_df, aes(x = x, y = y, fill = value)) +\n    scale_fill_viridis() +\n    theme_void() +\n    coord_equal() +\n    theme(legend.position = \"bottom\")\n)\ng_tmax + g_ppt\ng_tmax / g_ppt / g_tmax\ng_tmax + g_ppt + plot_layout(nrow = 2)\ng_tmax + g_ppt + g_tmax + g_ppt + plot_layout(nrow = 3, byrow = FALSE)\ng_tmax | (g_ppt / g_tmax)\n#--- space at the bottom ---#\ng_tmax <- g_tmax + theme(plot.margin = unit(c(0, 0, 5, 0), \"cm\"))\n\n#--- vertically stack ---#\ng_tmax / g_ppt"},{"path":"create-maps.html","id":"a-map-in-a-map-inset","chapter":"8 Creating Maps using ggplot2","heading":"8.4.2 A map in a map: inset","text":"Sometimes, useful present map covers larger geographical range area interest map. provides better sense geographic extent location area interest relative larger geographic extent readers familiar . example, suppose work restricted three counties Kansas: Cheyenne, Sherman, Wallace. map three counties:Well, familiar Kansas, might useful show Kansas located map (even Kansas U.S.). can achieved using ggplotGrob() annotation_custom(). steps following:create map area interest turn grob object using ggplotGrob()create map region includes area interest turn grob object using ggplotGrob()combine two using annotation_custom()Now two maps, can now put map using annotation_custom(). first task initiate ggplot coord_equal() follows:now blank canvas put images . Let’s add layer canvas using annotation_custom() provide grob object (map) specify range canvas map occupies. Since extent x y set [0, 1] coord_equal(xlim = c(0, 1), ylim = c(0, 1), expand = FALSE), following code put grob_aoi cover entire y range 0.8 x 0.Similarly, can add grob_region using annotation_custom(). Let’s put right lower corner map.Note resulting map still default theme inherit theme maps added annotation_custom(). , can add theme_void() map make border disappear.","code":"\nthree_counties <- filter(KS_county, NAME %in% c(\"Cheyenne\", \"Sherman\", \"Wallace\"))\n\n(\n  g_three_counties <- ggplot() +\n    geom_sf(data = three_counties) +\n    geom_sf_text(data = three_counties, aes(label = NAME)) +\n    theme_void()\n)\n#--- convert the ggplot into a grob ---#\ngrob_aoi <- ggplotGrob(g_three_counties)\n\n#--- check the class ---#\nclass(grob_aoi)[1] \"gtable\" \"gTree\"  \"grob\"   \"gDesc\" \n#--- create a map of Kansas ---#\ng_region <- ggplot() +\n  geom_sf(data = KS_county) +\n  geom_sf(data = three_counties, fill = \"blue\", color = \"red\", alpha = 0.5) +\n  theme_void()\n\n#--- convert to a grob ---#\ngrob_region <- ggplotGrob(g_region)\n(\n  g_inset <- ggplot() +\n    coord_equal(xlim = c(0, 1), ylim = c(0, 1), expand = FALSE)\n)\ng_inset +\n  annotation_custom(grob_aoi,\n    xmin = 0, xmax = 0.8, ymin = 0,\n    ymax = 1\n  )\ng_inset +\n  annotation_custom(grob_aoi,\n    xmin = 0, xmax = 0.8, ymin = 0,\n    ymax = 1\n  ) +\n  annotation_custom(grob_region,\n    xmin = 0.6, xmax = 1, ymin = 0,\n    ymax = 0.3\n  )\ng_inset +\n  annotation_custom(grob_aoi,\n    xmin = 0, xmax = 0.8, ymin = 0,\n    ymax = 1\n  ) +\n  annotation_custom(grob_region,\n    xmin = 0.6, xmax = 1, ymin = 0,\n    ymax = 0.3\n  ) +\n  theme_void()"},{"path":"create-maps.html","id":"fine-tune","chapter":"8 Creating Maps using ggplot2","heading":"8.5 Fine-tuning maps for publication","text":"section presents number small tips beautify maps can publishable professional reports. Often times, academic journals particular sets rules figures, reviewers boss might views maps look like. Whatever requirements requests, need accommodate requests modify maps accordingly. worth mentioning really nothing specific creating maps . Techniques presented applicable kind figure. just limit specific components figure likely want modify default creating maps. Specifically, pre-made ggplot2 themes modify legends facet strips discussed.","code":"\n(\n  gw_by_county <- st_join(KS_county, gw_KS_sf) %>%\n    data.table() %>%\n    .[, .(af_used = sum(af_used, na.rm = TRUE)), by = .(COUNTYFP, year)] %>%\n    left_join(KS_county, ., by = c(\"COUNTYFP\")) %>%\n    filter(!is.na(year))\n)Simple feature collection with 184 features and 14 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -102.0517 ymin: 36.99302 xmax: -94.58841 ymax: 40.00316\nGeodetic CRS:  NAD83\nFirst 10 features:\n   STATEFP COUNTYFP COUNTYNS       AFFGEOID GEOID         NAME\n1       20      075 00485327 0500000US20075 20075     Hamilton\n2       20      075 00485327 0500000US20075 20075     Hamilton\n3       20      149 00485038 0500000US20149 20149 Pottawatomie\n4       20      149 00485038 0500000US20149 20149 Pottawatomie\n5       20      033 00484986 0500000US20033 20033     Comanche\n6       20      033 00484986 0500000US20033 20033     Comanche\n7       20      189 00485056 0500000US20189 20189      Stevens\n8       20      189 00485056 0500000US20189 20189      Stevens\n9       20      161 00485044 0500000US20161 20161        Riley\n10      20      161 00485044 0500000US20161 20161        Riley\n              NAMELSAD STUSPS STATE_NAME LSAD      ALAND   AWATER year\n1      Hamilton County     KS     Kansas   06 2580958328  2893322 2010\n2      Hamilton County     KS     Kansas   06 2580958328  2893322 2011\n3  Pottawatomie County     KS     Kansas   06 2177507162 54149295 2010\n4  Pottawatomie County     KS     Kansas   06 2177507162 54149295 2011\n5      Comanche County     KS     Kansas   06 2041681089  3604155 2010\n6      Comanche County     KS     Kansas   06 2041681089  3604155 2011\n7       Stevens County     KS     Kansas   06 1883593926   464936 2010\n8       Stevens County     KS     Kansas   06 1883593926   464936 2011\n9         Riley County     KS     Kansas   06 1579116499 32002514 2010\n10        Riley County     KS     Kansas   06 1579116499 32002514 2011\n      af_used                       geometry\n1   37884.381 MULTIPOLYGON (((-102.0446 3...\n2   44854.827 MULTIPOLYGON (((-102.0446 3...\n3    3385.149 MULTIPOLYGON (((-96.72833 3...\n4    9694.107 MULTIPOLYGON (((-96.72833 3...\n5    8140.857 MULTIPOLYGON (((-99.54467 3...\n6   10533.399 MULTIPOLYGON (((-99.54467 3...\n7  218488.630 MULTIPOLYGON (((-101.5566 3...\n8  296955.428 MULTIPOLYGON (((-101.5566 3...\n9    1472.197 MULTIPOLYGON (((-96.96095 3...\n10   3365.272 MULTIPOLYGON (((-96.96095 3...\n(\n  g_base <- ggplot() +\n    geom_sf(data = gw_by_county, aes(fill = af_used)) +\n    facet_wrap(. ~ year)\n)"},{"path":"create-maps.html","id":"theme","chapter":"8 Creating Maps using ggplot2","heading":"8.5.1 Setting the theme","text":"Right now, map shows geographic coordinates, gray background, grid lines. aesthetically appealing. Adding pre-defined theme theme_*() can alter theme map quickly. One themes suitable maps theme_void().can see, axes information (axis.title, axis.ticks, axis.text,) panel information (panel.background, panel.border, panel.grid) gone among parts figure. can confirm evaluating theme_void().Applying theme map obviates need suppress parts figure individually. can suppress parts figure individually using theme(). example, following code gets rid axis.text., theme_void() overdoing things, can build theme specifically maps. example, theme used maps Chapter 1.Applying theme map:similar theme_void() except strip.background lost.can use theme_void() starting point override components like .theme_bw() also good theme maps.like gray grid lines, can remove like .themes suitable maps. example, theme_classic() good option can see :satisfied theme_void() willing make theme, may want take look pre-made themes available ggplot2 (see ) ggthemes (see ). Note themes invasive theme_void(), altering default color scale.","code":"\ng_base + theme_void()\ntheme_void()List of 92\n $ line                      : list()\n  ..- attr(*, \"class\")= chr [1:2] \"element_blank\" \"element\"\n $ rect                      : list()\n  ..- attr(*, \"class\")= chr [1:2] \"element_blank\" \"element\"\n $ text                      :List of 11\n  ..$ family       : chr \"\"\n  ..$ face         : chr \"plain\"\n  ..$ colour       : chr \"black\"\n  ..$ size         : num 11\n  ..$ hjust        : num 0.5\n  ..$ vjust        : num 0.5\n  ..$ angle        : num 0\n  ..$ lineheight   : num 0.9\n  ..$ margin       : 'margin' num [1:4] 0points 0points 0points 0points\n  .. ..- attr(*, \"unit\")= int 8\n  ..$ debug        : logi FALSE\n  ..$ inherit.blank: logi TRUE\n  ..- attr(*, \"class\")= chr [1:2] \"element_text\" \"element\"\n $ title                     : NULL\n $ aspect.ratio              : NULL\n $ axis.title                : list()\n  ..- attr(*, \"class\")= chr [1:2] \"element_blank\" \"element\"\n $ axis.title.x              : NULL\n $ axis.title.x.top          : NULL\n $ axis.title.x.bottom       : NULL\n $ axis.title.y              : NULL\n $ axis.title.y.left         : NULL\n $ axis.title.y.right        : NULL\n $ axis.text                 : list()\n  ..- attr(*, \"class\")= chr [1:2] \"element_blank\" \"element\"\n $ axis.text.x               : NULL\n $ axis.text.x.top           : NULL\n $ axis.text.x.bottom        : NULL\n $ axis.text.y               : NULL\n $ axis.text.y.left          : NULL\n $ axis.text.y.right         : NULL\n $ axis.ticks                : NULL\n $ axis.ticks.x              : NULL\n $ axis.ticks.x.top          : NULL\n $ axis.ticks.x.bottom       : NULL\n $ axis.ticks.y              : NULL\n $ axis.ticks.y.left         : NULL\n $ axis.ticks.y.right        : NULL\n $ axis.ticks.length         : 'simpleUnit' num 0points\n  ..- attr(*, \"unit\")= int 8\n $ axis.ticks.length.x       : NULL\n $ axis.ticks.length.x.top   : NULL\n $ axis.ticks.length.x.bottom: NULL\n $ axis.ticks.length.y       : NULL\n $ axis.ticks.length.y.left  : NULL\n $ axis.ticks.length.y.right : NULL\n $ axis.line                 : NULL\n $ axis.line.x               : NULL\n $ axis.line.x.top           : NULL\n $ axis.line.x.bottom        : NULL\n $ axis.line.y               : NULL\n $ axis.line.y.left          : NULL\n $ axis.line.y.right         : NULL\n $ legend.background         : NULL\n $ legend.margin             : NULL\n $ legend.spacing            : NULL\n $ legend.spacing.x          : NULL\n $ legend.spacing.y          : NULL\n $ legend.key                : NULL\n $ legend.key.size           : 'simpleUnit' num 1.2lines\n  ..- attr(*, \"unit\")= int 3\n $ legend.key.height         : NULL\n $ legend.key.width          : NULL\n $ legend.text               :List of 11\n  ..$ family       : NULL\n  ..$ face         : NULL\n  ..$ colour       : NULL\n  ..$ size         : 'rel' num 0.8\n  ..$ hjust        : NULL\n  ..$ vjust        : NULL\n  ..$ angle        : NULL\n  ..$ lineheight   : NULL\n  ..$ margin       : NULL\n  ..$ debug        : NULL\n  ..$ inherit.blank: logi TRUE\n  ..- attr(*, \"class\")= chr [1:2] \"element_text\" \"element\"\n $ legend.text.align         : NULL\n $ legend.title              :List of 11\n  ..$ family       : NULL\n  ..$ face         : NULL\n  ..$ colour       : NULL\n  ..$ size         : NULL\n  ..$ hjust        : num 0\n  ..$ vjust        : NULL\n  ..$ angle        : NULL\n  ..$ lineheight   : NULL\n  ..$ margin       : NULL\n  ..$ debug        : NULL\n  ..$ inherit.blank: logi TRUE\n  ..- attr(*, \"class\")= chr [1:2] \"element_text\" \"element\"\n $ legend.title.align        : NULL\n $ legend.position           : chr \"right\"\n $ legend.direction          : NULL\n $ legend.justification      : NULL\n $ legend.box                : NULL\n $ legend.box.just           : NULL\n $ legend.box.margin         : NULL\n $ legend.box.background     : NULL\n $ legend.box.spacing        : NULL\n $ panel.background          : NULL\n $ panel.border              : NULL\n $ panel.spacing             : 'simpleUnit' num 5.5points\n  ..- attr(*, \"unit\")= int 8\n $ panel.spacing.x           : NULL\n $ panel.spacing.y           : NULL\n $ panel.grid                : NULL\n $ panel.grid.major          : NULL\n $ panel.grid.minor          : NULL\n $ panel.grid.major.x        : NULL\n $ panel.grid.major.y        : NULL\n $ panel.grid.minor.x        : NULL\n $ panel.grid.minor.y        : NULL\n $ panel.ontop               : logi FALSE\n $ plot.background           : NULL\n $ plot.title                :List of 11\n  ..$ family       : NULL\n  ..$ face         : NULL\n  ..$ colour       : NULL\n  ..$ size         : 'rel' num 1.2\n  ..$ hjust        : num 0\n  ..$ vjust        : num 1\n  ..$ angle        : NULL\n  ..$ lineheight   : NULL\n  ..$ margin       : 'margin' num [1:4] 5.5points 0points 0points 0points\n  .. ..- attr(*, \"unit\")= int 8\n  ..$ debug        : NULL\n  ..$ inherit.blank: logi TRUE\n  ..- attr(*, \"class\")= chr [1:2] \"element_text\" \"element\"\n $ plot.title.position       : chr \"panel\"\n $ plot.subtitle             :List of 11\n  ..$ family       : NULL\n  ..$ face         : NULL\n  ..$ colour       : NULL\n  ..$ size         : NULL\n  ..$ hjust        : num 0\n  ..$ vjust        : num 1\n  ..$ angle        : NULL\n  ..$ lineheight   : NULL\n  ..$ margin       : 'margin' num [1:4] 5.5points 0points 0points 0points\n  .. ..- attr(*, \"unit\")= int 8\n  ..$ debug        : NULL\n  ..$ inherit.blank: logi TRUE\n  ..- attr(*, \"class\")= chr [1:2] \"element_text\" \"element\"\n $ plot.caption              :List of 11\n  ..$ family       : NULL\n  ..$ face         : NULL\n  ..$ colour       : NULL\n  ..$ size         : 'rel' num 0.8\n  ..$ hjust        : num 1\n  ..$ vjust        : num 1\n  ..$ angle        : NULL\n  ..$ lineheight   : NULL\n  ..$ margin       : 'margin' num [1:4] 5.5points 0points 0points 0points\n  .. ..- attr(*, \"unit\")= int 8\n  ..$ debug        : NULL\n  ..$ inherit.blank: logi TRUE\n  ..- attr(*, \"class\")= chr [1:2] \"element_text\" \"element\"\n $ plot.caption.position     : chr \"panel\"\n $ plot.tag                  :List of 11\n  ..$ family       : NULL\n  ..$ face         : NULL\n  ..$ colour       : NULL\n  ..$ size         : 'rel' num 1.2\n  ..$ hjust        : num 0.5\n  ..$ vjust        : num 0.5\n  ..$ angle        : NULL\n  ..$ lineheight   : NULL\n  ..$ margin       : NULL\n  ..$ debug        : NULL\n  ..$ inherit.blank: logi TRUE\n  ..- attr(*, \"class\")= chr [1:2] \"element_text\" \"element\"\n $ plot.tag.position         : chr \"topleft\"\n $ plot.margin               : 'simpleUnit' num [1:4] 0lines 0lines 0lines 0lines\n  ..- attr(*, \"unit\")= int 3\n $ strip.background          : NULL\n $ strip.background.x        : NULL\n $ strip.background.y        : NULL\n $ strip.placement           : NULL\n $ strip.text                :List of 11\n  ..$ family       : NULL\n  ..$ face         : NULL\n  ..$ colour       : NULL\n  ..$ size         : 'rel' num 0.8\n  ..$ hjust        : NULL\n  ..$ vjust        : NULL\n  ..$ angle        : NULL\n  ..$ lineheight   : NULL\n  ..$ margin       : NULL\n  ..$ debug        : NULL\n  ..$ inherit.blank: logi TRUE\n  ..- attr(*, \"class\")= chr [1:2] \"element_text\" \"element\"\n $ strip.text.x              : NULL\n $ strip.text.y              : NULL\n $ strip.switch.pad.grid     : 'simpleUnit' num 2.75points\n  ..- attr(*, \"unit\")= int 8\n $ strip.switch.pad.wrap     : 'simpleUnit' num 2.75points\n  ..- attr(*, \"unit\")= int 8\n - attr(*, \"class\")= chr [1:2] \"theme\" \"gg\"\n - attr(*, \"complete\")= logi TRUE\n - attr(*, \"validate\")= logi TRUE\ng_base + theme(axis.text = element_blank())\ntheme_for_map <-\n  theme(\n    axis.ticks = element_blank(),\n    axis.text = element_blank(),\n    axis.line = element_blank(),\n    panel.border = element_blank(),\n    panel.grid = element_line(color = \"transparent\"),\n    panel.background = element_blank(),\n    plot.background = element_rect(fill = \"transparent\", color = \"transparent\")\n  )\ng_base + theme_for_map\n#--- bring back a color to strip.background  ---#\ntheme_for_map_2 <- theme_void() + theme(strip.background = element_rect(fill = \"gray\"))\n\n#--- apply the new theme ---#\ng_base + theme_for_map_2\nggplot() +\n  geom_sf(data = gw_by_county, aes(fill = af_used)) +\n  facet_grid(year ~ .) +\n  theme_bw()\nggplot() +\n  geom_sf(data = gw_by_county, aes(fill = af_used)) +\n  facet_grid(year ~ .) +\n  theme_bw() +\n  theme(\n    panel.grid = element_line(color = \"transparent\")\n  )\ng_base + theme_classic()"},{"path":"create-maps.html","id":"legend","chapter":"8 Creating Maps using ggplot2","heading":"8.5.2 Legend","text":"Legends can modified using legend.*() options theme() guide_*(). impossible discuss every single one options functions. , section focuses common useful (consider) modifications can make legends.legend consists three elements: legend title, legend key (e.g., color bar), legend label (legend text). example, figure , af_used legend title, color bar legend key, numbers color bar legend labels. Knowing name elements helps name options contains name specific part legend.Let’s first change color scale Viridis using scale_fill_viridis_c() (see section 8.3 picking color scale).Right now, legend title af_used, tell readers means. general, can change title legend using name option inside scale function legend (, scale_fill_viridis_c()). , one works:Alternatively, can use labs() function.88. Since legend fill aesthetic attribute, add fill = \"legend title\" follows:Since legend title long, legend taking half space entire figure. , let’s put legend maps (bottom figure) adding theme(legend.position = \"bottom\").aesthetically better legend title top color bar. can done using guides() function. Since like alter aesthetics legend fill involving color bar, use fill = guide_colorbar(). place legend title top, add title.position=\"top\" inside guide_colorbar() function follows:looks better. , legend labels close hard read color bar short. Let’s elongate color bar enough space legend labels using legend.key.width = option theme(). Let’s also make legend thinner using legend.key.height = option.journal submitting article requesting specific font family texts figure, can use legend.text = element_text() legend.title = element_text() inside theme() legend labels legend title, respectively. following code uses font family “Times” font size 12 labels title.options control legend color bar, see .legend made discrete values, can use guide_legend(). Let’s use following map starting point.legend long, first put legend title top legend labels using code :Since legend color aesthetic attribute, color = guide_legend() used. legend labels still bit long, let’s arrange two rows using nrow = option.options guide_legend(), see .","code":"\n(\n  g_legend <- ggplot() +\n    geom_sf(data = gw_by_county, aes(fill = af_used)) +\n    facet_wrap(. ~ year) +\n    theme_void()\n)\ng_legend +\n  scale_fill_viridis_c()\ng_legend +\n  scale_fill_viridis_c(name = \"Groundwater pumping (acre-feet)\")\ng_legend +\n  scale_fill_viridis_c() +\n  labs(fill = \"Groundwater pumping (acre-feet)\")\ng_legend +\n  scale_fill_viridis_c() +\n  labs(fill = \"Groundwater pumping (acre-feet)\") +\n  theme(legend.position = \"bottom\")\ng_legend +\n  scale_fill_viridis_c() +\n  labs(fill = \"Groundwater pumping (acre-feet)\") +\n  theme(legend.position = \"bottom\") +\n  guides(fill = guide_colorbar(title.position = \"top\"))\ng_legend +\n  scale_fill_viridis_c() +\n  labs(fill = \"Groundwater pumping (acre-feet)\") +\n  theme(\n    legend.position = \"bottom\",\n    #--- NEW LINES HERE!! ---#\n    legend.key.height = unit(0.5, \"cm\"),\n    legend.key.width = unit(2, \"cm\")\n  ) +\n  guides(fill = guide_colorbar(title.position = \"top\"))\ng_legend +\n  scale_fill_viridis_c() +\n  labs(fill = \"Groundwater pumping (acre-feet)\") +\n  theme(\n    legend.position = \"bottom\",\n    legend.key.height = unit(0.5, \"cm\"),\n    legend.key.width = unit(2, \"cm\"),\n    legend.text = element_text(size = 12, family = \"Times\"),\n    legend.title = element_text(size = 12, family = \"Times\")\n    #--- NEW LINES HERE!! ---#\n  ) +\n  guides(fill = guide_colorbar(title.position = \"top\"))\n#--- convert af_used to a discrete variable ---#\ngw_Stevens <- mutate(gw_Stevens, af_used_cat = cut_number(af_used, n = 5))\n(\n  g_legend_2 <-\n    ggplot(data = gw_Stevens) +\n    geom_sf(aes(color = af_used_cat), size = 2) +\n    scale_color_viridis(discrete = TRUE, option = \"C\") +\n    labs(color = \"Groundwater pumping (acre-feet)\") +\n    theme_void() +\n    theme(legend.position = \"bottom\")\n)\ng_legend_2 +\n  guides(\n    color = guide_legend(title.position = \"top\")\n  )\ng_legend_2 +\n  guides(\n    color = guide_legend(title.position = \"top\", nrow = 2)\n  )"},{"path":"create-maps.html","id":"facet-strips","chapter":"8 Creating Maps using ggplot2","heading":"8.5.3 Facet strips","text":"Facet strips refer area boxed values faceting variables printed. figure , ’s gray strips top maps. can change look using strip.* options theme() also partially inside facet_wrap() facet_grid(). list available options:strip.background, strip.background.x, strip.background.ystrip.placementstrip.text, strip.text.x, strip.text.ystrip.switch.pad.gridstrip.switch.pad.wrapTo make texts strips descriptive actually mean can make variable texts want show map values.probably noticed high water use cases now appear top. panels figures arranged way strip texts alphabetically ordered. High water use precedes Low water use. Sometimes, desirable. force specific order, can turn faceting variable (high_low) factor order values defined using levels =. following code converts high_low factor “Low water use” first level “High water use” second level.Now, “Low water use” cases appear first (top).can control strip texts strip boxes appear using strip.text strip.background options. example:Instead descriptions cases top figures, one descriptions right side figures using facet_grid().Now case descriptions high_low long squeezing space maps. Let’s flip high_low year.slightly better , much. Let’s rotate strip texts year using angle = option.Since want change angle strip texts second faceting variable, need work strip.text.y (want work first one, use strip.text.x.).Let’s change size strip texts 12 use Times font family.strip texts high_low close maps letter “g” “High” truncated. Let’s move .Now upper part letters truncated. just put margin texts using margin = margin(top, right, bottom, left, unit text) option.completeness, let’s make legend look better well (discussed section 8.5.1).Alright, setting aside problem whether information provided maps meaningful , maps look great least.","code":"\nggplot() +\n  #--- KS county boundary ---#\n  geom_sf(data = st_transform(KS_county, 32614)) +\n  #--- wells ---#\n  geom_sf(data = gw_KS_sf, aes(color = af_used)) +\n  #--- facet by year (side by side) ---#\n  facet_wrap((af_used > 500) ~ year) +\n  theme_void() +\n  scale_color_viridis_c() +\n  theme(legend.position = \"bottom\")\ngw_KS_sf <-\n  gw_KS_sf %>%\n  mutate(\n    high_low = ifelse(af_used > 500, \"High water use\", \"Low water use\"),\n    year_txt = paste(\"Year: \", year)\n  )\n(\n  g_facet <-\n    ggplot() +\n    #--- KS county boundary ---#\n    geom_sf(data = st_transform(KS_county, 32614)) +\n    #--- wells ---#\n    geom_sf(data = gw_KS_sf, aes(color = af_used)) +\n    #--- facet by year (side by side) ---#\n    facet_wrap(high_low ~ year_txt) +\n    theme_void() +\n    scale_color_viridis_c() +\n    theme(legend.position = \"bottom\")\n)\ngw_KS_sf <-\n  mutate(\n    gw_KS_sf,\n    high_low = factor(\n      high_low,\n      levels = c(\"Low water use\", \"High water use\")\n    )\n  )\ng_facet <- \n  ggplot() +\n  #--- KS county boundary ---#\n  geom_sf(data = st_transform(KS_county, 32614)) +\n  #--- wells ---#\n  geom_sf(data = gw_KS_sf, aes(color = af_used)) +\n  #--- facet by year (side by side) ---#\n  facet_wrap(high_low ~ year_txt) +\n  theme_void() +\n  scale_color_viridis_c() +\n  theme(legend.position = \"bottom\")\n\ng_facet\ng_facet + theme(\n  strip.text.x = element_text(size = 12, family = \"Times\", color = \"blue\"),\n  strip.background = element_rect(fill = \"red\", color = \"black\")\n)\ng_facet +\n  #--- this overrides facet_wrap(high_low ~ year_txt) ---#\n  facet_grid(high_low ~ year_txt)\ng_facet +\n  #--- this overrides facet_grid(high_low ~ year_txt) ---#\n  facet_grid(year_txt ~ high_low)\ng_facet +\n  #--- this overrides facet_grid(high_low ~ year_txt) ---#\n  facet_grid(year_txt ~ high_low) +\n  theme(\n    strip.text.y = element_text(angle = -90)\n  )\ng_facet +\n  #--- this overrides facet_grid(high_low ~ year_txt) ---#\n  facet_grid(year_txt ~ high_low) +\n  theme(\n    strip.text.y = element_text(angle = -90, size = 12, family = \"Times\"),\n    #--- moves up the strip texts ---#\n    strip.text.x = element_text(size = 12, family = \"Times\")\n  )\ng_facet +\n  #--- this overrides facet_grid(high_low ~ year_txt) ---#\n  facet_grid(year_txt ~ high_low) +\n  theme(\n    strip.text.y = element_text(angle = -90, size = 12, family = \"Times\"),\n    #--- moves up the strip texts ---#\n    strip.text.x = element_text(vjust = 2, size = 12, family = \"Times\")\n  )\ng_facet +\n  #--- this overrides facet_grid(high_low ~ year_txt) ---#\n  facet_grid(year_txt ~ high_low) +\n  theme(\n    strip.text.y = element_text(angle = -90, size = 12, family = \"Times\"),\n    strip.text.x = element_text(margin = margin(0, 0, 0.2, 0, \"cm\"), size = 12, family = \"Times\")\n  )\n(\n  g_facet <- g_facet +\n    #--- this overrides facet_grid(high_low ~ year_txt) ---#\n    facet_grid(year_txt ~ high_low) +\n    theme(\n      strip.text.y = element_text(angle = -90, size = 12, family = \"Times\"),\n      strip.text.x = element_text(margin = margin(0, 0, 0.2, 0, \"cm\"), size = 12, family = \"Times\")\n    ) +\n    theme(legend.position = \"bottom\") +\n    labs(color = \"Groundwater pumping (acre-feet)\") +\n    theme(\n      legend.position = \"bottom\",\n      legend.key.height = unit(0.5, \"cm\"),\n      legend.key.width = unit(2, \"cm\"),\n      legend.text = element_text(size = 12, family = \"Times\"),\n      legend.title = element_text(size = 12, family = \"Times\")\n      #--- NEW LINES HERE!! ---#\n    ) +\n    guides(color = guide_colorbar(title.position = \"top\"))\n)"},{"path":"create-maps.html","id":"north-arrow-and-scale-bar","chapter":"8 Creating Maps using ggplot2","heading":"8.5.4 North arrow and scale bar","text":"ggspatial package lets put north arrow scale bar map using annotation_scale() annotation_north_arrow().example code adds scale bar:location determines scale bar . first letter either t (top) b (bottom), second letter either l (left) r (right). width_hint length scale bar relative plot. distance number (200 km) generated automatically according length bar.can add pads plot border fine tune location scale bar:positive number means scale bar placed away closest border plot.can add north arrow using annotation_north_arrow(). Accepted arguments similar annotation_scale().several styles can pick . Run ?north_arrow_orienteering see options.","code":"\n#--- get North Carolina county borders ---#\nnc <- st_read(system.file(\"shape/nc.shp\", package = \"sf\"))Reading layer `nc' from data source \n  `/Library/Frameworks/R.framework/Versions/4.1/Resources/library/sf/shape/nc.shp' \n  using driver `ESRI Shapefile'\nSimple feature collection with 100 features and 14 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -84.32385 ymin: 33.88199 xmax: -75.45698 ymax: 36.58965\nGeodetic CRS:  NAD27\n(\n  #--- create a simple map ---#\n  g_nc <- ggplot(nc) +\n    geom_sf() +\n    theme_void()\n)\n#--- load ggspatial ---#\nlibrary(ggspatial)\n\n#--- add scale bar ---#\ng_nc +\n  annotation_scale(\n    location = \"bl\",\n    width_hint = 0.2\n  )\ng_nc +\n  annotation_scale(\n    location = \"bl\",\n    width_hint = 0.2,\n    pad_x = unit(3, \"cm\"),\n    pad_y = unit(2, \"cm\")\n  )\ng_nc +\n  annotation_scale(\n    location = \"bl\",\n    width_hint = 0.2,\n    pad_x = unit(3, \"cm\")\n  ) +\n  #--- add north arrow ---#\n  annotation_north_arrow(\n    location = \"tl\",\n    pad_x = unit(0.5, \"in\"),\n    pad_y = unit(0.1, \"in\"),\n    style = north_arrow_fancy_orienteering\n  )"},{"path":"create-maps.html","id":"ggsave","chapter":"8 Creating Maps using ggplot2","heading":"8.6 Saving a ggplot object as an image","text":"Maps created ggplot2 can saved using ggsave() following syntax:Many different file formats supported including pdf, svg, eps, png, jpg, tif, etc. One thing want keep mind type graphics:vector graphics (pdf, svg, eps)raster graphics (jpg, png, tif)vector graphics scalable, raster graphics . enlarge raster graphics, cells making figure become visible, making figure unappealing. , unless required save figures raster graphics, encouraged save figures vector graphics.89Let’s try save following ggplot object.ggsave() automatically detects file format file name. example, following code saves g_nc nc.pdf. ggsave() knows ggplot object intended saved pdf file extension specified file name.Similarly,can change output size height width options. example, following code creates pdf file height = 5 inches width = 7 inches.change unit units option. default, (inches) used.can control resolution output image specifying DPI (dots per inch) using dpi option. default DPI value 300, can specify value suitable output image, including “retina” (320) “screen” (72). 600 higher recommended high resolution output required.","code":"ggsave(filename = file name, plot = ggplot object)\n\n#--- or just this ---#\nggsave(file name, ggplot object)\n#--- get North Carolina county borders ---#\nnc <- st_read(system.file(\"shape/nc.shp\", package = \"sf\"))Reading layer `nc' from data source \n  `/Library/Frameworks/R.framework/Versions/4.1/Resources/library/sf/shape/nc.shp' \n  using driver `ESRI Shapefile'\nSimple feature collection with 100 features and 14 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -84.32385 ymin: 33.88199 xmax: -75.45698 ymax: 36.58965\nGeodetic CRS:  NAD27\n(\n  #--- create a map ---#\n  g_nc <- ggplot(nc) +\n    geom_sf()\n)\nggsave(\"nc.pdf\", g_nc)\n#--- save as an eps file ---#\nggsave(\"nc.eps\", g_nc)\n\n#--- save as an eps file ---#\nggsave(\"nc.svg\", g_nc)\nggsave(\"nc.pdf\", g_nc, height = 5, width = 7)\n#--- dpi = 320 ---#\nggsave(\"nc_dpi_320.png\", g_nc, height = 5, width = 7, dpi = 320)\n\n#--- dpi = 72 ---#\nggsave(\"nc_dpi_screen.png\", g_nc, height = 5, width = 7, dpi = \"screen\")"},{"path":"download-data.html","id":"download-data","chapter":"9 Download and process spatial datasets from within R","heading":"9 Download and process spatial datasets from within R","text":"","code":""},{"path":"download-data.html","id":"before-you-start-8","chapter":"9 Download and process spatial datasets from within R","heading":"Before you start","text":"many publicly available spatial datasets can downloaded using R. Programming data downloading using R instead manually downloading data websites can save lots time also enhances reproducibility analysis. section, introduce datasets show download process data.","code":""},{"path":"download-data.html","id":"direction-for-replication-8","chapter":"9 Download and process spatial datasets from within R","heading":"Direction for replication","text":"DatasetsNo datasets download Chapter.PackagesRun following code install load (already installed) pacman package, install load (already installed) listed package inside pacman::p_load() function.Run following code define theme map:","code":"\nif (!require(\"pacman\")) install.packages(\"pacman\")\npacman::p_load(\n  stars, # spatiotemporal data handling\n  terra, # raster data handling\n  raster, # raster data handling\n  sf, # vector data handling\n  dplyr, # data wrangling\n  stringr, # string manipulation\n  lubridate, # dates handling\n  data.table, # data wrangling\n  tidyr, # reshape\n  tidyUSDA, # download USDA NASS data\n  keyring, # API key management\n  FedData, # download Daymet data\n  daymetr, # download Daymet data\n  ggplot2, # make maps\n  tmap, # make maps\n  future.apply, # parallel processing\n  CropScapeR, # download CDL data\n  prism, # download PRISM data\n  exactextractr # extract raster values to sf\n)  \ntheme_set(theme_bw())\n\ntheme_for_map <- theme(\n  axis.ticks = element_blank(),\n  axis.text= element_blank(), \n  axis.line = element_blank(),\n  panel.border = element_blank(),\n  panel.grid.major = element_line(color='transparent'),\n  panel.grid.minor = element_line(color='transparent'),\n  panel.background = element_blank(),\n  plot.background = element_rect(fill = \"transparent\",color='transparent')\n)  "},{"path":"download-data.html","id":"nass-quick","chapter":"9 Download and process spatial datasets from within R","heading":"9.1 USDA NASS QuickStat with tidyUSDA","text":"several packages lets download data USDA NASS QuickStat. use tidyUSDA package (Lindblad 2020). nice thing tidyUSDA gives option download data sf object, means can immediately visualize data spatially interact spatial objects.First thing want get API key website, need actually download data.can download data using getQuickstat(). number options can use narrow scope data downloading including data_item, geographic_level, year, commodity, . See manual full list parameters can set. example, code download corn-related data county Illinois year 2016 sf object.can see, sf object geometry column due geometry = TRUE option. means can immediately create map data (Figure 9.1):\nFigure 9.1: Corn Yield (bu/acre) Illinois 2016\ncan download data multiple states years time like (want data whole U.S., don’t specify state parameter).","code":"\n(\nIL_corn_yield <- \n  getQuickstat(\n    #--- put your API key in place of key_get() ---#\n    key = key_get(\"usda_nass_qs_api\"),\n    program = \"SURVEY\",\n    commodity = \"CORN\",\n    geographic_level = \"COUNTY\",\n    state = \"ILLINOIS\",\n    year = \"2016\",\n    geometry = TRUE\n  )  %>% \n  #--- keep only some of the variables ---#\n  dplyr::select(\n    year, county_name, county_code, state_name, \n    state_fips_code, short_desc, Value\n  )\n)Simple feature collection with 384 features and 7 fields (with 16 geometries empty)\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -91.51308 ymin: 36.9703 xmax: -87.01993 ymax: 42.50848\nGeodetic CRS:  NAD83\nFirst 10 features:\n   year county_name county_code state_name state_fips_code           short_desc\n1  2016      BUREAU         011   ILLINOIS              17 CORN - ACRES PLANTED\n2  2016     CARROLL         015   ILLINOIS              17 CORN - ACRES PLANTED\n3  2016       HENRY         073   ILLINOIS              17 CORN - ACRES PLANTED\n4  2016  JO DAVIESS         085   ILLINOIS              17 CORN - ACRES PLANTED\n5  2016         LEE         103   ILLINOIS              17 CORN - ACRES PLANTED\n6  2016      MERCER         131   ILLINOIS              17 CORN - ACRES PLANTED\n7  2016        OGLE         141   ILLINOIS              17 CORN - ACRES PLANTED\n8  2016      PUTNAM         155   ILLINOIS              17 CORN - ACRES PLANTED\n9  2016 ROCK ISLAND         161   ILLINOIS              17 CORN - ACRES PLANTED\n10 2016  STEPHENSON         177   ILLINOIS              17 CORN - ACRES PLANTED\n    Value                       geometry\n1  273500 MULTIPOLYGON (((-89.85691 4...\n2  147500 MULTIPOLYGON (((-90.16133 4...\n3  235000 MULTIPOLYGON (((-90.43247 4...\n4  100500 MULTIPOLYGON (((-90.50668 4...\n5  258500 MULTIPOLYGON (((-89.63118 4...\n6  142500 MULTIPOLYGON (((-90.99255 4...\n7  228000 MULTIPOLYGON (((-89.68598 4...\n8   37200 MULTIPOLYGON (((-89.33303 4...\n9   65000 MULTIPOLYGON (((-90.33573 4...\n10 179500 MULTIPOLYGON (((-89.92054 4...\nggplot() +\n  geom_sf(\n    data = filter(IL_corn_yield, short_desc == \"CORN, GRAIN - YIELD, MEASURED IN BU / ACRE\"), \n    aes(fill = Value)\n  ) +\n  theme_for_map\n(\nIL_CO_NE_corn <- \n  getQuickstat(\n    key = key_get(\"usda_nass_qs_api\"),\n    program = \"SURVEY\",\n    commodity = \"CORN\",\n    geographic_level = \"COUNTY\",\n    state = c(\"ILLINOIS\", \"COLORADO\", \"NEBRASKA\"),\n    year = paste(2014:2018),\n    geometry = TRUE\n  ) %>% \n  #--- keep only some of the variables ---#\n  dplyr::select(\n    year, county_name, county_code, state_name, \n    state_fips_code, short_desc, Value\n  )\n)Simple feature collection with 6384 features and 7 fields (with 588 geometries empty)\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -109.0459 ymin: 36.9703 xmax: -87.01993 ymax: 43.00171\nGeodetic CRS:  NAD83\nFirst 10 features:\n   year county_name county_code state_name state_fips_code           short_desc\n1  2017     BOULDER         013   COLORADO              08 CORN - ACRES PLANTED\n2  2016     BOULDER         013   COLORADO              08 CORN - ACRES PLANTED\n3  2016     LARIMER         069   COLORADO              08 CORN - ACRES PLANTED\n4  2015     LARIMER         069   COLORADO              08 CORN - ACRES PLANTED\n5  2014     LARIMER         069   COLORADO              08 CORN - ACRES PLANTED\n6  2015       LOGAN         075   COLORADO              08 CORN - ACRES PLANTED\n7  2014       LOGAN         075   COLORADO              08 CORN - ACRES PLANTED\n8  2018      MORGAN         087   COLORADO              08 CORN - ACRES PLANTED\n9  2017      MORGAN         087   COLORADO              08 CORN - ACRES PLANTED\n10 2015      MORGAN         087   COLORADO              08 CORN - ACRES PLANTED\n   Value                       geometry\n1   3000 MULTIPOLYGON (((-105.6486 4...\n2   3300 MULTIPOLYGON (((-105.6486 4...\n3  12800 MULTIPOLYGON (((-105.8225 4...\n4  14900 MULTIPOLYGON (((-105.8225 4...\n5  13600 MULTIPOLYGON (((-105.8225 4...\n6  74500 MULTIPOLYGON (((-103.5737 4...\n7  86100 MULTIPOLYGON (((-103.5737 4...\n8  78300 MULTIPOLYGON (((-103.7148 4...\n9  78200 MULTIPOLYGON (((-103.7148 4...\n10 60200 MULTIPOLYGON (((-103.7148 4..."},{"path":"download-data.html","id":"look-for-parameter-values","chapter":"9 Download and process spatial datasets from within R","heading":"9.1.1 Look for parameter values","text":"package function lets see possible parameter values can use many parameters. example, suppose know like irrigated corn yields Colorado, sure parameter value (string) supply data_item parameter. , can :90You can use key words like “CORN,” “YIELD,” “IRRIGATED” narrow entire list using grep()91:Looking list, know exact text value want, first entry vector.complete list functions gives possible values parameters getQuickstat().","code":"\n#--- get all the possible values for data_item ---#\nall_items <- tidyUSDA::allDataItem \n\n#--- take a look at the first six ---#\nhead(all_items)                                                                           short_desc1 \n                                                                     \"AG LAND - ACRES\" \n                                                                           short_desc2 \n                                                      \"AG LAND - NUMBER OF OPERATIONS\" \n                                                                           short_desc3 \n                                                   \"AG LAND - OPERATIONS WITH TREATED\" \n                                                                           short_desc4 \n                                                \"AG LAND - TREATED, MEASURED IN ACRES\" \n                                                                           short_desc5 \n                           \"AG LAND, (EXCL CROPLAND & PASTURELAND & WOODLAND) - ACRES\" \n                                                                           short_desc6 \n\"AG LAND, (EXCL CROPLAND & PASTURELAND & WOODLAND) - AREA, MEASURED IN PCT OF AG LAND\" \nall_items %>% \n  grep(pattern = \"CORN\", ., value = TRUE) %>% \n  grep(pattern = \"YIELD\", ., value = TRUE) %>% \n  grep(pattern = \"IRRIGATED\", ., value = TRUE)                                                            short_desc9227 \n                 \"CORN, GRAIN, IRRIGATED - YIELD, MEASURED IN BU / ACRE\" \n                                                          short_desc9228 \n     \"CORN, GRAIN, IRRIGATED - YIELD, MEASURED IN BU / NET PLANTED ACRE\" \n                                                          short_desc9233 \n    \"CORN, GRAIN, IRRIGATED, ENTIRE CROP - YIELD, MEASURED IN BU / ACRE\" \n                                                          short_desc9236 \n   \"CORN, GRAIN, IRRIGATED, NONE OF CROP - YIELD, MEASURED IN BU / ACRE\" \n                                                          short_desc9238 \n   \"CORN, GRAIN, IRRIGATED, PART OF CROP - YIELD, MEASURED IN BU / ACRE\" \n                                                          short_desc9249 \n             \"CORN, GRAIN, NON-IRRIGATED - YIELD, MEASURED IN BU / ACRE\" \n                                                          short_desc9250 \n \"CORN, GRAIN, NON-IRRIGATED - YIELD, MEASURED IN BU / NET PLANTED ACRE\" \n                                                          short_desc9291 \n              \"CORN, SILAGE, IRRIGATED - YIELD, MEASURED IN TONS / ACRE\" \n                                                          short_desc9296 \n \"CORN, SILAGE, IRRIGATED, ENTIRE CROP - YIELD, MEASURED IN TONS / ACRE\" \n                                                          short_desc9299 \n\"CORN, SILAGE, IRRIGATED, NONE OF CROP - YIELD, MEASURED IN TONS / ACRE\" \n                                                          short_desc9301 \n\"CORN, SILAGE, IRRIGATED, PART OF CROP - YIELD, MEASURED IN TONS / ACRE\" \n                                                          short_desc9307 \n          \"CORN, SILAGE, NON-IRRIGATED - YIELD, MEASURED IN TONS / ACRE\" \n                                                         short_desc28557 \n                 \"SWEET CORN, IRRIGATED - YIELD, MEASURED IN CWT / ACRE\" \n                                                         short_desc28564 \n             \"SWEET CORN, NON-IRRIGATED - YIELD, MEASURED IN CWT / ACRE\" \n(\nCO_ir_corn_yield <- \n  getQuickstat(\n    key = key_get(\"usda_nass_qs_api\"),\n    program = \"SURVEY\",\n    data_item = \"CORN, GRAIN, IRRIGATED - YIELD, MEASURED IN BU / ACRE\",\n    geographic_level = \"COUNTY\",\n    state = \"COLORADO\",\n    year = \"2018\",\n    geometry = TRUE\n  ) %>% \n  #--- keep only some of the variables ---#\n  dplyr::select(year, NAME, county_code, short_desc, Value)\n)\ntidyUSDA::allCategory \ntidyUSDA::allSector \ntidyUSDA::allGroup \ntidyUSDA::allCommodity \ntidyUSDA::allDomain \ntidyUSDA::allCounty \ntidyUSDA::allProgram \ntidyUSDA::allDataItem \ntidyUSDA::allState \ntidyUSDA::allGeogLevel "},{"path":"download-data.html","id":"caveats","chapter":"9 Download and process spatial datasets from within R","heading":"9.1.2 Caveats","text":"retrieve \\(50,000\\) (limit set QuickStat) rows data. query requests much \\(50,000\\) observations, fail. case, need narrow search chop task smaller tasks.Another caveat query returns error observation satisfy query criteria. example, even though “CORN, GRAIN, IRRIGATED - YIELD, MEASURED BU / ACRE” one values can use data_item, entry statistic Illinois 2018. Therefore, following query fails.","code":"\nmany_states_corn <- getQuickstat(\n    key = key_get(\"usda_nass_qs_api\"),\n    program = \"SURVEY\",\n    commodity = \"CORN\",\n    geographic_level = \"COUNTY\",\n    state = c(\"ILLINOIS\", \"COLORADO\", \"NEBRASKA\", \"IOWA\", \"KANSAS\"),\n    year = paste(1995:2018),\n    geometry = TRUE\n  ) Error in UseMethod(\"left_join\"): no applicable method for 'left_join' applied to an object of class \"list\"\nmany_states_corn <- \n  getQuickstat(\n    key = key_get(\"usda_nass_qs_api\"),\n    program = \"SURVEY\",\n    data_item = \"CORN, GRAIN, IRRIGATED - YIELD, MEASURED IN BU / ACRE\",\n    geographic_level = \"COUNTY\",\n    state = \"ILLINOIS\",\n    year = \"2018\",\n    geometry = TRUE\n  ) Error in UseMethod(\"left_join\"): no applicable method for 'left_join' applied to an object of class \"list\""},{"path":"download-data.html","id":"CropScapeR","chapter":"9 Download and process spatial datasets from within R","heading":"9.2 CDL with CropScapeR","text":"Cropland Data Layer (CDL) data product produced National Agricultural Statistics Service U.S. Department Agriculture. CDL provides geo-referenced, high accuracy, 30 (2007) 56 (2006 2007) meter resolution, crop-specific cropland land cover information 48 contiguous states U.S. 1997 present. data product extensively used agricultural research. CropScape interactive Web CDL exploring system (https://nassgeodata.gmu.edu/CropScape/), developed query, visualize, disseminate, analyze CDL data geospatially standard geospatial web services publicly accessible -line environment (Han et al., 2012).section shows use CropScapeR package (Chen 2020) download explore CDL data. package implements useful geospatial processing services provided CropScape, allows users efficiently process CDL data within R environment. Specifically, CropScapeR package provides four functions implement different kinds geospatial processing services provided CropScape. section introduces functions providing examples. GetCDLData() particular important function lets download raw CDL data. functions provide users CDL data summarized transformed particular manners may suit need users.Note: known problem Mac users requesting CDL data services using CropScape API, causes errors using functions provided package. Please see section 9.2.4 workaround.CropScapeR package can installed directly ‘CRAN’:development version package can downloaded GitHub website using following codes:Let’s load package.Acknowledgment: development CropScapeR package supported USDA-NRCS Agreement . NR193A750016C001 Cooperative Ecosystem Studies Units network. opinions, findings, conclusions, recommendations expressed author(s) necessarily reflect view U.S. Department Agriculture.","code":"\ninstall.packages(\"CropScapeR\")\nlibrary(devtools)\ndevtools::install_github(\"cbw1243/CropScapeR\")  \nlibrary(CropScapeR)  "},{"path":"download-data.html","id":"getcdldata-download-the-cdl-data-as-raster-data","chapter":"9 Download and process spatial datasets from within R","heading":"9.2.1 GetCDLData: Download the CDL data as raster data","text":"GetCDLData() allows us obtain CDL data Area Interest (AOI) given year. requires three parameters make valid data request:aoi: Area Interest (AOI).year: Year data request.type: Type AOI.following AOI-type combinations accepted:spatial object sf sfc object - type = \"b\"county (defined 5-digit county FIPS code) - type = \"f\"state (defined 2-digit state FIPS code) - type = \"f\"bounding box (defined four corner points) - type = \"b\"polygon area (defined least three coordinates) - type = \"ps\"single point (defined coordinate) - type = \"p\"section discusses download data sf object, county, state likely common AOI. See package github site (https://github.com/cbw1243/CropScapeR) see options work.","code":""},{"path":"download-data.html","id":"downloading-cdl-data-for-sf-county-and-state","chapter":"9 Download and process spatial datasets from within R","heading":"9.2.1.1 Downloading CDL data for sf, county, and state","text":"Downloading CDL data sfLet’s download 2018 CDL data area covers Champaign, Vermilion, Ford, Iroquois Counties Illinois (map ).use sf object aoi, CDL data downloaded bounding box (type = \"b\") encompasses entire geographic area sf object irrespective type objects sf object (whether points, polygons, lines). case, CDL data downloaded red area map .Let’s download CDL data four counties:can see, downloaded data RasterLayer object92. Note CDL data uses Albers equal-area conic projection.Take look downloaded CDL data.want values outside sf object, can use raster::mask() turn NA follows:can see , values outside four counties now NA (black area):Downloading CDL data countyThe following code makes request download CDL data Champaign county Illinois 2018.code, FIPS code Champaign County (17019) supplied aoi option. county used , type argument specified ‘f.’Downloading CDL data stateThe following code makes request download CDL data state Illinois 2018.code, state FIPS code Illinois (\\(17\\)) supplied aoi option. county used , type argument specified ‘f.’","code":"\n#--- get the sf for the four counties  ---# \nIL_4_county <- tigris::counties(state = \"IL\", cb = TRUE) %>% \n  st_as_sf() %>% \n  filter(NAME %in% c(\"Champaign\", \"Vermilion\", \"Ford\", \"Iroquois\")) \nggplot() +\n  geom_sf(data = IL_county) +\n  geom_sf(data = IL_county_4, fill = \"lightblue\") +\n  theme_void()\nggplot() +\n  geom_sf(data = IL_county) +\n  geom_sf(data = st_as_sfc(st_bbox(IL_county_4)), fill = \"red\", alpha = 0.4) +\n  theme_void() \n(\ncdl_IL_4 <- GetCDLData(\n  aoi = IL_county_4, \n  year = \"2018\", \n  type = \"b\" \n  )\n)class      : RasterLayer \ndimensions : 4431, 2826, 12522006  (nrow, ncol, ncell)\nresolution : 30, 30  (x, y)\nextent     : 631935, 716715, 1898745, 2031675  (xmin, xmax, ymin, ymax)\ncrs        : +proj=aea +lat_0=23 +lon_0=-96 +lat_1=29.5 +lat_2=45.5 +x_0=0 +y_0=0 +datum=NAD83 +units=m +no_defs \nsource     : IL4.tif \nnames      : IL4 \nvalues     : 0, 255  (min, max)\nterra::crs(cdl_IL_4) Coordinate Reference System:\nDeprecated Proj.4 representation:\n +proj=aea +lat_0=23 +lon_0=-96 +lat_1=29.5 +lat_2=45.5 +x_0=0 +y_0=0\n+datum=NAD83 +units=m +no_defs \nWKT2 2019 representation:\nPROJCRS[\"Albers Conical Equal Area\",\n    BASEGEOGCRS[\"NAD83\",\n        DATUM[\"North American Datum 1983\",\n            ELLIPSOID[\"GRS 1980\",6378137,298.257222101004,\n                LENGTHUNIT[\"metre\",1]]],\n        PRIMEM[\"Greenwich\",0,\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        ID[\"EPSG\",4269]],\n    CONVERSION[\"Albers Equal Area\",\n        METHOD[\"Albers Equal Area\",\n            ID[\"EPSG\",9822]],\n        PARAMETER[\"Latitude of false origin\",23,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8821]],\n        PARAMETER[\"Longitude of false origin\",-96,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8822]],\n        PARAMETER[\"Latitude of 1st standard parallel\",29.5,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8823]],\n        PARAMETER[\"Latitude of 2nd standard parallel\",45.5,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8824]],\n        PARAMETER[\"Easting at false origin\",0,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8826]],\n        PARAMETER[\"Northing at false origin\",0,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8827]]],\n    CS[Cartesian,2],\n        AXIS[\"easting\",east,\n            ORDER[1],\n            LENGTHUNIT[\"metre\",1,\n                ID[\"EPSG\",9001]]],\n        AXIS[\"northing\",north,\n            ORDER[2],\n            LENGTHUNIT[\"metre\",1,\n                ID[\"EPSG\",9001]]]] \nplot(cdl_IL_4)\ncdl_IL_4_masked <- IL_county_4 %>% \n  #--- change the CRS first to that of the raster data ---#\n  st_transform(., projection(cdl_IL_4)) %>% \n  #--- mask the values outside the sf (turn them into NA) ---#\n  raster::mask(cdl_IL_4, .)  \nplot(cdl_IL_4_masked)\n(\ncdl_Champaign <- GetCDLData(aoi = 17019, year = 2018, type = 'f')\n)class      : RasterLayer \ndimensions : 2060, 1626, 3349560  (nrow, ncol, ncell)\nresolution : 30, 30  (x, y)\nextent     : 633825, 682605, 1898745, 1960545  (xmin, xmax, ymin, ymax)\ncrs        : +proj=aea +lat_0=23 +lon_0=-96 +lat_1=29.5 +lat_2=45.5 +x_0=0 +y_0=0 +datum=NAD83 +units=m +no_defs \nsource     : ch.tif \nnames      : ch \nvalues     : 0, 255  (min, max)\nplot(cdl_Champaign) \n(\ncdl_IL <- GetCDLData(aoi = 17, year = 2018, type = 'f')\n)\nplot(cdl_IL) "},{"path":"download-data.html","id":"other-format-options","chapter":"9 Download and process spatial datasets from within R","heading":"9.2.1.2 Other format options","text":"GeoTiffYou save downloaded CDL data tif file adding save_path = option GetCDLData() follows:code, downloaded data saved “IL_4.tif” “Data” folder residing current working directory.sfThe GetCDLData function lets download CDL data sf points, coordinates points coordinates centroid raster cells. can done adding format = sf option.first column (value) crop code. course, can manually convert RasterLayer sf points follows:format = sf option makes GetCDLData() conversion internally want CDL data sf consisting points instead RasterLayer.","code":"\n(\ncdl_IL_4 <- GetCDLData(\n  aoi = IL_county_4, \n  year = \"2018\", \n  type = \"b\",\n  save_path = \"/Data/IL_4.tif\"\n  )\n)\n(\ncdl_sf <- GetCDLData(aoi = 17019, year = 2018, type = 'f', format = \"sf\")\n)\nas.data.frame(cdl_Champaign, xy = TRUE) %>% \n  #--- to sf consisting of points ---#\n  st_as_sf(coords = c(\"x\", \"y\")) %>% \n  #--- Albert conic projection ---#\n  st_set_crs(\"+proj=aea +lat_1=29.5 +lat_2=45.5 +lat_0=23 +lon_0=-96 +x_0=0 +y_0=0 +ellps=GRS80 +towgs84=0,0,0,0,0,0,0 +units=m +no_defs\")  Simple feature collection with 3349560 features and 1 field\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 633840 ymin: 1898760 xmax: 682590 ymax: 1960530\nProjected CRS: SOURCECRS\nFirst 10 features:\n   ch               geometry\n1   0 POINT (633840 1960530)\n2   0 POINT (633870 1960530)\n3   0 POINT (633900 1960530)\n4   0 POINT (633930 1960530)\n5   0 POINT (633960 1960530)\n6   0 POINT (633990 1960530)\n7   0 POINT (634020 1960530)\n8   0 POINT (634050 1960530)\n9   0 POINT (634080 1960530)\n10  0 POINT (634110 1960530)"},{"path":"download-data.html","id":"data-processing-after-downloading-data","chapter":"9 Download and process spatial datasets from within R","heading":"9.2.2 Data processing after downloading data","text":"downloaded raster data readily usable immediately economic analysis. Typically variable interest frequency land use types shares. can use raster::freq() get frequency (number raster cells) land use type.Clearly, frequencies found, can easily get shares well:point, data tell value corresponds crop. find crop names associated crop codes (value), can get reference table using data(linkdata) CropScapeR package.93You can merge two data sets using value CDL data MasterCat linkdata merging keys:NoData Crop corresponds black area figure, portion raster data overlap boundary Champaign County. points NoData can removed using filter function.","code":"\n(\ncrop_freq <- freq(cdl_Champaign)\n)      value   count\n [1,]     0  476477\n [2,]     1 1211343\n [3,]     4      15\n [4,]     5 1173150\n [5,]    23       8\n [6,]    24    8869\n [7,]    26    1168\n [8,]    27      34\n [9,]    28      52\n[10,]    36    4418\n[11,]    37    6804\n[12,]    43       2\n[13,]    59    1064\n[14,]    60      79\n[15,]    61      54\n[16,]   111    6112\n[17,]   121  111191\n[18,]   122  155744\n[19,]   123   38898\n[20,]   124   12232\n[21,]   131    1333\n[22,]   141   49012\n[23,]   142      15\n[24,]   143       7\n[25,]   152      77\n[26,]   176   84463\n[27,]   190    6545\n[28,]   195     339\n[29,]   222       1\n[30,]   229      16\n[31,]   241      38\n(\ncrop_data <-crop_freq %>% \n  #--- matrix to data.frame ---#\n  data.frame(.) %>% \n  #--- find share ---#\n  mutate(share = count/sum(count))\n)   value   count        share\n1      0  476477 1.422506e-01\n2      1 1211343 3.616424e-01\n3      4      15 4.478200e-06\n4      5 1173150 3.502400e-01\n5     23       8 2.388373e-06\n6     24    8869 2.647810e-03\n7     26    1168 3.487025e-04\n8     27      34 1.015059e-05\n9     28      52 1.552443e-05\n10    36    4418 1.318979e-03\n11    37    6804 2.031312e-03\n12    43       2 5.970933e-07\n13    59    1064 3.176537e-04\n14    60      79 2.358519e-05\n15    61      54 1.612152e-05\n16   111    6112 1.824717e-03\n17   121  111191 3.319570e-02\n18   122  155744 4.649685e-02\n19   123   38898 1.161287e-02\n20   124   12232 3.651823e-03\n21   131    1333 3.979627e-04\n22   141   49012 1.463237e-02\n23   142      15 4.478200e-06\n24   143       7 2.089827e-06\n25   152      77 2.298809e-05\n26   176   84463 2.521615e-02\n27   190    6545 1.953988e-03\n28   195     339 1.012073e-04\n29   222       1 2.985467e-07\n30   229      16 4.776747e-06\n31   241      38 1.134477e-05\n#--- load the crop code reference data ---#\ndata(\"linkdata\")\n(\ncrop_data <- dplyr::left_join(crop_data, linkdata, by = c('value' = 'MasterCat'))\n)   value   count        share                     Crop\n1      0  476477 1.422506e-01                   NoData\n2      1 1211343 3.616424e-01                     Corn\n3      4      15 4.478200e-06                  Sorghum\n4      5 1173150 3.502400e-01                 Soybeans\n5     23       8 2.388373e-06             Spring_Wheat\n6     24    8869 2.647810e-03             Winter_Wheat\n7     26    1168 3.487025e-04 Dbl_Crop_WinWht/Soybeans\n8     27      34 1.015059e-05                      Rye\n9     28      52 1.552443e-05                     Oats\n10    36    4418 1.318979e-03                  Alfalfa\n11    37    6804 2.031312e-03    Other_Hay/Non_Alfalfa\n12    43       2 5.970933e-07                 Potatoes\n13    59    1064 3.176537e-04           Sod/Grass_Seed\n14    60      79 2.358519e-05              Switchgrass\n15    61      54 1.612152e-05     Fallow/Idle_Cropland\n16   111    6112 1.824717e-03               Open_Water\n17   121  111191 3.319570e-02     Developed/Open_Space\n18   122  155744 4.649685e-02  Developed/Low_Intensity\n19   123   38898 1.161287e-02  Developed/Med_Intensity\n20   124   12232 3.651823e-03 Developed/High_Intensity\n21   131    1333 3.979627e-04                   Barren\n22   141   49012 1.463237e-02         Deciduous_Forest\n23   142      15 4.478200e-06         Evergreen_Forest\n24   143       7 2.089827e-06             Mixed_Forest\n25   152      77 2.298809e-05                Shrubland\n26   176   84463 2.521615e-02        Grassland/Pasture\n27   190    6545 1.953988e-03           Woody_Wetlands\n28   195     339 1.012073e-04      Herbaceous_Wetlands\n29   222       1 2.985467e-07                   Squash\n30   229      16 4.776747e-06                 Pumpkins\n31   241      38 1.134477e-05   Dbl_Crop_Corn/Soybeans"},{"path":"download-data.html","id":"other-forms-of-cdl-data","chapter":"9 Download and process spatial datasets from within R","heading":"9.2.3 Other forms of CDL data","text":"Instead downloading raw CDL data, CropScape provides option download summarized CDL data.GetCDLComp: request data land use changesGetCDLStat: get acreage estimates CDLGetCDLImage: download image files CDL dataThese may come handy satisfy needs can skip post-downloading processing steps.GetCDLComp(): request data land use changesThe GetCDLComp function allows users request data changes land cover time CDL. Specifically, function returns acres changed one crop category another crop category two years user-defined AOI.Let’s see example. following codes request data acreage changes land cover Champaign County (FIPS = 17019) 2017 (year1 = 2017) 2018 (year2 = 2018).returned data.frame (data.table) 5 columns. columns “” “” crop names. column “Count” pixel count, “Acreage” acres corresponding pixel counts. last column “aoi” selected AOI. first row returned data table shows acreage (.e., 40,362 acres) continuous corn 2017 2018. third row shows acreage (.e., 240,506 acres) rotated corn soybeans 2017 2018.Remember spatial resolution changes 56 meters 30 meters starting 2008. means data requested land use changes 2007 2008, two CDL raster layers different spatial resolutions. Consequently, CropScape API fails resolve issue return error message saying “Mismatch size file 1 file 2.” GetCDLComp() function manually resolves problem resampling two CDL raster files using nearest neighbor resampling technique rasters spatial resolutions. finer resolution raster downscaled lower resolution. , resampled raster layers merged together calculate cropland changes. Users can turn default behavior adding manual_try = FALSE option. case, error message CropScape API returned land use changes results.GetCDLStat(): get acreage estimates CDLThe GetCDLStat function allows users get acreage land cover category user defined AOI year. example, following codes request data acreage land cover categories Champaign County Illinois 2018. can see pixel counts already converted acres category names attached.GetCDLImage(): Download image files CDL data\nGetCDLImage function allows users download image files CDL data. function similar GetCDLData function, except image files returned. function can helpful want look picture CDL data. default, picture saved ‘png’ format. can also save ‘kml’ format.","code":"\n(\ndata_change <- GetCDLComp(aoi = '17019', year1 = 2017, year2 = 2018, type = 'f')\n)                     From                       To   Count  Acreage   aoi\n  1:                 Corn                     Corn  181490  40362.4 17019\n  2:                 Corn                  Sorghum       1      0.2 17019\n  3:                 Corn                 Soybeans 1081442 240506.9 17019\n  4:                 Corn             Winter Wheat    1950    433.7 17019\n  5:                 Corn Dbl Crop WinWht/Soybeans     110     24.5 17019\n ---                                                                     \n241:  Herbaceous Wetlands      Herbaceous Wetlands      18      4.0 17019\n242: Dbl Crop WinWht/Corn                     Corn       1      0.2 17019\n243:             Pumpkins                     Corn      69     15.3 17019\n244:             Pumpkins                  Sorghum       2      0.4 17019\n245:             Pumpkins                 Soybeans      62     13.8 17019\ndata_change <- GetCDLComp(aoi = '17019', year1 = 2007, year2 = 2008, type = 'f', `manual_try` = FALSE) Error in GetCDLCompF(fips = aoi, year1 = year1, year2 = year2, mat = mat, : Error: The requested data might not exist in the CDL database. \nError message from CropScape is :<faultstring>Error: Mismatch size of file 1 and file 2.\n<\/faultstring>\n(\ndata_stat <- GetCDLStat(aoi = 17019, year = 2018, type = 'f')\n)    Value                   Category  Acreage\n 1:     1                       Corn 269396.2\n 2:     4                    Sorghum      3.3\n 3:     5                   Soybeans 260902.3\n 4:    23               Spring Wheat      1.8\n 5:    24               Winter Wheat   1972.4\n 6:    26   Dbl Crop WinWht/Soybeans    259.8\n 7:    27                        Rye      7.6\n 8:    28                       Oats     11.6\n 9:    36                    Alfalfa    982.5\n10:    37      Other Hay/Non Alfalfa   1513.2\n11:    43                   Potatoes      0.4\n12:    59             Sod/Grass Seed    236.6\n13:    60                Switchgrass     17.6\n14:    61       Fallow/Idle Cropland     12.0\n15:   111                 Open Water   1359.3\n16:   121       Developed/Open Space  24728.3\n17:   122    Developed/Low Intensity  34636.6\n18:   123 Developed/Medium Intensity   8650.7\n19:   124   Developed/High Intensity   2720.3\n20:   131                     Barren    296.5\n21:   141           Deciduous Forest  10900.0\n22:   142           Evergreen Forest      3.3\n23:   143               Mixed Forest      1.6\n24:   152                  Shrubland     17.1\n25:   176              Grass/Pasture  18784.1\n26:   190             Woody Wetlands   1455.6\n27:   195        Herbaceous Wetlands     75.4\n28:   222                     Squash      0.2\n29:   229                   Pumpkins      3.6\n30:   241     Dbl Crop Corn/Soybeans      8.5\n    Value                   Category  Acreage\nGetCDLImage(aoi = 17019, year = 2018, type = 'f', verbose = F)"},{"path":"download-data.html","id":"mac-problem","chapter":"9 Download and process spatial datasets from within R","heading":"9.2.4 SSL certificate problem on Mac","text":"SSL refers Secure Sockets Layer, SSL certificate displays important information verifying owner website encrypting web traffic SSL/TLS securing connection. known problem Mac users encounter following error GetCDLData() used: ‘SSL certificate problem: SSL certificate expired.’ name suggests, CropScape server expired certificate. affects Mac users, Windows users expect issue.avoid problem Mac users, CropScapeR workaround involves downloading GeoTiff file requested AOI first, read file using raster() function RasterLayer.94You first need run following code requesting data CDL.Now, can download CDL data specifying file path save_path option like :Hopefully, problem fixed maintainer CropScape workaround necessary Mac users.","code":"\n#--- Skip the SSL check ---#\nhttr::set_config(httr::config(ssl_verifypeer = 0L)) \n#--- Download the raster TIF file into specified path, also read into R ---#\ndata <- GetCDLData(aoi = 17019, year = 2018, type = 'f', save_path = \"ch.tif\")"},{"path":"download-data.html","id":"download-prism","chapter":"9 Download and process spatial datasets from within R","heading":"9.3 PRISM with prism","text":"","code":""},{"path":"download-data.html","id":"basics","chapter":"9 Download and process spatial datasets from within R","heading":"9.3.1 Basics","text":"PRISM dataset provide model-based estimates precipitation, tmax, tmin U.S. 4km 4km spatial resolution. , use get_prism_dailys() prism package (Hart Bell 2015) download daily data. general syntax:variables types can select “ppt” (precipitation), “tmean” (mean temperature), “tmin” (minimum temperature), “tmax” (maximum temperature). minDate maxDate, dates must specified specific format “YYYY-MM-DD.” keepZip = FALSE keep zipped folders downloaded files name suggests.download PRISM data using function, recommended set path folder downloaded PRISM stored using options(prism.path = \"path\"). example, following set path “Data/PRISM/” relative current working directory.following code downloads daily precipitation data January 1, 2000 Jan 10, 2000.download data using code, notice creates one folder one day. example, precipitation data “2000-01-01,” can get path downloaded file follows:can easily read data using terra::rast() stars::read_stars() prefer stars way.quick visualization data (Figure 9.2):\nFigure 9.2: PRISM precipitation data January 1, 2000\ncan see, data covers entire contiguous U.S.","code":"#--- NOT RUN ---#\nget_prism_dailys(\n  type = variable type,\n  minDate = starting date as character,\n  maxDate = ending date as character,\n  keepZip = TRUE or FALSE\n) \noptions(prism.path = \"Data/PRISM/\")\n#--- NOT RUN ---#\nget_prism_dailys(\n  type = \"ppt\",\n  minDate = \"2000-01-01\",\n  maxDate = \"2000-01-10\",\n  keepZip = FALSE\n) \nvar_type <- \"ppt\" # variable type\ndates_prism_txt <- str_remove_all(\"2000-01-01\", \"-\") # date without dashes\n\n#--- folder name ---#\nfolder_name <- paste0(\"PRISM_\", var_type, \"_stable_4kmD2_\", dates_prism_txt, \"_bil\") \n\n#--- file name of the downloaded data inside the above folder ---#\nfile_name <- paste0(\"PRISM_\", var_type, \"_stable_4kmD2_\", dates_prism_txt, \"_bil.bil\") \n\n#--- path to the file relative to the designated data folder (here, it's \"Data/PRISM/\") ---#\n(\nfile_path <- paste0(\"Data/PRISM/\", folder_name, \"/\", file_name)\n)[1] \"Data/PRISM/PRISM_ppt_stable_4kmD2_20000101_bil/PRISM_ppt_stable_4kmD2_20000101_bil.bil\"\n#--- as SpatRaster ---#\n(\nprism_2000_01_01_sr <- rast(file_path) \n)class       : SpatRaster \ndimensions  : 621, 1405, 1  (nrow, ncol, nlyr)\nresolution  : 0.04166667, 0.04166667  (x, y)\nextent      : -125.0208, -66.47917, 24.0625, 49.9375  (xmin, xmax, ymin, ymax)\ncoord. ref. : lon/lat NAD83 \nsource      : PRISM_ppt_stable_4kmD2_20000101_bil.bil \nname        : PRISM_ppt_stable_4kmD2_20000101_bil \nmin value   :                                   0 \nmax value   :                              49.848 \n#--- as stars ---#\n(\nprism_2000_01_01_stars <- read_stars(file_path) \n)stars object with 2 dimensions and 1 attribute\nattribute(s):\n                                   Min. 1st Qu. Median      Mean 3rd Qu.   Max.\nPRISM_ppt_stable_4kmD2_2000010...     0       0      0 0.4952114       0 49.848\n                                     NA's\nPRISM_ppt_stable_4kmD2_2000010...  390874\ndimension(s):\n  from   to   offset      delta refsys point values x/y\nx    1 1405 -125.021  0.0416667  NAD83    NA   NULL [x]\ny    1  621  49.9375 -0.0416667  NAD83    NA   NULL [y]\nplot(prism_2000_01_01_stars)"},{"path":"download-data.html","id":"download-daily-prism-data-for-many-years-and-build-your-own-datasets","chapter":"9 Download and process spatial datasets from within R","heading":"9.3.2 Download daily PRISM data for many years and build your own datasets","text":", example create sets PRISM datasets presented. Creating datasets locally can useful expect use data many different projects future.Suppose interested saving daily PRISM precipitation data year-month 1980 2018. write loop loops year-month combinations. writing loop, let’s work code particular year-month combination: 1990-December. However, write codes way can easily translated looped operations later. Specifically, define following variables use variables looped loop.first need set path folder daily PRISM files downloaded.set start end dates get_prism_dailys().now download PRISM data year-month working .data downloaded, read import onto R. , need path downloaded files.now read stars object, set third dimension date using Dates object class, save R dataset (ensure date dimensions kept. See Chapter 7.4).alternatively read files SpatRaster object save data GeoTIFF file.Note option course date third dimension. Moreover, RDS file takes 14 Mb, tif file occupies 108 Mb.Finally, like, can delete individual PRISM files:Okay, now know particular year-month combination, can easily write loop go year-month combinations period interest. Since processes observed single year-month combination embarrassingly parallel, easy parallelize using future.apply::future_lapply() parallel::mclapply() (Linux/Mac users ). use future_lapply(). Let’s first get number logical cores.following function goes steps saw single year-month combination.create data.frame year-month combinations:now parallelized loop year-month combinations (looping rows month_year_data):’s . course, can thing tmax :Now PRISM datasets, can extract values raster layers vector data analysis, covered extensively Chapters 5 7 (stars objects).want save data year (file 168 Mb). .","code":"\n#--- month to work on ---#  \ntemp_month <- 12\n\n#--- year to work on ---#  \ntemp_year <- 1990\n#--- set your own path ---#\noptions(prism.path = \"Data/PRISM/\") \n#--- starting date of the working month-year ---#\n(\nstart_date <- dmy(paste0(\"1/\", temp_month, \"/\", temp_year))\n)[1] \"1990-12-01\"\n#--- ending date: add a month and then go back 1 day ---#\n(\nend_date <- start_date %m+% months(1) - 1\n)[1] \"1990-12-31\"\n#--- download daily PRISM data for the working month-year ---#\nget_prism_dailys(\n  type = \"ppt\",\n  minDate = as.character(start_date),\n  maxDate = as.character(end_date),\n  keepZip = FALSE\n) \n#--- list of dates of the working month-year ---#\ndates_ls <- seq(start_date, end_date, \"days\") \n\n#--- remove dashes ---#\ndates_prism_txt <- str_remove_all(dates_ls, \"-\")\n\n#--- folder names ---#\nfolder_name <- paste0(\"PRISM_\", var_type, \"_stable_4kmD2_\", dates_prism_txt, \"_bil\") \n#--- the file name of the downloaded data ---#\nfile_name <- paste0(\"PRISM_\", var_type, \"_stable_4kmD2_\", dates_prism_txt, \"_bil.bil\") \n#--- complete path to the downloaded files ---#\n(\nfile_path <- paste0(\"Data/PRISM/\", folder_name, \"/\", file_name) \n) [1] \"Data/PRISM/PRISM_ppt_stable_4kmD2_19901201_bil/PRISM_ppt_stable_4kmD2_19901201_bil.bil\"\n [2] \"Data/PRISM/PRISM_ppt_stable_4kmD2_19901202_bil/PRISM_ppt_stable_4kmD2_19901202_bil.bil\"\n [3] \"Data/PRISM/PRISM_ppt_stable_4kmD2_19901203_bil/PRISM_ppt_stable_4kmD2_19901203_bil.bil\"\n [4] \"Data/PRISM/PRISM_ppt_stable_4kmD2_19901204_bil/PRISM_ppt_stable_4kmD2_19901204_bil.bil\"\n [5] \"Data/PRISM/PRISM_ppt_stable_4kmD2_19901205_bil/PRISM_ppt_stable_4kmD2_19901205_bil.bil\"\n [6] \"Data/PRISM/PRISM_ppt_stable_4kmD2_19901206_bil/PRISM_ppt_stable_4kmD2_19901206_bil.bil\"\n [7] \"Data/PRISM/PRISM_ppt_stable_4kmD2_19901207_bil/PRISM_ppt_stable_4kmD2_19901207_bil.bil\"\n [8] \"Data/PRISM/PRISM_ppt_stable_4kmD2_19901208_bil/PRISM_ppt_stable_4kmD2_19901208_bil.bil\"\n [9] \"Data/PRISM/PRISM_ppt_stable_4kmD2_19901209_bil/PRISM_ppt_stable_4kmD2_19901209_bil.bil\"\n[10] \"Data/PRISM/PRISM_ppt_stable_4kmD2_19901210_bil/PRISM_ppt_stable_4kmD2_19901210_bil.bil\"\n[11] \"Data/PRISM/PRISM_ppt_stable_4kmD2_19901211_bil/PRISM_ppt_stable_4kmD2_19901211_bil.bil\"\n[12] \"Data/PRISM/PRISM_ppt_stable_4kmD2_19901212_bil/PRISM_ppt_stable_4kmD2_19901212_bil.bil\"\n[13] \"Data/PRISM/PRISM_ppt_stable_4kmD2_19901213_bil/PRISM_ppt_stable_4kmD2_19901213_bil.bil\"\n[14] \"Data/PRISM/PRISM_ppt_stable_4kmD2_19901214_bil/PRISM_ppt_stable_4kmD2_19901214_bil.bil\"\n[15] \"Data/PRISM/PRISM_ppt_stable_4kmD2_19901215_bil/PRISM_ppt_stable_4kmD2_19901215_bil.bil\"\n[16] \"Data/PRISM/PRISM_ppt_stable_4kmD2_19901216_bil/PRISM_ppt_stable_4kmD2_19901216_bil.bil\"\n[17] \"Data/PRISM/PRISM_ppt_stable_4kmD2_19901217_bil/PRISM_ppt_stable_4kmD2_19901217_bil.bil\"\n[18] \"Data/PRISM/PRISM_ppt_stable_4kmD2_19901218_bil/PRISM_ppt_stable_4kmD2_19901218_bil.bil\"\n[19] \"Data/PRISM/PRISM_ppt_stable_4kmD2_19901219_bil/PRISM_ppt_stable_4kmD2_19901219_bil.bil\"\n[20] \"Data/PRISM/PRISM_ppt_stable_4kmD2_19901220_bil/PRISM_ppt_stable_4kmD2_19901220_bil.bil\"\n[21] \"Data/PRISM/PRISM_ppt_stable_4kmD2_19901221_bil/PRISM_ppt_stable_4kmD2_19901221_bil.bil\"\n[22] \"Data/PRISM/PRISM_ppt_stable_4kmD2_19901222_bil/PRISM_ppt_stable_4kmD2_19901222_bil.bil\"\n[23] \"Data/PRISM/PRISM_ppt_stable_4kmD2_19901223_bil/PRISM_ppt_stable_4kmD2_19901223_bil.bil\"\n[24] \"Data/PRISM/PRISM_ppt_stable_4kmD2_19901224_bil/PRISM_ppt_stable_4kmD2_19901224_bil.bil\"\n[25] \"Data/PRISM/PRISM_ppt_stable_4kmD2_19901225_bil/PRISM_ppt_stable_4kmD2_19901225_bil.bil\"\n[26] \"Data/PRISM/PRISM_ppt_stable_4kmD2_19901226_bil/PRISM_ppt_stable_4kmD2_19901226_bil.bil\"\n[27] \"Data/PRISM/PRISM_ppt_stable_4kmD2_19901227_bil/PRISM_ppt_stable_4kmD2_19901227_bil.bil\"\n[28] \"Data/PRISM/PRISM_ppt_stable_4kmD2_19901228_bil/PRISM_ppt_stable_4kmD2_19901228_bil.bil\"\n[29] \"Data/PRISM/PRISM_ppt_stable_4kmD2_19901229_bil/PRISM_ppt_stable_4kmD2_19901229_bil.bil\"\n[30] \"Data/PRISM/PRISM_ppt_stable_4kmD2_19901230_bil/PRISM_ppt_stable_4kmD2_19901230_bil.bil\"\n[31] \"Data/PRISM/PRISM_ppt_stable_4kmD2_19901231_bil/PRISM_ppt_stable_4kmD2_19901231_bil.bil\"(\n#--- combine all the PRISM files as stars ---#\ntemp_stars <- read_stars(file_path, along = 3)\n  #--- set the third dimension as data ---# \n  st_set_dimensions(\"band\", values = dates_ls, name = \"date\")\n)\n\n#--- save the stars as an rds file ---#\nsaveRDS(\n  temp_stars, \n  paste0(\"Data/PRISM/PRISM_\", var_type, \"_y\", temp_year, \"_m\", temp_month, \".rds\")\n)  \n(\n#--- combine all the PRISM files as a RasterStack ---#\ntemp_stars <- terra::rast(file_path) \n)\n\n#--- save as a multi-band GeoTIFF file ---#\nwriteRaster(temp_stars, paste0(\"Data/PRISM/PRISM_\", var_type, \"_y\", temp_year, \"_m\", temp_month, \".tif\"), overwrite = T) \n#--- delete all the downloaded files ---#\nunlink(paste0(\"Data/PRISM/\", folder_name), recursive = TRUE) \nlibrary(parallel)\nnum_cores <- detectCores() \n\nplan(multiprocess, workers = num_cores)\n#--- define a function to download and save PRISM data stacked by month ---#\nget_save_prism <- function(i, var_type) {\n\n  print(paste0(\"working on \", i))\n\n  temp_month <- month_year_data[i, month] # working month\n  temp_year <- month_year_data[i, year] # working year\n  \n  #--- starting date of the working month-year ---#\n  start_date <- dmy(paste0(\"1/\", temp_month, \"/\", temp_year))\n  #--- end date ---#\n  end_date <- start_date %m+% months(1) - 1\n\n  \n  #--- download daily PRISM data for the working month-year ---#\n  get_prism_dailys(\n    type = var_type,\n    minDate = as.character(start_date),\n    maxDate = as.character(end_date),\n    keepZip = FALSE\n  ) \n\n  #--- list of dates of the working month-year ---#\n  dates_ls <- seq(start_date, end_date, \"days\") \n\n  #--- remove dashes ---#\n  dates_prism_txt <- str_remove_all(dates_ls, \"-\")\n\n  #--- folder names ---#\n  folder_name <- paste0(\"PRISM_\", var_type, \"_stable_4kmD2_\", dates_prism_txt, \"_bil\") \n  #--- the file name of the downloaded data ---#\n  file_name <- paste0(\"PRISM_\", var_type, \"_stable_4kmD2_\", dates_prism_txt, \"_bil.bil\") \n  #--- complete path to the downloaded files ---#\n  file_path <- paste0(\"Data/PRISM/\", folder_name, \"/\", file_name)\n\n  #--- combine all the PRISM files as a RasterStack ---#\n  temp_stars <- stack(file_path) %>% \n    #--- convert to stars ---#\n    st_as_stars() %>% \n    #--- set the third dimension as data ---# \n    st_set_dimensions(\"band\", values = dates_ls, name = \"date\")\n\n  #--- save the stars as an rds file ---#\n  saveRDS(\n    temp_stars, \n    paste0(\"Data/PRISM/PRISM_\", var_type, \"_y\", temp_year, \"_m\", temp_month, \".rds\")\n  )\n\n  #--- delete all the downloaded files ---#\n  unlink(paste0(\"Data/PRISM/\", folder_name), recursive = TRUE)\n} \n(\n#--- create a set of year-month combinations to loop over ---#\nmonth_year_data <- expand.grid(month  = 1:12, year = 1990:2018) %>% \n  data.table()\n)     month year\n  1:     1 1990\n  2:     2 1990\n  3:     3 1990\n  4:     4 1990\n  5:     5 1990\n ---           \n344:     8 2018\n345:     9 2018\n346:    10 2018\n347:    11 2018\n348:    12 2018\n#--- run the above code in parallel ---#\nfuture_lapply(\n  1:nrow(month_year_data), \n  function (x) get_save_prism(x, \"ppt\")\n)\n#--- run the above code in parallel ---#\nfuture_lapply(\n  1:nrow(month_year_data), \n  function (x) get_save_prism(x, \"tmax\")\n)\n#--- define a function to download and save PRISM data stacked by year ---#\nget_save_prism_y <- function(temp_year, var_type) {\n\n  print(paste0(\"working on \", temp_year))\n  \n  #--- starting date of the working month-year ---#\n  start_date <- dmy(paste0(\"1/1/\", temp_year))\n  #--- end date ---#\n  end_date <- dmy(paste0(\"1/1/\", temp_year + 1)) - 1\n\n  #--- download daily PRISM data for the working month-year ---#\n  get_prism_dailys(\n    type = var_type,\n    minDate = as.character(start_date),\n    maxDate = as.character(end_date),\n    keepZip = FALSE\n  ) \n\n  #--- list of dates of the working month-year ---#\n  dates_ls <- seq(start_date, end_date, \"days\") \n\n  #--- remove dashes ---#\n  dates_prism_txt <- str_remove_all(dates_ls, \"-\")\n\n  #--- folder names ---#\n  folder_name <- paste0(\"PRISM_\", var_type, \"_stable_4kmD2_\", dates_prism_txt, \"_bil\") \n  #--- the file name of the downloaded data ---#\n  file_name <- paste0(\"PRISM_\", var_type, \"_stable_4kmD2_\", dates_prism_txt, \"_bil.bil\") \n  #--- complete path to the downloaded files ---#\n  file_path <- paste0(\"Data/PRISM/\", folder_name, \"/\", file_name)\n\n  #--- combine all the PRISM files as a RasterStack ---#\n  temp_stars <- stack(file_path) %>% \n    #--- convert to stars ---#\n    st_as_stars() %>% \n    #--- set the third dimension as data ---# \n    st_set_dimensions(\"band\", values = dates_ls, name = \"date\")\n\n  #--- save the stars as an rds file ---#\n  saveRDS(\n    temp_stars, \n    paste0(\"Data/PRISM/PRISM_\", var_type, \"_y\", temp_year, \".rds\")\n  )\n\n  #--- delete all the downloaded files ---#\n  unlink(paste0(\"Data/PRISM/\", folder_name), recursive = TRUE)\n} \n\n#--- run the above code in parallel ---#\nfuture_lapply(\n  1990:2018, \n  function (x) get_save_prism_y(x, \"tmax\")\n)"},{"path":"download-data.html","id":"daymet-with-daymetr-and-feddata","chapter":"9 Download and process spatial datasets from within R","heading":"9.4 Daymet with daymetr and FedData","text":"section, use daymetr (Hufkens et al. 2018) FedData packages (Bocinsky 2016).Daymet data consists “tiles,” consisting raster cells 1km 1km. map tiles (Figure 9.3)\nFigure 9.3: Daymet Tiles\nlist weather variables:vapor pressureminimum maximum temperaturesnow water equivalentsolar radiationprecipitationday lengthSo, Daymet provides information weather variables PRISM, helps find weather-dependent metrics, evapotranspiration.easiest way find Daymet values vector data depends whether points polygons data. points data, daymetr::download_daymet() easiest directly return weather values point interest. Internally, finds cell point located, return values cell specified length period. daymetr::download_daymet() . polygons, need first download pertinent Daymet data region interest first extract values polygons , learned Chapter 5. Unfortunately, netCDF data downloaded daymetr::download_daymet_ncss() easily read raster package stars package. contrary, FedData::get_daymet() download requested Daymet data save RasterBrick object, can easily turned stars object using st_as_stars().","code":"\nlibrary(daymetr)  \nlibrary(FedData)  "},{"path":"download-data.html","id":"for-points-data","chapter":"9 Download and process spatial datasets from within R","heading":"9.4.1 For points data","text":"points data, easiest way associate daily weather values use download_daymet(). download_daymet() can download daily weather data single point time finding cell tile point located within.key parameters function:lat: latitudelon: longitudestart: start_yearend: end_yearinternal: TRUE (dafault) FALSEFor example, code downloads daily weather data point (lat = \\(36\\), longitude = \\(-100\\)) starting 2000 2002 assigns downloaded data temp_daymet.can see, temp_daymet bunch site information download Daymet data.might noticed, yday date observation, day year. can easily convert dates like :One dates obtained, can use lubridate package extract day, month, year using day(), month(), year(), respectively.helps find group statistics like monthly precipitation.Downloading Daymet data many points just applying operations using loop. Let’s create random points within California get coordinates.loop points, can first write function like :function returns 1st row random_points:can now simply loop rows.better yet, can easily parallelize process follows (see Chapter familiar parallelization R):","code":"\n#--- download daymet data ---#\ntemp_daymet <- download_daymet(\n  lat = 36,\n  lon = -100,\n  start = 2000,\n  end = 2002\n) \n\n#--- structure ---#\nstr(temp_daymet)List of 7\n $ site     : chr \"Daymet\"\n $ tile     : num 11380\n $ latitude : num 36\n $ longitude: num -100\n $ altitude : num 746\n $ tile     : num 11380\n $ data     :'data.frame':  1095 obs. of  9 variables:\n  ..$ year         : int [1:1095] 2000 2000 2000 2000 2000 2000 2000 2000 2000 2000 ...\n  ..$ yday         : int [1:1095] 1 2 3 4 5 6 7 8 9 10 ...\n  ..$ dayl..s.     : num [1:1095] 34571 34606 34644 34685 34729 ...\n  ..$ prcp..mm.day.: num [1:1095] 0 0 0 0 0 0 0 0 0 0 ...\n  ..$ srad..W.m.2. : num [1:1095] 334 231 148 319 338 ...\n  ..$ swe..kg.m.2. : num [1:1095] 18.2 15 13.6 13.6 13.6 ...\n  ..$ tmax..deg.c. : num [1:1095] 20.9 13.88 4.49 9.18 13.37 ...\n  ..$ tmin..deg.c. : num [1:1095] -2.73 1.69 -2.6 -10.16 -8.97 ...\n  ..$ vp..Pa.      : num [1:1095] 500 690 504 282 310 ...\n - attr(*, \"class\")= chr \"daymetr\"\n#--- get the data part ---#\ntemp_daymet_data <- temp_daymet$data\n\n#--- take a look ---#\nhead(temp_daymet_data)  year yday dayl..s. prcp..mm.day. srad..W.m.2. swe..kg.m.2. tmax..deg.c.\n1 2000    1 34571.11             0       334.34        18.23        20.90\n2 2000    2 34606.19             0       231.20        15.00        13.88\n3 2000    3 34644.13             0       148.29        13.57         4.49\n4 2000    4 34684.93             0       319.13        13.57         9.18\n5 2000    5 34728.55             0       337.67        13.57        13.37\n6 2000    6 34774.96             0       275.72        13.11         9.98\n  tmin..deg.c. vp..Pa.\n1        -2.73  499.56\n2         1.69  689.87\n3        -2.60  504.40\n4       -10.16  282.35\n5        -8.97  310.05\n6        -4.89  424.78\ntemp_daymet_data <- mutate(temp_daymet_data, date = as.Date(paste(year, yday, sep = \"-\"), \"%Y-%j\"))\nlibrary(lubridate)\n\ntemp_daymet_data <- mutate(temp_daymet_data,\n  day = day(date),\n  month = month(date),\n  #--- this is already there though ---#\n  year = year(date)\n)\n\n#--- take a look ---#\ndplyr::select(temp_daymet_data, year, month, day) %>%  head  year month day\n1 2000     1   1\n2 2000     1   2\n3 2000     1   3\n4 2000     1   4\n5 2000     1   5\n6 2000     1   6\ntemp_daymet_data %>% \n  group_by(month) %>% \n  summarize(prcp = mean(prcp..mm.day.))# A tibble: 12 × 2\n   month  prcp\n   <dbl> <dbl>\n 1     1 0.762\n 2     2 1.20 \n 3     3 2.08 \n 4     4 1.24 \n 5     5 3.53 \n 6     6 3.45 \n 7     7 1.78 \n 8     8 1.19 \n 9     9 1.32 \n10    10 4.78 \n11    11 0.407\n12    12 0.743\nset.seed(389548)\n\nrandom_points <- \n  tigris::counties(state = \"CA\") %>% \n  st_as_sf() %>% \n  #--- 10 points ---#\n  st_sample(10) %>% \n  #--- get the coordinates ---#\n  st_coordinates() %>% \n  #--- as tibble (data.frame) ---#\n  as_tibble() %>% \n  #--- assign site id ---#\n  mutate(site_id = 1:n())\nget_daymet <- function(i){\n\n  temp_lat <- random_points[i, ] %>% pull(Y)\n  temp_lon <- random_points[i, ] %>% pull(X)\n  temp_site <- random_points[i, ] %>% pull(site_id)\n\n  temp_daymet <- download_daymet(\n    lat = temp_lat,\n    lon = temp_lon,\n    start = 2000,\n    end = 2002\n  ) %>% \n  #--- just get the data part ---#\n  .$data %>% \n  #--- convert to tibble (not strictly necessary) ---#\n  as_tibble() %>% \n  #--- assign site_id so you know which record is for which site_id ---#\n  mutate(site_id = temp_site) %>% \n  #--- get date from day of the year ---#\n  mutate(date = as.Date(paste(year, yday, sep = \"-\"), \"%Y-%j\"))\n\n  return(temp_daymet)\n}  \nget_daymet(1)# A tibble: 1,095 × 11\n    year  yday dayl..s. prcp..mm.day. srad..W.m.2. swe..kg.m.2. tmax..deg.c.\n   <int> <int>    <dbl>         <dbl>        <dbl>        <dbl>        <dbl>\n 1  2000     1   34131.             0         246.            0         7.02\n 2  2000     2   34168.             0         251.            0         4.67\n 3  2000     3   34208.             0         247.            0         7.32\n 4  2000     4   34251              0         267.            0        10   \n 5  2000     5   34297              0         240.            0         4.87\n 6  2000     6   34346.             0         274.            0         7.79\n 7  2000     7   34398.             0         262.            0         7.29\n 8  2000     8   34453.             0         291.            0        11.2 \n 9  2000     9   34510.             0         268.            0        10.0 \n10  2000    10   34570.             0         277.            0        11.8 \n# … with 1,085 more rows, and 4 more variables: tmin..deg.c. <dbl>,\n#   vp..Pa. <dbl>, site_id <int>, date <date>\n(\ndaymet_all_points <- lapply(1:nrow(random_points), get_daymet) %>% \n  #--- need to combine the list of data.frames into a single data.frame ---#\n  bind_rows()\n)# A tibble: 10,950 × 11\n    year  yday dayl..s. prcp..mm.day. srad..W.m.2. swe..kg.m.2. tmax..deg.c.\n   <dbl> <dbl>    <dbl>         <dbl>        <dbl>        <dbl>        <dbl>\n 1  2000     1   33523.             0         230.            0         12  \n 2  2000     2   33523.             0         240             0         11.5\n 3  2000     3   33523.             0         243.            0         13.5\n 4  2000     4   33523.             1         230.            0         13  \n 5  2000     5   33523.             0         243.            0         14  \n 6  2000     6   33523.             0         243.            0         13  \n 7  2000     7   33523.             0         246.            0         14  \n 8  2000     8   33869.             0         230.            0         13  \n 9  2000     9   33869.             0         227.            0         12.5\n10  2000    10   33869.             0         214.            0         14  \n# … with 10,940 more rows, and 4 more variables: tmin..deg.c. <dbl>,\n#   vp..Pa. <dbl>, site_id <int>, date <date>\nlibrary(future.apply)    \nlibrary(parallel)\n\n#--- parallelization planning ---#\nplan(multiprocess, workers = detectCores() - 1)\n\n#--- parallelized lapply ---#\ndaymet_all_points <- future_lapply(1:nrow(random_points), get_daymet) %>% \n  #--- need to combine the list of data.frames into a single data.frame ---#\n  bind_rows()"},{"path":"download-data.html","id":"daymet-poly","chapter":"9 Download and process spatial datasets from within R","heading":"9.4.2 For polygons data","text":"Suppose interested getting Daymet data select counties Michigan (Figure 9.4).\nFigure 9.4: Select Michigan counties download Daymet data\ncan use FedData::get_daymet() download Daymet data covers spatial extent polygons data. downloaded dataset can assigned R object RasterBrick (alternatively write downloaded data file). order let function know spatial extent download Daymet data, supply SpatialPolygonsDataFrame object supported sp package. Since main vector data handling package sf need convert sf object sp object.code downloads prcp tmax spatial extent Michigan counties 2000 2001:can see, Daymet prcp tmax data stored separately RasterBrick. Now stars objects, can extract values target polygons data using methods described Chapter 5.use stars package raster data handling (see Chapter 7), can convert stars object using st_as_stars().Now, third dimension (band) recognized dates. can use st_set_dimension() change (see Chapter 7.5). , first need recover Date values “band” values follows:Notice date dimension NA delta. Daymet removes observations December 31 leap years make time dimension 365 consistently across years. means one-day gap “2000-12-30” “2000-01-01” can see :","code":"\n#--- entire MI ---#\nMI_counties <- tigris::counties(state = \"MI\")\n\n#--- select counties ---#\nMI_counties_select <- filter(MI_counties, NAME %in% c(\"Luce\", \"Chippewa\", \"Mackinac\"))\n(\nMI_daymet_select <- FedData::get_daymet(\n  #--- supply the vector data in sp ---#\n  template = as(MI_counties_select, \"Spatial\"),\n  #--- label ---#\n  label = \"MI_counties_select\",\n  #--- variables to download ---#\n  elements = c(\"prcp\",\"tmax\"),\n  #--- years ---#\n  years = 2000:2001\n)\n)$prcp\nclass      : RasterBrick \ndimensions : 96, 156, 14976, 730  (nrow, ncol, ncell, nlayers)\nresolution : 1000, 1000  (x, y)\nextent     : 1027250, 1183250, 455500, 551500  (xmin, xmax, ymin, ymax)\ncrs        : +proj=lcc +lon_0=-100 +lat_0=42.5 +x_0=0 +y_0=0 +lat_1=25 +lat_2=60 +ellps=WGS84 \nsource     : memory\nnames      : X2000.01.01, X2000.01.02, X2000.01.03, X2000.01.04, X2000.01.05, X2000.01.06, X2000.01.07, X2000.01.08, X2000.01.09, X2000.01.10, X2000.01.11, X2000.01.12, X2000.01.13, X2000.01.14, X2000.01.15, ... \nmin values :           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0, ... \nmax values :           6,          26,          19,          17,           6,           9,           6,           1,           0,          13,          15,           6,           0,           4,           7, ... \n\n\n$tmax\nclass      : RasterBrick \ndimensions : 96, 156, 14976, 730  (nrow, ncol, ncell, nlayers)\nresolution : 1000, 1000  (x, y)\nextent     : 1027250, 1183250, 455500, 551500  (xmin, xmax, ymin, ymax)\ncrs        : +proj=lcc +lon_0=-100 +lat_0=42.5 +x_0=0 +y_0=0 +lat_1=25 +lat_2=60 +ellps=WGS84 \nsource     : memory\nnames      : X2000.01.01, X2000.01.02, X2000.01.03, X2000.01.04, X2000.01.05, X2000.01.06, X2000.01.07, X2000.01.08, X2000.01.09, X2000.01.10, X2000.01.11, X2000.01.12, X2000.01.13, X2000.01.14, X2000.01.15, ... \nmin values :         0.5,        -4.5,        -8.0,        -8.5,        -7.0,        -2.5,        -6.5,        -1.5,         1.5,         2.0,        -1.5,        -8.5,       -14.0,        -9.5,        -3.0, ... \nmax values :         3.0,         3.0,         0.5,        -2.5,        -2.5,         0.0,         0.5,         1.5,         4.0,         3.0,         3.5,         0.5,        -6.5,        -4.0,         0.0, ... \n#--- tmax as stars ---#\ntmax_stars <- st_as_stars(MI_daymet_select$tmax)\n\n#--- prcp as stars ---#\nprcp_stars <- st_as_stars(MI_daymet_select$prcp)\ndate_values <- tmax_stars %>% \n  #--- get band values ---#\n  st_get_dimension_values(., \"band\") %>% \n  #--- remove X ---#\n  gsub(\"X\", \"\", .) %>% \n  #--- convert to date ---#\n  ymd(.)\n\n#--- take a look ---#\nhead(date_values)[1] \"2000-01-01\" \"2000-01-02\" \"2000-01-03\" \"2000-01-04\" \"2000-01-05\"\n[6] \"2000-01-06\"\n#--- tmax ---#\nst_set_dimensions(tmax_stars, 3, values = date_values, names = \"date\" )  stars object with 3 dimensions and 1 attribute\nattribute(s), summary of first 1e+05 cells:\n             Min. 1st Qu. Median      Mean 3rd Qu. Max. NA's\nX2000.01.01  -8.5      -4     -2 -1.884266       0    3  198\ndimension(s):\n     from  to  offset delta                       refsys point\nx       1 156 1027250  1000 +proj=lcc +lon_0=-100 +la...    NA\ny       1  96  551500 -1000 +proj=lcc +lon_0=-100 +la...    NA\ndate    1 730      NA    NA                         Date    NA\n                        values x/y\nx                         NULL [x]\ny                         NULL [y]\ndate 2000-01-01,...,2001-12-31    \n#--- prcp ---#\nst_set_dimensions(prcp_stars, 3, values = date_values, names = \"date\" )  stars object with 3 dimensions and 1 attribute\nattribute(s), summary of first 1e+05 cells:\n             Min. 1st Qu. Median     Mean 3rd Qu. Max. NA's\nX2000.01.01     0       0      3 6.231649      12   26  198\ndimension(s):\n     from  to  offset delta                       refsys point\nx       1 156 1027250  1000 +proj=lcc +lon_0=-100 +la...    NA\ny       1  96  551500 -1000 +proj=lcc +lon_0=-100 +la...    NA\ndate    1 730      NA    NA                         Date    NA\n                        values x/y\nx                         NULL [x]\ny                         NULL [y]\ndate 2000-01-01,...,2001-12-31    \ndate_values[364:367][1] \"2000-12-29\" \"2000-12-30\" \"2001-01-01\" \"2001-01-02\""},{"path":"download-data.html","id":"gridMET","chapter":"9 Download and process spatial datasets from within R","heading":"9.5 gridMET","text":"gridMET dataset daily meteorological data covers contiguous US since 1979. spatial resolution PRISM data 4-km 4-km. Indeed gridMET product combining PRISM Land Data Assimilation System (https://ldas.gsfc.nasa.gov/nldas/NLDAS2forcing.php) (particular NLDAS-2). offer variables PRISM, including maximum temperature, minimum temperature, precipitation accumulation, downward surface shortwave radiation, wind-velocity, humidity (maximum minimum relative humidity specific humidity. also offers derived products reference evapotranspiration (calculated based Penman-Montieth equation).can use downloadr::download() function download gridMET data variable-year. example, download precipitation data 2018, can run following code:set url dataset interest url option, set destination file name, mode wb binary download.gridMET datasets direct download “http://www.northwestknowledge.net/metdata/data/” beginning, followed file name (, pr_2018.nc). file names follow convention variable_abbreviation_year.nc. , can easily write loop get data multiple variables multiple years.list variable abbreviations:sph: (Near-Surface Specific Humidity)vpd: (Mean Vapor Pressure Deficit)pr: (Precipitation)rmin: (Minimum Near-Surface Relative Humidity)rmax: (Maximum Near-Surface Relative Humidity)srad: (Surface Downwelling Solar Radiation)tmmn: (Minimum Near-Surface Air Temperature)tmmx: (Maximum Near-Surface Air Temperature)vs: (Wind speed 10 m)th: (Wind direction 10 m)pdsi: (Palmer Drought Severity Index)pet: (Reference grass evaportranspiration)etr: (Reference alfalfa evaportranspiration)erc: (model-G)bi: (model-G)fm100: (100-hour dead fuel moisture)fm1000: (1000-hour dead fuel moisture)another example, interested downloading wind speed data 2020, can use following code.","code":"\ndownloader::download(\n  url = \"http://www.northwestknowledge.net/metdata/data/pr_2018.nc\",\n  destfile = \"Data/pr_2018.nc\",\n  mode = 'wb'\n)\ndownloader::download(\n  url = \"http://www.northwestknowledge.net/metdata/data/vs_2020.nc\",\n  destfile = \"Data/vs_2020.nc\",\n  mode = 'wb'\n)"},{"path":"download-data.html","id":"practical-examples","chapter":"9 Download and process spatial datasets from within R","heading":"9.5.1 Practical Examples","text":"Suppose final goal get average daily precipitation (pr) reference grass evapotranspiration (pet) 2015 2020 counties California.First get county boundaries California:writing loop, let’s work single case (pet 2015). First, download read data.can see, multi-layer raster object layer represents single day 2015. quick visualization first layer.Now, can use exactexactr::exact_extract() assign cell values county transform convenient form:can see data 367 columns: 365 (days) + 1 (rowid) + 1 (coverage fraction). Let’s take look name first six variables.5-digit number end name variables evapotranspiration represents days since Jan 1st, 1900. can confirmed using ncdf4:nc_open() (see middle output day 4 dimensions):universally true gridMET data. can use information recover date. First, let’s transform data wide format long format easier operations:now use str_sub() get 5-digit numbers variable, represents days since Jan 1st, 1900. can recover dates using lubridate package.Finally, let’s calculate coverage-weighted average pet county-date.Since rowid value n corresponds n th row CA_counties, easy merge pet_county_avg CA_counties (alternatively, can use cbind()).Now know process single gridMET dataset, ready write function goes choice gridMET dataset write loop achieve goal. function:Let’s now loop variables years interest. first set dataset variable-year combinations loop .now loop rows par_data parallel:","code":"\nCA_counties <- tigris::counties(state = \"CA\") %>% \n  dplyr::select(STATEFP, COUNTYFP)\n#--- download data ---#\ndownloader::download(\n  url = \"http://www.northwestknowledge.net/metdata/data/pet_2015.nc\",\n  destfile = \"Data/pet_2015.nc\",\n  mode = 'wb'\n) \n\n#--- read the raster data ---#\n(\npet_2015 <- rast(\"Data/pet_2015.nc\")\n)class       : SpatRaster \ndimensions  : 585, 1386, 365  (nrow, ncol, nlyr)\nresolution  : 0.04166667, 0.04166667  (x, y)\nextent      : -124.7875, -67.0375, 25.04583, 49.42083  (xmin, xmax, ymin, ymax)\ncoord. ref. : lon/lat WGS 84 (EPSG:4326) \nsource      : pet_2015.nc \nvarname     : potential_evapotranspiration (pet) \nnames       : poten~42003, poten~42004, poten~42005, poten~42006, poten~42007, poten~42008, ... \nunit        :          mm,          mm,          mm,          mm,          mm,          mm, ... \nplot(pet_2015[[1]]) \npet_county <- \n  #--- extract data for each county ---#\n  exact_extract(pet_2015, CA_counties, progress = FALSE) %>% \n  #--- list of data.frames into data.table ---#\n  rbindlist(idcol = \"rowid\")\n\n#--- check the dimension of the output ---#\ndim(pet_county) [1] 28201   367\nhead(names(pet_county))[1] \"rowid\"                                 \n[2] \"potential_evapotranspiration_day=42003\"\n[3] \"potential_evapotranspiration_day=42004\"\n[4] \"potential_evapotranspiration_day=42005\"\n[5] \"potential_evapotranspiration_day=42006\"\n[6] \"potential_evapotranspiration_day=42007\"\nncdf4::nc_open(\"Data/pet_2015.nc\")  File Data/pet_2015.nc (NC_FORMAT_NETCDF4):\n\n     1 variables (excluding dimension variables):\n        unsigned short potential_evapotranspiration[lon,lat,day]   (Chunking: [231,98,61])  (Compression: level 9)\n            _FillValue: 32767\n            units: mm\n            description: Daily reference evapotranspiration (short grass)\n            long_name: pet\n            standard_name: pet\n            missing_value: 32767\n            dimensions: lon lat time\n            grid_mapping: crs\n            coordinate_system: WGS84,EPSG:4326\n            scale_factor: 0.1\n            add_offset: 0\n            coordinates: lon lat\n            _Unsigned: true\n\n     4 dimensions:\n        lon  Size:1386 \n            units: degrees_east\n            description: longitude\n            long_name: longitude\n            standard_name: longitude\n            axis: X\n        lat  Size:585 \n            units: degrees_north\n            description: latitude\n            long_name: latitude\n            standard_name: latitude\n            axis: Y\n        day  Size:365 \n            description: days since 1900-01-01\n            units: days since 1900-01-01 00:00:00\n            long_name: time\n            standard_name: time\n            calendar: gregorian\n        crs  Size:1 \n            grid_mapping_name: latitude_longitude\n            longitude_of_prime_meridian: 0\n            semi_major_axis: 6378137\n            long_name: WGS 84\n            inverse_flattening: 298.257223563\n            GeoTransform: -124.7666666333333 0.041666666666666 0  49.400000000000000 -0.041666666666666\n            spatial_ref: GEOGCS[\"WGS 84\",DATUM[\"WGS_1984\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]],AUTHORITY[\"EPSG\",\"6326\"]],PRIMEM[\"Greenwich\",0,AUTHORITY[\"EPSG\",\"8901\"]],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]],AUTHORITY[\"EPSG\",\"4326\"]]\n\n    19 global attributes:\n        geospatial_bounds_crs: EPSG:4326\n        Conventions: CF-1.6\n        geospatial_bounds: POLYGON((-124.7666666333333 49.400000000000000, -124.7666666333333 25.066666666666666, -67.058333300000015 25.066666666666666, -67.058333300000015 49.400000000000000, -124.7666666333333 49.400000000000000))\n        geospatial_lat_min: 25.066666666666666\n        geospatial_lat_max: 49.40000000000000\n        geospatial_lon_min: -124.7666666333333\n        geospatial_lon_max: -67.058333300000015\n        geospatial_lon_resolution: 0.041666666666666\n        geospatial_lat_resolution: 0.041666666666666\n        geospatial_lat_units: decimal_degrees north\n        geospatial_lon_units: decimal_degrees east\n        coordinate_system: EPSG:4326\n        author: John Abatzoglou - University of Idaho, jabatzoglou@uidaho.edu\n        date: 04 July 2019\n        note1: The projection information for this file is: GCS WGS 1984.\n        note2: Citation: Abatzoglou, J.T., 2013, Development of gridded surface meteorological data for ecological applications and modeling, International Journal of Climatology, DOI: 10.1002/joc.3413\n        note3: Data in slices after last_permanent_slice (1-based) are considered provisional and subject to change with subsequent updates\n        note4: Data in slices after last_provisional_slice (1-based) are considered early and subject to change with subsequent updates\n        note5: Days correspond approximately to calendar days ending at midnight, Mountain Standard Time (7 UTC the next calendar day)\npet_county <-  \n  #--- wide to long ---#\n  melt(pet_county, id.var = c(\"rowid\", \"coverage_fraction\")) %>% \n  #--- remove observations with NA values ---#\n  .[!is.na(value), ]   \n\n#--- take a look ---#\npet_county          rowid coverage_fraction                               variable value\n       1:     1       0.004266545 potential_evapotranspiration_day=42003   2.3\n       2:     1       0.254054248 potential_evapotranspiration_day=42003   2.2\n       3:     1       0.175513789 potential_evapotranspiration_day=42003   2.0\n       4:     1       0.442011684 potential_evapotranspiration_day=42003   2.1\n       5:     1       1.000000000 potential_evapotranspiration_day=42003   2.2\n      ---                                                                     \n10115971:    58       0.538234591 potential_evapotranspiration_day=42367   2.1\n10115972:    58       0.197555855 potential_evapotranspiration_day=42367   2.7\n10115973:    58       0.224901110 potential_evapotranspiration_day=42367   2.2\n10115974:    58       0.554238617 potential_evapotranspiration_day=42367   2.2\n10115975:    58       0.272462875 potential_evapotranspiration_day=42367   2.1\npet_county[, variable := str_sub(variable, -5, -1) %>% as.numeric()] %>% \n  #--- recover dates ---#\n  .[, date := variable + lubridate::ymd(\"1900-01-01\")] \n\n#--- take a look ---#\npet_county          rowid coverage_fraction variable value       date\n       1:     1       0.004266545    42003   2.3 2015-01-01\n       2:     1       0.254054248    42003   2.2 2015-01-01\n       3:     1       0.175513789    42003   2.0 2015-01-01\n       4:     1       0.442011684    42003   2.1 2015-01-01\n       5:     1       1.000000000    42003   2.2 2015-01-01\n      ---                                                  \n10115971:    58       0.538234591    42367   2.1 2015-12-31\n10115972:    58       0.197555855    42367   2.7 2015-12-31\n10115973:    58       0.224901110    42367   2.2 2015-12-31\n10115974:    58       0.554238617    42367   2.2 2015-12-31\n10115975:    58       0.272462875    42367   2.1 2015-12-31\npet_county_avg <- \n  pet_county[, \n    .(value = sum(value * coverage_fraction) / sum(coverage_fraction)),\n    by = .(rowid, date)\n  ] %>% \n  setnames(\"value\", \"pet\")\nCA_pet <- CA_counties %>% \n  mutate(rowid = seq_len(nrow(.))) %>% \n  left_join(pet_county_avg, ., by = \"rowid\")\nget_grid_MET <- function(var_name, year) {\n  #--- for testing ---#\n  # var_name <- \"pet\"\n  # year <- 2020\n\n  target_url <- \n  paste0(\n    \"http://www.northwestknowledge.net/metdata/data/\",\n    var_name, \"_\", year,\n    \".nc\"\n  )\n\n  file_name <-\n  paste0(\n    \"Data/\",\n    var_name, \"_\", year,\n    \".nc\"\n  )\n\n  downloader::download(\n    url = target_url,\n    destfile = file_name,\n    mode = 'wb'\n  )\n\n  #--- read the raster data ---#\n  temp_rast <- rast(file_name)\n\n  temp_data <- \n  #--- extract data for each county ---#\n  exact_extract(temp_rast, CA_counties) %>% \n  #--- list of data.frames into data.table ---#\n  rbindlist(idcol = \"rowid\") %>% \n  #--- wide to long ---#\n  melt(id.var = c(\"rowid\", \"coverage_fraction\")) %>% \n  #--- remove observations with NA values ---#\n  .[!is.na(value), ] %>% \n  #--- get only the numeric part ---#\n  .[, variable := str_sub(variable, -5, -1) %>% as.numeric()] %>% \n  #--- recover dates ---#\n  .[, date := variable + ymd(\"1900-01-01\")] %>% \n  #--- find daily coverage-weight average by county ---#\n  .[, \n    .(value = sum(value * coverage_fraction) / sum(coverage_fraction)), \n    by = .(rowid, date)\n  ] %>% \n  .[, var := var_name]\n\n  return(temp_data)\n\n}\n#--- create a dataset of parameters to be looped over---#\n(\npar_data <- \n  expand.grid(\n    var_name = c(\"pr\", \"pet\"),\n    year = 2015:2020\n  ) %>% \n  data.table() %>% \n  .[, var_name := as.character(var_name)]\n)\n#--- parallel processing ---#\nlibrary(future.apply)\nplan(multiprocess, workers = 12)\n\n(\nall_data <- \n  future_lapply(\n    seq_len(nrow(par_data)), \n    function(x) get_grid_MET(par_data[x, var_name], par_data[x, year])\n  ) %>% \n  rbindlist() %>% \n  dcast(rowid + date ~ var, value.var = \"value\")\n)        rowid       date      pet           pr\n     1:     1 2015-01-01 1.676250 5.085478e-05\n     2:     1 2015-01-02 1.333865 3.675661e-02\n     3:     1 2015-01-03 1.299940 0.000000e+00\n     4:     1 2015-01-04 1.687250 0.000000e+00\n     5:     1 2015-01-05 1.453381 0.000000e+00\n    ---                                       \n127132:    58 2020-12-27 1.851531 1.670983e+01\n127133:    58 2020-12-28 1.025404 1.435838e+01\n127134:    58 2020-12-29 1.559160 0.000000e+00\n127135:    58 2020-12-30 2.048376 1.066704e-01\n127136:    58 2020-12-31 1.398130 1.841696e-01"},{"path":"par-comp.html","id":"par-comp","chapter":"A Loop and Parallel Computing","heading":"A Loop and Parallel Computing","text":"","code":""},{"path":"par-comp.html","id":"before-you-start-9","chapter":"A Loop and Parallel Computing","heading":"Before you start","text":"learn program repetitive operations effectively fast. start basics loop familiar concept. cover parallel computation using future.lapply parallel package. familiar lapply() can go straight Chapter .2.specific learning objectives chapter.Learn use loop lapply() complete repetitive jobsLearn loop things can easily vectorizedLearn parallelize repetitive jobs using future_lapply() function future.apply package","code":""},{"path":"par-comp.html","id":"direction-for-replication-9","chapter":"A Loop and Parallel Computing","heading":"Direction for replication","text":"data Chapter generated.","code":""},{"path":"par-comp.html","id":"packages-to-install-and-load","chapter":"A Loop and Parallel Computing","heading":"Packages to install and load","text":"Run following code install load (already installed) pacman package, install load (already installed) listed package inside pacman::p_load() function.packages loaded demonstration.","code":"\nif (!require(\"pacman\")) install.packages(\"pacman\")\npacman::p_load(\n  dplyr, # data wrangling\n  data.table # data wrangling\n)  "},{"path":"par-comp.html","id":"repetitive-processes-and-looping","chapter":"A Loop and Parallel Computing","heading":"A.1 Repetitive processes and looping","text":"","code":""},{"path":"par-comp.html","id":"what-is-looping","chapter":"A Loop and Parallel Computing","heading":"A.1.1 What is looping?","text":"sometimes need run process often slight changes parameters. case, time-consuming messy write steps one bye one. example, suppose interested knowing square 1 5 (\\([1, 2, 3, 4, 5]\\)). following code certainly works:However, imagine 1000 integers. Yes, don’t want write one one one occupy 1000 lines code, time-consuming. Things even worse need repeat much complicated processes like Monte Carlo simulations. , let’s learn write program repetitive jobs effectively using loop.Looping repeatedly evaluating (except parameters) process . example , process action squaring. change among processes run. changes square. Looping can help write concise code implement repetitive processes.","code":"\n1^2 [1] 1\n2^2 [1] 4\n3^2 [1] 9\n4^2 [1] 16\n5^2 [1] 25"},{"path":"par-comp.html","id":"for-loop","chapter":"A Loop and Parallel Computing","heading":"A.1.2 For loop","text":"loop works general:example, let’s use looping syntax get results manual squaring 1 5:, list values \\(1, 2, 3, 4, 5]\\). value list, square (x^2) print (print()). want get square \\(1:1000\\), thing need change list values loop :, length code depend many repeats , obvious improvement manual typing every single process one one. Note use \\(x\\) refer object going use. combination letters long use code want inside loop. , work just fine,","code":"#--- NOT RUN ---#\nfor (x in a_list_of_values){\n  you do what you want to do with x\n}\nfor (x in 1:5){\n  print(x^2)\n}[1] 1\n[1] 4\n[1] 9\n[1] 16\n[1] 25\n#--- evaluation not reported as it's too long ---#\nfor (x in 1:1000){\n  print(x^2)\n}\nfor (bluh_bluh_bluh in 1:5){\n  print(bluh_bluh_bluh^2)\n}[1] 1\n[1] 4\n[1] 9\n[1] 16\n[1] 25"},{"path":"par-comp.html","id":"for-loop-using-the-lapply-function","chapter":"A Loop and Parallel Computing","heading":"A.1.3 For loop using the lapply() function","text":"can loop using lapply() function well.95 works:\\(\\) list values go one one order values stored, \\(B\\) function like apply values \\(\\). example, following code exactly thing loop example., \\(\\) \\([1, 2, 3, 4, 5]\\). \\(B\\) function takes \\(x\\) square . , code applies function \\([1, 2, 3, 4, 5]\\) one one. many circumstances, can write looping actions much concise manner using lapply function explicitly writing loop process loop examples. might noticed output list. Yes, lapply() returns outcomes list. l lapply() comes .operation like repeat becomes complicated (almost always case), advisable create function process first.Finally, myth always use lapply() instead explicit loop syntax lapply() (apply() families) faster. basically .96","code":"\n#--- NOT RUN ---#  \nlapply(A,B)\nlapply(1:5, function(x){x^2})[[1]]\n[1] 1\n\n[[2]]\n[1] 4\n\n[[3]]\n[1] 9\n\n[[4]]\n[1] 16\n\n[[5]]\n[1] 25\n#--- define the function first ---#\nsquare_it <- function(x){\n  return(x^2)\n}\n\n#--- lapply using the pre-defined function ---#\nlapply(1:5, square_it)[[1]]\n[1] 1\n\n[[2]]\n[1] 4\n\n[[3]]\n[1] 9\n\n[[4]]\n[1] 16\n\n[[5]]\n[1] 25"},{"path":"par-comp.html","id":"looping-over-multiple-variables-using-lapply","chapter":"A Loop and Parallel Computing","heading":"A.1.4 Looping over multiple variables using lapply()","text":"lapply() allows loop one variable. However, often case want loop multiple variables97. However, easy achieve . trick create data.frame variables complete list combinations variables stored, loop row data.frame. example, suppose interested understanding sensitivity corn revenue corn price applied nitrogen amount. consider range $3.0/bu $5.0/bu corn price 0 lb/acre 300/acre nitrogen rate.creating vectors parameters, combine create complete combination parameters using expand.grid() function, convert data.frame object98.now define function takes row number, refer parameters_data extract parameters stored row number, calculate corn yield revenue based extracted parameters.function takes \\(\\) (act row number within function), extract corn price nitrogen \\(\\)th row parameters_mat, used calculate yield revenue99. Finally, returns data.frame information used (parameters outcomes).Successful! Now, us use outcome purposes like analysis visualization, need results combined single data.frame instead list data.frames. , use either bind_rows() dplyr package rbindlist() data.table package.","code":"\n#--- corn price vector ---#\ncorn_price_vec <- seq(3, 5, by = 1)\n\n#--- nitrogen vector ---#\nnitrogen_vec <- seq(0, 300, by = 100)\n#--- crate a data.frame that holds parameter sets to loop over ---#\nparameters_data <- expand.grid(corn_price = corn_price_vec, nitrogen = nitrogen_vec) %>% \n  #--- convert the matrix to a data.frame ---#\n  data.frame()\n\n#--- take a look ---#\nparameters_data   corn_price nitrogen\n1           3        0\n2           4        0\n3           5        0\n4           3      100\n5           4      100\n6           5      100\n7           3      200\n8           4      200\n9           5      200\n10          3      300\n11          4      300\n12          5      300\ngen_rev_corn <- function(i) {\n\n  #--- define corn price ---#\n  corn_price <- parameters_data[i,'corn_price']\n\n  #--- define nitrogen  ---#\n  nitrogen <- parameters_data[i,'nitrogen']\n\n  #--- calculate yield ---#\n  yield <- 240 * (1 - exp(0.4 - 0.02 * nitrogen))\n\n  #--- calculate revenue ---#\n  revenue <- corn_price * yield \n\n  #--- combine all the information you would like to have  ---#\n  data_to_return <- data.frame(\n    corn_price = corn_price,\n    nitrogen = nitrogen,\n    revenue = revenue\n  )\n\n  return(data_to_return)\n}\n#--- loop over all the parameter combinations ---#\nrev_data <- lapply(1:nrow(parameters_data), gen_rev_corn)\n\n#--- take a look ---#\nrev_data[[1]]\n  corn_price nitrogen   revenue\n1          3        0 -354.1138\n\n[[2]]\n  corn_price nitrogen   revenue\n1          4        0 -472.1517\n\n[[3]]\n  corn_price nitrogen   revenue\n1          5        0 -590.1896\n\n[[4]]\n  corn_price nitrogen  revenue\n1          3      100 574.6345\n\n[[5]]\n  corn_price nitrogen  revenue\n1          4      100 766.1793\n\n[[6]]\n  corn_price nitrogen  revenue\n1          5      100 957.7242\n\n[[7]]\n  corn_price nitrogen  revenue\n1          3      200 700.3269\n\n[[8]]\n  corn_price nitrogen  revenue\n1          4      200 933.7692\n\n[[9]]\n  corn_price nitrogen  revenue\n1          5      200 1167.212\n\n[[10]]\n  corn_price nitrogen  revenue\n1          3      300 717.3375\n\n[[11]]\n  corn_price nitrogen  revenue\n1          4      300 956.4501\n\n[[12]]\n  corn_price nitrogen  revenue\n1          5      300 1195.563\n#--- bind_rows ---#\nbind_rows(rev_data)   corn_price nitrogen   revenue\n1           3        0 -354.1138\n2           4        0 -472.1517\n3           5        0 -590.1896\n4           3      100  574.6345\n5           4      100  766.1793\n6           5      100  957.7242\n7           3      200  700.3269\n8           4      200  933.7692\n9           5      200 1167.2115\n10          3      300  717.3375\n11          4      300  956.4501\n12          5      300 1195.5626\n#--- rbindlist ---#\nrbindlist(rev_data)    corn_price nitrogen   revenue\n 1:          3        0 -354.1138\n 2:          4        0 -472.1517\n 3:          5        0 -590.1896\n 4:          3      100  574.6345\n 5:          4      100  766.1793\n 6:          5      100  957.7242\n 7:          3      200  700.3269\n 8:          4      200  933.7692\n 9:          5      200 1167.2115\n10:          3      300  717.3375\n11:          4      300  956.4501\n12:          5      300 1195.5626"},{"path":"par-comp.html","id":"do-you-really-need-to-loop","chapter":"A Loop and Parallel Computing","heading":"A.1.5 Do you really need to loop?","text":"Actually, used loop lapply() examples practice100 can easily vectorized. Vectorized operations take vectors inputs work element vectors parallel101.typical example vectorized operation :non-vectorized version calculation :produce results. However, R written way much better vectorized operations. Let’s time using microbenchmark() function microbenchmark package. , unlist() lapply() just focus multiplication part.can see, vectorized version faster. time difference comes R conduct many internal checks hidden operations non-vectorized one102. Yes, talking fraction milliseconds . , objects operate get larger, difference vectorized non-vectorized operations can become substantial103.lapply() examples can easily vectorized.Instead :can just :can also easily vectorize revenue calculation demonstrated . First, define function differently revenue calculation can take corn price nitrogen vectors return revenue vector.use function calculate revenue assign new variable parameters_data data.Let’s compare two:Yes, vectorized version faster. , lesson can vectorize, vectorize instead using lapply(). , course, things vectorized many cases.","code":"\n#--- define numeric vectors ---#\nx <- 1:1000\ny <- 1:1000\n\n#--- element wise addition ---#\nz_vec <- x + y \nz_la <- lapply(1:1000, function(i) x[i] + y[i]) %>%  unlist\n\n#--- check if identical with z_vec ---#\nall.equal(z_la, z_vec)[1] TRUE\nlibrary(microbenchmark)\n\nmicrobenchmark(\n  #--- vectorized ---#\n  \"vectorized\" = { x + y }, \n  #--- not vectorized ---#\n  \"not vectorized\" = { lapply(1:1000, function(i) x[i] + y[i])},\n  times = 100, \n  unit = \"ms\"\n)Unit: milliseconds\n           expr      min        lq       mean   median        uq       max\n     vectorized 0.002428 0.0028800 0.00316221 0.003082 0.0033105  0.005597\n not vectorized 0.496875 0.5368535 0.72333386 0.552854 0.5681425 14.101327\n neval\n   100\n   100\nlapply(1:1000, square_it)\nsquare_it(1:1000)\ngen_rev_corn_short <- function(corn_price, nitrogen) {\n\n  #--- calculate yield ---#\n  yield <- 240 * (1 - exp(0.4 - 0.02 * nitrogen))\n\n  #--- calculate revenue ---#\n  revenue <- corn_price * yield \n\n  return(revenue)\n}\nrev_data_2 <- mutate(\n  parameters_data,\n  revenue = gen_rev_corn_short(corn_price, nitrogen)\n) \nmicrobenchmark(\n\n  #--- vectorized ---#\n  \"vectorized\" = { rev_data <- mutate(parameters_data, revenue = gen_rev_corn_short(corn_price, nitrogen)) },\n  #--- not vectorized ---#\n  \"not vectorized\" = { parameters_data$revenue <- lapply(1:nrow(parameters_data), gen_rev_corn) },\n  times = 100, \n  unit = \"ms\"\n)Unit: milliseconds\n           expr      min       lq     mean   median       uq      max neval\n     vectorized 0.961107 1.041289 1.554750 1.129748 1.309716 23.56163   100\n not vectorized 1.894142 2.118015 2.514545 2.282599 2.796153  5.66208   100"},{"path":"par-comp.html","id":"parcomp","chapter":"A Loop and Parallel Computing","heading":"A.2 Parallelization of embarrassingly parallel processes","text":"Parallelization computation involves distributing task hand multiple cores multiple processes done parallel. , learn parallelize computation R. focus called embarrassingly parallel processes. Embarrassingly parallel processes refer collection processes process completely independent another. , one process use outputs processes. example integer squaring embarrassingly parallel. order calculate \\(1^2\\), need use result \\(2^2\\) squares. Embarrassingly parallel processes easy parallelize worry process complete first make processes happen. Fortunately, processes interested parallelizing fall category104.use future_lapply() function future.apply package parallelization105. Using package, parallelization piece cake basically syntactically lapply().can find many cores available parallel computation computer using detectCores() function parallel package.implement parallelized lapply(), need declare backend process using plan(). , use plan(multiprocess)106. plan() function, can specify number workers. use total number cores less 1107.future_lapply() works exactly like lapply().. difference see serialized processing using lapply() changed function name future_lapply().Okay, now know parallelize computation. Let’s check much improvement implementation time got parallelization.Hmmmm, okay, parallelization made code slower… ? communicating jobs core takes time well. , iterative processes super fast (like example just square number), time spent communicating cores outweighs time saving due parallel computation. Parallelization beneficial repetitive processes takes long.One good use cases parallelization MC simulation. following MC simulation tests whether correlation independent variable error term cause bias (yes, know answer). MC_sim function first generates dataset (50,000 observations) according following data generating process:\\[\n y = 1 + x + v\n\\]\\(\\mu \\sim N(0,1)\\), \\(x \\sim N(0,1) + \\mu\\), \\(v \\sim N(0,1) + \\mu\\). \\(\\mu\\) term cause correlation \\(x\\) (covariate) \\(v\\) (error term). estimates coefficient \\(x\\) vis OLS, return estimate. like repeat process 1,000 times understand property OLS estimators data generating process. Monte Carlo simulation embarrassingly parallel process independent .Let’s run one iteration,Okay, takes 0.023 second one iteration. Now, let’s run 1000 times without parallelization.parallelizedParallelizedAs can see, parallelization makes much quicker noticeable difference elapsed time. made code 5.25 times faster. However, make process 15 times faster even though used 15 cores parallelized process. overhead associated distributing tasks cores. relative advantage parallelization greater iteration took time. example, running process takes 2 minutes 1000 times, take approximately 33 hours 20 minutes. , may take 4 hours parallelize 15 cores, maybe even 2 hours run 30 cores.","code":"\n#--- load packages ---#\nlibrary(future.apply) \nlibrary(parallel)\n\n#--- number of all cores ---#\ndetectCores()[1] 16\nplan(multiprocess, workers = detectCores() - 1)\nsq_ls <- future_lapply(1:1000, function(x) x^2)\nmicrobenchmark(\n  #--- parallelized ---#\n  \"parallelized\" = { sq_ls <- future_lapply(1:1000, function(x) x^2) }, \n  #--- non-parallelized ---#\n  \"not parallelized\" = { sq_ls <- lapply(1:1000, function(x) x^2) },\n  times = 100, \n  unit = \"ms\"\n)Unit: milliseconds\n             expr       min         lq       mean    median         uq\n     parallelized 74.337087 88.5161295 99.5311075 97.196593 108.210706\n not parallelized  0.411587  0.4805975  0.7229046  0.536682   0.683037\n       max neval\n 136.86259   100\n  11.07314   100\n#--- repeat steps 1-3 B times ---#\nMC_sim <- function(i){\n\n  N <- 50000 # sample size\n\n  #--- steps 1 and 2:  ---#\n  mu <- rnorm(N) # the common term shared by both x and u\n  x <- rnorm(N) + mu # independent variable\n  v <- rnorm(N) + mu # error\n  y <- 1 + x + v # dependent variable\n  data <- data.table(y = y, x = x)\n\n  #--- OLS ---# \n  reg <- lm(y~x, data = data) # OLS\n\n  #--- return the coef ---#\n  return(reg$coef['x'])\n}\ntic()\nMC_sim(1)\ntoc()       x \n1.503353 elapsed \n  0.023 \n#--- non-parallel ---#\ntic()\nMC_results <- lapply(1:1000, MC_sim)\ntoc()elapsed \n 22.987 \n#--- parallel ---#\ntic()\nMC_results <- future_lapply(1:1000, MC_sim)\ntoc()elapsed \n  4.376 "},{"path":"par-comp.html","id":"mac-or-linux-users","chapter":"A Loop and Parallel Computing","heading":"A.2.1 Mac or Linux users","text":"Mac users, parallel::mclapply() just compelling (pbmclapply::pbmclapply() want nice progress report, helpful particularly process long). just easy use future_lapply() syntax lapply(). can control number cores employ adding mc.cores option. example code MC simulations conducted :","code":"\n#--- mclapply ---#\nlibrary(parallel)\nMC_results <- mclapply(1:1000, MC_sim, mc.cores = detectCores() - 1)\n\n#--- or with progress bar ---#\nlibrary(pbmclapply)\nMC_results <- pbmclapply(1:1000, MC_sim, mc.cores = detectCores() - 1)"},{"path":"ggplot2-minimals.html","id":"ggplot2-minimals","chapter":"B ggplot2 minimals","heading":"B ggplot2 minimals","text":"Note: section provide complete treatment basics ggplot2 package. Rather, provides minimal knowledge package readers familiar package can still understand codes map making presented Chapter 8.ggplot2 package general extensive data visualization tool. popular among R users due elegance ease use generating high-quality figures. ggplot2 package designed following “grammar graphics,” makes possible visualize data easy consistent manner irrespective type figures generated, whether simple scatter plot complicated map. means learning basics ggplot2 works directly helps creating maps well. chapter goes basics ggplot2 works general.ggplot2, first specify data use specify use data visualization depending types figures intend make using geom_*(). simple example, let’s use mpg data create simple scatter plot. mpg dataset looks like:code creates scatter plot displ hwy variables mpg dataset.\nFigure B.1: Scatter plot observations color-differentiated class\nHowever, one work color = class outside aes() R look class object inside mpg.can still specify color applied universally observations dataset like :examples clarify aes() : makes aesthetics figure data-dependent.code create Figure B.1, default color option used color-differentiation class. can specify color scheme using scale_*(). scale_*() function generally takes form o fscale_x_y(), x type aesthetics want control, y method specifying color scheme. example, code , type aesthetics color. suppose like use brewer method. scale function using scale_color_brewer(). code uses scale_color_brewer() palette option specify color scheme .\nFigure B.2: Scatter plot color scheme defined user\ncan see color scheme now changed. many different types pallets available.create different type figure scatter plot, can pick different geom_*(). example, geom_histogram() creates histogram.can save created figure (precisely data underpins figure) R object follows:can call saved object see figure.Another important feature ggplot2 can add layers existing ggplot object + geom_*(). example, following code adds linear regression line plot:feature makes easy plot different spatial objects single map find later.“Faceting” another useful feature package. Faceting splits data groups generates figure group aesthetics figures consistent across groups. Faceting can done using facet_wrap() facet_grid(). example using facet_wrap():year ~ . inside facet_wrap() tells R split data year. . year ~ . means “variable.”108 , code splits mpg data year, applies geom_point() geom_smooth(), applies scale_color_brewer() , creates figure group. created figures presented side--side.109 feature can handy, example, like display changes land use time faceting done year.important ggplot2 features aware make informative maps, discuss . Rather, introduce first appear lecture examples. interested learning basics ggplot2, numerous books written market. prominent ones ","code":"\nmpg # A tibble: 234 × 11\n   manufacturer model      displ  year   cyl trans drv     cty   hwy fl    class\n   <chr>        <chr>      <dbl> <int> <int> <chr> <chr> <int> <int> <chr> <chr>\n 1 audi         a4           1.8  1999     4 auto… f        18    29 p     comp…\n 2 audi         a4           1.8  1999     4 manu… f        21    29 p     comp…\n 3 audi         a4           2    2008     4 manu… f        20    31 p     comp…\n 4 audi         a4           2    2008     4 auto… f        21    30 p     comp…\n 5 audi         a4           2.8  1999     6 auto… f        16    26 p     comp…\n 6 audi         a4           2.8  1999     6 manu… f        18    26 p     comp…\n 7 audi         a4           3.1  2008     6 auto… f        18    27 p     comp…\n 8 audi         a4 quattro   1.8  1999     4 manu… 4        18    26 p     comp…\n 9 audi         a4 quattro   1.8  1999     4 auto… 4        16    25 p     comp…\n10 audi         a4 quattro   2    2008     4 manu… 4        20    28 p     comp…\n# … with 224 more rows\nggplot(data = mpg) + \n  geom_point(aes(x = displ, y = hwy))\nggplot(data = mpg) + \n  geom_point(aes(x = displ, y = hwy, color = class))\nggplot(data = mpg) + \n  geom_point(aes(x = displ, y = hwy), color = class)Error in rep(value[[k]], length.out = n): attempt to replicate an object of type 'builtin'\nggplot(data = mpg) + \n  geom_point(aes(x = displ, y = hwy), color = \"blue\")\nggplot(data = mpg) + \n  geom_point(aes(x = displ, y = hwy, color = class)) +\n  scale_color_brewer(palette = 1)\nggplot(data = mpg) + \n  geom_histogram(aes(x = hwy), color = \"blue\", fill = \"white\")\n#--- save the figure to g_plot ---#\ng_plot <- ggplot(data = mpg) + \n  geom_point(aes(x = displ, y = hwy, color = class)) +\n  scale_color_brewer(palette = 1)\n\n#--- see the class ---#\nclass(g_plot)[1] \"gg\"     \"ggplot\"\ng_plot\ng_plot + \n  geom_smooth(aes(x = displ, y = hwy), method = \"lm\")\nggplot(data = mpg) + \n  geom_point(aes(x = displ, y = hwy, color = class)) + \n  geom_smooth(aes(x = displ, y = hwy), method = \"lm\") +\n  scale_color_brewer(palette = 1) +\n  facet_wrap(year ~ .) "},{"path":"references.html","id":"references","chapter":"References","heading":"References","text":"","code":""}]
